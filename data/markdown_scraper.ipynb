{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Semantic Search Engine with Python and pgAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to build a simple semantic search engine using Python and the pgAI extension for PostgreSQL. We'll use a set of Markdown files as our data source, process them, store them in a database, and then use pgAI's vector search capabilities to perform semantic search.\n",
    "\n",
    " **Here's a breakdown of the steps:**\n",
    "\n",
    "1. **Gather Markdown Files:** We'll start by collecting all the Markdown files from a specified directory.\n",
    "2. **Chunk the Data:**  Each Markdown file will be split into smaller, more manageable chunks based on headers. This helps in organizing and retrieving information more effectively.\n",
    "3. **Store in Database:**  We'll store these chunks in a PostgreSQL database, along with relevant metadata.\n",
    "4. **Embed with pgAI:** Using pgAI's vectorizer, we'll generate embeddings for each chunk of text. These embeddings capture the semantic meaning of the text and are crucial for semantic search.\n",
    "5. **Semantic Search with SQL:** We'll use SQL queries to perform semantic searches against our database, leveraging the power of pgAI.\n",
    "6. **Python Integration:** Finally, we'll demonstrate how to integrate these semantic search capabilities into a Python application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gathering Markdown Files\n",
    "\n",
    "# We'll define a function to locate all Markdown files within a given directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_markdown_files(directory):\n",
    "    markdown_files = []\n",
    "    # Walk through the directory\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            # Check if the file has a .md extension\n",
    "            if file.endswith('.md'):\n",
    "                # Append the full path to the list\n",
    "                markdown_files.append(os.path.join(root, file))\n",
    "    return markdown_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets quickly print the files to make sure we are good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pydantic-ai/README.md', 'pydantic-ai/pydantic_ai_slim/README.md', 'pydantic-ai/tests/example_modules/README.md', 'pydantic-ai/docs/troubleshooting.md', 'pydantic-ai/docs/dependencies.md', 'pydantic-ai/docs/install.md', 'pydantic-ai/docs/help.md', 'pydantic-ai/docs/message-history.md', 'pydantic-ai/docs/testing-evals.md', 'pydantic-ai/docs/multi-agent-applications.md', 'pydantic-ai/docs/results.md', 'pydantic-ai/docs/index.md', 'pydantic-ai/docs/models.md', 'pydantic-ai/docs/contributing.md', 'pydantic-ai/docs/agents.md', 'pydantic-ai/docs/logfire.md', 'pydantic-ai/docs/graph.md', 'pydantic-ai/docs/tools.md', 'pydantic-ai/docs/examples/rag.md', 'pydantic-ai/docs/examples/bank-support.md', 'pydantic-ai/docs/examples/flight-booking.md', 'pydantic-ai/docs/examples/stream-whales.md', 'pydantic-ai/docs/examples/sql-gen.md', 'pydantic-ai/docs/examples/pydantic-model.md', 'pydantic-ai/docs/examples/chat-app.md', 'pydantic-ai/docs/examples/question-graph.md', 'pydantic-ai/docs/examples/index.md', 'pydantic-ai/docs/examples/stream-markdown.md', 'pydantic-ai/docs/examples/weather-agent.md', 'pydantic-ai/docs/api/agent.md', 'pydantic-ai/docs/api/exceptions.md', 'pydantic-ai/docs/api/settings.md', 'pydantic-ai/docs/api/messages.md', 'pydantic-ai/docs/api/result.md', 'pydantic-ai/docs/api/usage.md', 'pydantic-ai/docs/api/format_as_xml.md', 'pydantic-ai/docs/api/tools.md', 'pydantic-ai/docs/api/models/anthropic.md', 'pydantic-ai/docs/api/models/groq.md', 'pydantic-ai/docs/api/models/openai.md', 'pydantic-ai/docs/api/models/base.md', 'pydantic-ai/docs/api/models/ollama.md', 'pydantic-ai/docs/api/models/function.md', 'pydantic-ai/docs/api/models/mistral.md', 'pydantic-ai/docs/api/models/vertexai.md', 'pydantic-ai/docs/api/models/gemini.md', 'pydantic-ai/docs/api/models/test.md', 'pydantic-ai/docs/api/pydantic_graph/exceptions.md', 'pydantic-ai/docs/api/pydantic_graph/state.md', 'pydantic-ai/docs/api/pydantic_graph/nodes.md', 'pydantic-ai/docs/api/pydantic_graph/mermaid.md', 'pydantic-ai/docs/api/pydantic_graph/graph.md', 'pydantic-ai/examples/README.md', 'pydantic-ai/pydantic_graph/README.md']\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "directory_path = 'pydantic-ai'\n",
    "markdown_files = find_markdown_files(directory_path)\n",
    "print(markdown_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pydantic-ai/README.md\n",
      "pydantic-ai/pydantic_ai_slim/README.md\n",
      "pydantic-ai/tests/example_modules/README.md\n",
      "pydantic-ai/docs/troubleshooting.md\n",
      "pydantic-ai/docs/dependencies.md\n",
      "pydantic-ai/docs/install.md\n",
      "pydantic-ai/docs/help.md\n",
      "pydantic-ai/docs/message-history.md\n",
      "pydantic-ai/docs/testing-evals.md\n",
      "pydantic-ai/docs/multi-agent-applications.md\n",
      "pydantic-ai/docs/results.md\n",
      "pydantic-ai/docs/index.md\n",
      "pydantic-ai/docs/models.md\n",
      "pydantic-ai/docs/contributing.md\n",
      "pydantic-ai/docs/agents.md\n",
      "pydantic-ai/docs/logfire.md\n",
      "pydantic-ai/docs/graph.md\n",
      "pydantic-ai/docs/tools.md\n",
      "pydantic-ai/docs/examples/rag.md\n",
      "pydantic-ai/docs/examples/bank-support.md\n",
      "pydantic-ai/docs/examples/flight-booking.md\n",
      "pydantic-ai/docs/examples/stream-whales.md\n",
      "pydantic-ai/docs/examples/sql-gen.md\n",
      "pydantic-ai/docs/examples/pydantic-model.md\n",
      "pydantic-ai/docs/examples/chat-app.md\n",
      "pydantic-ai/docs/examples/question-graph.md\n",
      "pydantic-ai/docs/examples/index.md\n",
      "pydantic-ai/docs/examples/stream-markdown.md\n",
      "pydantic-ai/docs/examples/weather-agent.md\n",
      "pydantic-ai/docs/api/agent.md\n",
      "pydantic-ai/docs/api/exceptions.md\n",
      "pydantic-ai/docs/api/settings.md\n",
      "pydantic-ai/docs/api/messages.md\n",
      "pydantic-ai/docs/api/result.md\n",
      "pydantic-ai/docs/api/usage.md\n",
      "pydantic-ai/docs/api/format_as_xml.md\n",
      "pydantic-ai/docs/api/tools.md\n",
      "pydantic-ai/docs/api/models/anthropic.md\n",
      "pydantic-ai/docs/api/models/groq.md\n",
      "pydantic-ai/docs/api/models/openai.md\n",
      "pydantic-ai/docs/api/models/base.md\n",
      "pydantic-ai/docs/api/models/ollama.md\n",
      "pydantic-ai/docs/api/models/function.md\n",
      "pydantic-ai/docs/api/models/mistral.md\n",
      "pydantic-ai/docs/api/models/vertexai.md\n",
      "pydantic-ai/docs/api/models/gemini.md\n",
      "pydantic-ai/docs/api/models/test.md\n",
      "pydantic-ai/docs/api/pydantic_graph/exceptions.md\n",
      "pydantic-ai/docs/api/pydantic_graph/state.md\n",
      "pydantic-ai/docs/api/pydantic_graph/nodes.md\n",
      "pydantic-ai/docs/api/pydantic_graph/mermaid.md\n",
      "pydantic-ai/docs/api/pydantic_graph/graph.md\n",
      "pydantic-ai/examples/README.md\n",
      "pydantic-ai/pydantic_graph/README.md\n"
     ]
    }
   ],
   "source": [
    "# pretty print the markdown file names\n",
    "for file in markdown_files:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chunking the Markdown Files\n",
    "\n",
    "To make our data more manageable and improve search accuracy, we'll split each Markdown file into smaller chunks based on headers. This function will handle the chunking process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div align=\"center\">\n",
      "  <a href=\"https://ai.pydantic.dev/\">\n",
      "    <picture>\n",
      "      <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://ai.pydantic.dev/img/pydantic-ai-dark.svg\">\n",
      "      <img src=\"https://ai.pydantic.dev/img/pydantic-ai-light.svg\" alt=\"PydanticAI\">\n",
      "    </picture>\n",
      "  </a>\n",
      "</div>\n",
      "<div align=\"center\">\n",
      "  <em>Agent Framework / shim to use Pydantic with LLMs</em>\n",
      "</div>\n",
      "<div align=\"center\">\n",
      "  <a href=\"https://github.com/pydantic/pydantic-ai/actions/workflows/ci.yml?query=branch%3Amain\"><img src=\"https://github.com/pydantic/pydantic-ai/actions/workflows/ci.yml/badge.svg?event=push\" alt=\"CI\"></a>\n",
      "  <a href=\"https://coverage-badge.samuelcolvin.workers.dev/redirect/pydantic/pydantic-ai\"><img src=\"https://coverage-badge.samuelcolvin.workers.dev/pydantic/pydantic-ai.svg\" alt=\"Coverage\"></a>\n",
      "  <a href=\"https://pypi.python.org/pypi/pydantic-ai\"><img src=\"https://img.shields.io/pypi/v/pydantic-ai.svg\" alt=\"PyPI\"></a>\n",
      "  <a href=\"https://github.com/pydantic/pydantic-ai\"><img src=\"https://img.shields.io/pypi/pyversions/pydantic-ai.svg\" alt=\"versions\"></a>\n",
      "  <a href=\"https://github.com/pydantic/pydantic-ai/blob/main/LICENSE\"><img src=\"https://img.shields.io/github/license/pydantic/pydantic-ai.svg?v\" alt=\"license\"></a>\n",
      "</div>\n",
      "\n",
      "---\n",
      "\n",
      "**Documentation**: [ai.pydantic.dev](https://ai.pydantic.dev/)\n",
      "\n",
      "---\n",
      "\n",
      "PydanticAI is a Python agent framework designed to make it less painful to build production grade applications with Generative AI.\n",
      "\n",
      "FastAPI revolutionized web development by offering an innovative and ergonomic design, built on the foundation of [Pydantic](https://docs.pydantic.dev).\n",
      "\n",
      "Similarly, virtually every agent framework and LLM library in Python uses Pydantic, yet when we began to use LLMs in [Pydantic Logfire](https://pydantic.dev/logfire), we couldn't find anything that gave us the same feeling.\n",
      "\n",
      "We built PydanticAI with one simple aim: to bring that FastAPI feeling to GenAI app development.\n",
      "\n",
      "## Why use PydanticAI\n",
      "\n",
      "* __Built by the Pydantic Team__\n",
      "Built by the team behind [Pydantic](https://docs.pydantic.dev/latest/) (the validation layer of the OpenAI SDK, the Anthropic SDK, LangChain, LlamaIndex, AutoGPT, Transformers, CrewAI, Instructor and many more).\n",
      "\n",
      "* __Model-agnostic__\n",
      "Supports OpenAI, Anthropic, Gemini, Ollama, Groq, and Mistral, and there is a simple interface to implement support for [other models](https://ai.pydantic.dev/models/).\n",
      "\n",
      "* __Pydantic Logfire Integration__\n",
      "Seamlessly [integrates](https://ai.pydantic.dev/logfire/) with [Pydantic Logfire](https://pydantic.dev/logfire) for real-time debugging, performance monitoring, and behavior tracking of your LLM-powered applications.\n",
      "\n",
      "* __Type-safe__\n",
      "Designed to make [type checking](https://ai.pydantic.dev/agents/#static-type-checking) as powerful and informative as possible for you.\n",
      "\n",
      "* __Python-centric Design__\n",
      "Leverages Python's familiar control flow and agent composition to build your AI-driven projects, making it easy to apply standard Python best practices you'd use in any other (non-AI) project.\n",
      "\n",
      "* __Structured Responses__\n",
      "Harnesses the power of [Pydantic](https://docs.pydantic.dev/latest/) to [validate and structure](https://ai.pydantic.dev/results/#structured-result-validation) model outputs, ensuring responses are consistent across runs.\n",
      "\n",
      "* __Dependency Injection System__\n",
      "Offers an optional [dependency injection](https://ai.pydantic.dev/dependencies/) system to provide data and services to your agent's [system prompts](https://ai.pydantic.dev/agents/#system-prompts), [tools](https://ai.pydantic.dev/tools/) and [result validators](https://ai.pydantic.dev/results/#result-validators-functions).\n",
      "This is useful for testing and eval-driven iterative development.\n",
      "\n",
      "* __Streamed Responses__\n",
      "Provides the ability to [stream](https://ai.pydantic.dev/results/#streamed-results) LLM outputs continuously, with immediate validation, ensuring rapid and accurate results.\n",
      "\n",
      "* __Graph Support__\n",
      "[Pydantic Graph](https://ai.pydantic.dev/graph) provides a powerful way to define graphs using typing hints, this is useful in complex applications where standard control flow can degrade to spaghetti code.\n",
      "\n",
      "## In Beta!\n",
      "\n",
      "PydanticAI is in early beta, the API is still subject to change and there's a lot more to do.\n",
      "[Feedback](https://github.com/pydantic/pydantic-ai/issues) is very welcome!\n",
      "\n",
      "## Hello World Example\n",
      "\n",
      "Here's a minimal example of PydanticAI:\n",
      "\n",
      "```python\n",
      "from pydantic_ai import Agent\n",
      "\n",
      "# Define a very simple agent including the model to use, you can also set the model when running the agent.\n",
      "agent = Agent(\n",
      "    'gemini-1.5-flash',\n",
      "    # Register a static system prompt using a keyword argument to the agent.\n",
      "    # For more complex dynamically-generated system prompts, see the example below.\n",
      "    system_prompt='Be concise, reply with one sentence.',\n",
      ")\n",
      "\n",
      "# Run the agent synchronously, conducting a conversation with the LLM.\n",
      "# Here the exchange should be very short: PydanticAI will send the system prompt and the user query to the LLM,\n",
      "# the model will return a text response. See below for a more complex run.\n",
      "result = agent.run_sync('Where does \"hello world\" come from?')\n",
      "print(result.data)\n",
      "\"\"\"\n",
      "The first known use of \"hello, world\" was in a 1974 textbook about the C programming language.\n",
      "\"\"\"\n",
      "```\n",
      "\n",
      "_(This example is complete, it can be run \"as is\")_\n",
      "\n",
      "Not very interesting yet, but we can easily add \"tools\", dynamic system prompts, and structured responses to build more powerful agents.\n",
      "\n",
      "## Tools & Dependency Injection Example\n",
      "\n",
      "Here is a concise example using PydanticAI to build a support agent for a bank:\n",
      "\n",
      "**(Better documented example [in the docs](https://ai.pydantic.dev/#tools-dependency-injection-example))**\n",
      "\n",
      "```python\n",
      "from dataclasses import dataclass\n",
      "\n",
      "from pydantic import BaseModel, Field\n",
      "from pydantic_ai import Agent, RunContext\n",
      "\n",
      "from bank_database import DatabaseConn\n",
      "\n",
      "\n",
      "# SupportDependencies is used to pass data, connections, and logic into the model that will be needed when running\n",
      "# system prompt and tool functions. Dependency injection provides a type-safe way to customise the behavior of your agents.\n",
      "@dataclass\n",
      "class SupportDependencies:\n",
      "    customer_id: int\n",
      "    db: DatabaseConn\n",
      "\n",
      "\n",
      "# This pydantic model defines the structure of the result returned by the agent.\n",
      "class SupportResult(BaseModel):\n",
      "    support_advice: str = Field(description='Advice returned to the customer')\n",
      "    block_card: bool = Field(description=\"Whether to block the customer's card\")\n",
      "    risk: int = Field(description='Risk level of query', ge=0, le=10)\n",
      "\n",
      "\n",
      "# This agent will act as first-tier support in a bank.\n",
      "# Agents are generic in the type of dependencies they accept and the type of result they return.\n",
      "# In this case, the support agent has type `Agent[SupportDependencies, SupportResult]`.\n",
      "support_agent = Agent(\n",
      "    'openai:gpt-4o',\n",
      "    deps_type=SupportDependencies,\n",
      "    # The response from the agent will, be guaranteed to be a SupportResult,\n",
      "    # if validation fails the agent is prompted to try again.\n",
      "    result_type=SupportResult,\n",
      "    system_prompt=(\n",
      "        'You are a support agent in our bank, give the '\n",
      "        'customer support and judge the risk level of their query.'\n",
      "    ),\n",
      ")\n",
      "\n",
      "\n",
      "# Dynamic system prompts can make use of dependency injection.\n",
      "# Dependencies are carried via the `RunContext` argument, which is parameterized with the `deps_type` from above.\n",
      "# If the type annotation here is wrong, static type checkers will catch it.\n",
      "@support_agent.system_prompt\n",
      "async def add_customer_name(ctx: RunContext[SupportDependencies]) -> str:\n",
      "    customer_name = await ctx.deps.db.customer_name(id=ctx.deps.customer_id)\n",
      "    return f\"The customer's name is {customer_name!r}\"\n",
      "\n",
      "\n",
      "# `tool` let you register functions which the LLM may call while responding to a user.\n",
      "# Again, dependencies are carried via `RunContext`, any other arguments become the tool schema passed to the LLM.\n",
      "# Pydantic is used to validate these arguments, and errors are passed back to the LLM so it can retry.\n",
      "@support_agent.tool\n",
      "async def customer_balance(\n",
      "    ctx: RunContext[SupportDependencies], include_pending: bool\n",
      ") -> float:\n",
      "    \"\"\"Returns the customer's current account balance.\"\"\"\n",
      "    # The docstring of a tool is also passed to the LLM as the description of the tool.\n",
      "    # Parameter descriptions are extracted from the docstring and added to the parameter schema sent to the LLM.\n",
      "    balance = await ctx.deps.db.customer_balance(\n",
      "        id=ctx.deps.customer_id,\n",
      "        include_pending=include_pending,\n",
      "    )\n",
      "    return balance\n",
      "\n",
      "\n",
      "...  # In a real use case, you'd add more tools and a longer system prompt\n",
      "\n",
      "\n",
      "async def main():\n",
      "    deps = SupportDependencies(customer_id=123, db=DatabaseConn())\n",
      "    # Run the agent asynchronously, conducting a conversation with the LLM until a final response is reached.\n",
      "    # Even in this fairly simple case, the agent will exchange multiple messages with the LLM as tools are called to retrieve a result.\n",
      "    result = await support_agent.run('What is my balance?', deps=deps)\n",
      "    # The result will be validated with Pydantic to guarantee it is a `SupportResult`, since the agent is generic,\n",
      "    # it'll also be typed as a `SupportResult` to aid with static type checking.\n",
      "    print(result.data)\n",
      "    \"\"\"\n",
      "    support_advice='Hello John, your current account balance, including pending transactions, is $123.45.' block_card=False risk=1\n",
      "    \"\"\"\n",
      "\n",
      "    result = await support_agent.run('I just lost my card!', deps=deps)\n",
      "    print(result.data)\n",
      "    \"\"\"\n",
      "    support_advice=\"I'm sorry to hear that, John. We are temporarily blocking your card to prevent unauthorized transactions.\" block_card=True risk=8\n",
      "    \"\"\"\n",
      "```\n",
      "\n",
      "## Next Steps\n",
      "\n",
      "To try PydanticAI yourself, follow the instructions [in the examples](https://ai.pydantic.dev/examples/).\n",
      "\n",
      "Read the [docs](https://ai.pydantic.dev/agents/) to learn more about building applications with PydanticAI.\n",
      "\n",
      "Read the [API Reference](https://ai.pydantic.dev/api/agent/) to understand PydanticAI's interface.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print the contents of the first file\n",
    "with open(markdown_files[0], 'r') as file:\n",
    "    print(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Dict\n",
    "\n",
    "def chunk_markdown_file(file_path: str) -> List[Dict]:\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    # Split by headers (## or #)\n",
    "    chunks = re.split(r'(?=^#{1,2}\\s)', content, flags=re.MULTILINE)\n",
    "    \n",
    "    processed_chunks = []\n",
    "    for chunk in chunks:\n",
    "        if chunk.strip():  # Skip empty chunks\n",
    "            # Extract header if it exists\n",
    "            header_match = re.match(r'^#{1,2}\\s+(.+)$', chunk.split('\\n')[0])\n",
    "            header = header_match.group(1) if header_match else \"No Header\"\n",
    "            \n",
    "            processed_chunks.append({\n",
    "                'source_file': file_path,\n",
    "                'title': header,\n",
    "                'content': chunk.strip(),\n",
    "                'metadata': {\n",
    "                    'file_path': file_path,\n",
    "                    'section_title': header\n",
    "                }\n",
    "            })\n",
    "    \n",
    "    return processed_chunks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's test the chunking function on one of our Markdown files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing file: pydantic-ai/docs/results.md\n",
      "\n",
      "\n",
      "=== Chunk 1 ===\n",
      "Title: No Header\n",
      "Source: pydantic-ai/docs/results.md\n",
      "\n",
      "Content Preview (first 768 chars):\n",
      "Results are the final values returned from [running an agent](agents.md#running-agents).\n",
      "The result values are wrapped in [`RunResult`][pydantic_ai.result.RunResult] and [`StreamedRunResult`][pydantic_ai.result.StreamedRunResult] so you can access other data like [usage][pydantic_ai.usage.Usage] of the run and [message history](message-history.md#accessing-messages-from-results)\n",
      "\n",
      "Both `RunResult` and `StreamedRunResult` are generic in the data they wrap, so typing information about the data returned by the agent is preserved.\n",
      "\n",
      "```python {title=\"olympics.py\"}\n",
      "from pydantic import BaseModel\n",
      "\n",
      "from pydantic_ai import Agent\n",
      "\n",
      "\n",
      "class CityLocation(BaseModel):\n",
      "    city: str\n",
      "    country: str\n",
      "\n",
      "\n",
      "agent = Agent('gemini-1.5-flash', result_type=CityLocation)\n",
      "result = agent. ...\n",
      "\n",
      "Metadata: {'file_path': 'pydantic-ai/docs/results.md', 'section_title': 'No Header'}\n",
      "==================================================\n",
      "\n",
      "=== Chunk 2 ===\n",
      "Title: Result data {#structured-result-validation}\n",
      "Source: pydantic-ai/docs/results.md\n",
      "\n",
      "Content Preview (first 768 chars):\n",
      "## Result data {#structured-result-validation}\n",
      "\n",
      "When the result type is `str`, or a union including `str`, plain text responses are enabled on the model, and the raw text response from the model is used as the response data.\n",
      "\n",
      "If the result type is a union with multiple members (after remove `str` from the members), each member is registered as a separate tool with the model in order to reduce the complexity of the tool schemas and maximise the chances a model will respond correctly.\n",
      "\n",
      "If the result type schema is not of type `\"object\"`, the result type is wrapped in a single element object, so the schema of all tools registered with the model are object schemas.\n",
      "\n",
      "Structured results (like tools) use Pydantic to build the JSON schema used for the tool, and to v ...\n",
      "\n",
      "Metadata: {'file_path': 'pydantic-ai/docs/results.md', 'section_title': 'Result data {#structured-result-validation}'}\n",
      "==================================================\n",
      "\n",
      "=== Chunk 3 ===\n",
      "Title: Streamed Results\n",
      "Source: pydantic-ai/docs/results.md\n",
      "\n",
      "Content Preview (first 768 chars):\n",
      "## Streamed Results\n",
      "\n",
      "There two main challenges with streamed results:\n",
      "\n",
      "1. Validating structured responses before they're complete, this is achieved by \"partial validation\" which was recently added to Pydantic in [pydantic/pydantic#10748](https://github.com/pydantic/pydantic/pull/10748).\n",
      "2. When receiving a response, we don't know if it's the final response without starting to stream it and peeking at the content. PydanticAI streams just enough of the response to sniff out if it's a tool call or a result, then streams the whole thing and calls tools, or returns the stream as a [`StreamedRunResult`][pydantic_ai.result.StreamedRunResult].\n",
      "\n",
      "### Streaming Text\n",
      "\n",
      "Example of streamed text result:\n",
      "\n",
      "```python {title=\"streamed_hello_world.py\" line_length=\"120\"}\n",
      "from py ...\n",
      "\n",
      "Metadata: {'file_path': 'pydantic-ai/docs/results.md', 'section_title': 'Streamed Results'}\n",
      "==================================================\n",
      "\n",
      "=== Chunk 4 ===\n",
      "Title: Examples\n",
      "Source: pydantic-ai/docs/results.md\n",
      "\n",
      "Content Preview (first 768 chars):\n",
      "## Examples\n",
      "\n",
      "The following examples demonstrate how to use streamed responses in PydanticAI:\n",
      "\n",
      "- [Stream markdown](examples/stream-markdown.md)\n",
      "- [Stream Whales](examples/stream-whales.md) ...\n",
      "\n",
      "Metadata: {'file_path': 'pydantic-ai/docs/results.md', 'section_title': 'Examples'}\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Test on the first markdown file\n",
    "test_file = markdown_files[10]\n",
    "print(f\"Testing file: {test_file}\\n\")\n",
    "\n",
    "# Get chunks for the test file\n",
    "test_chunks = chunk_markdown_file(test_file)\n",
    "\n",
    "# Print each chunk in a readable format\n",
    "for i, chunk in enumerate(test_chunks, 1):\n",
    "    print(f\"\\n=== Chunk {i} ===\")\n",
    "    print(f\"Title: {chunk['title']}\")\n",
    "    print(f\"Source: {chunk['source_file']}\")\n",
    "    print(f\"\\nContent Preview (first 768 chars):\")\n",
    "    print(chunk['content'][:768], \"...\\n\")\n",
    "    print(\"Metadata:\", chunk['metadata'])\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Storing Chunks in the Database\n",
    "\n",
    "Now we'll store these chunks in a PostgreSQL database. We'll use the `psycopg2` library to interact with the database. Make sure you have a database set up and replace the connection parameters with your own.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all markdown files\n",
    "all_chunks = []\n",
    "for file_path in markdown_files:\n",
    "    chunks = chunk_markdown_file(file_path)\n",
    "    all_chunks.extend(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'source_file': 'pydantic-ai/README.md', 'title': 'No Header', 'content': '<div align=\"center\">\\n  <a href=\"https://ai.pydantic.dev/\">\\n    <picture>\\n      <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://ai.pydantic.dev/img/pydantic-ai-dark.svg\">\\n      <img src=\"https://ai.pydantic.dev/img/pydantic-ai-light.svg\" alt=\"PydanticAI\">\\n    </picture>\\n  </a>\\n</div>\\n<div align=\"center\">\\n  <em>Agent Framework / shim to use Pydantic with LLMs</em>\\n</div>\\n<div align=\"center\">\\n  <a href=\"https://github.com/pydantic/pydantic-ai/actions/workflows/ci.yml?query=branch%3Amain\"><img src=\"https://github.com/pydantic/pydantic-ai/actions/workflows/ci.yml/badge.svg?event=push\" alt=\"CI\"></a>\\n  <a href=\"https://coverage-badge.samuelcolvin.workers.dev/redirect/pydantic/pydantic-ai\"><img src=\"https://coverage-badge.samuelcolvin.workers.dev/pydantic/pydantic-ai.svg\" alt=\"Coverage\"></a>\\n  <a href=\"https://pypi.python.org/pypi/pydantic-ai\"><img src=\"https://img.shields.io/pypi/v/pydantic-ai.svg\" alt=\"PyPI\"></a>\\n  <a href=\"https://github.com/pydantic/pydantic-ai\"><img src=\"https://img.shields.io/pypi/pyversions/pydantic-ai.svg\" alt=\"versions\"></a>\\n  <a href=\"https://github.com/pydantic/pydantic-ai/blob/main/LICENSE\"><img src=\"https://img.shields.io/github/license/pydantic/pydantic-ai.svg?v\" alt=\"license\"></a>\\n</div>\\n\\n---\\n\\n**Documentation**: [ai.pydantic.dev](https://ai.pydantic.dev/)\\n\\n---\\n\\nPydanticAI is a Python agent framework designed to make it less painful to build production grade applications with Generative AI.\\n\\nFastAPI revolutionized web development by offering an innovative and ergonomic design, built on the foundation of [Pydantic](https://docs.pydantic.dev).\\n\\nSimilarly, virtually every agent framework and LLM library in Python uses Pydantic, yet when we began to use LLMs in [Pydantic Logfire](https://pydantic.dev/logfire), we couldn\\'t find anything that gave us the same feeling.\\n\\nWe built PydanticAI with one simple aim: to bring that FastAPI feeling to GenAI app development.', 'metadata': {'file_path': 'pydantic-ai/README.md', 'section_title': 'No Header'}}, {'source_file': 'pydantic-ai/README.md', 'title': 'Why use PydanticAI', 'content': \"## Why use PydanticAI\\n\\n* __Built by the Pydantic Team__\\nBuilt by the team behind [Pydantic](https://docs.pydantic.dev/latest/) (the validation layer of the OpenAI SDK, the Anthropic SDK, LangChain, LlamaIndex, AutoGPT, Transformers, CrewAI, Instructor and many more).\\n\\n* __Model-agnostic__\\nSupports OpenAI, Anthropic, Gemini, Ollama, Groq, and Mistral, and there is a simple interface to implement support for [other models](https://ai.pydantic.dev/models/).\\n\\n* __Pydantic Logfire Integration__\\nSeamlessly [integrates](https://ai.pydantic.dev/logfire/) with [Pydantic Logfire](https://pydantic.dev/logfire) for real-time debugging, performance monitoring, and behavior tracking of your LLM-powered applications.\\n\\n* __Type-safe__\\nDesigned to make [type checking](https://ai.pydantic.dev/agents/#static-type-checking) as powerful and informative as possible for you.\\n\\n* __Python-centric Design__\\nLeverages Python's familiar control flow and agent composition to build your AI-driven projects, making it easy to apply standard Python best practices you'd use in any other (non-AI) project.\\n\\n* __Structured Responses__\\nHarnesses the power of [Pydantic](https://docs.pydantic.dev/latest/) to [validate and structure](https://ai.pydantic.dev/results/#structured-result-validation) model outputs, ensuring responses are consistent across runs.\\n\\n* __Dependency Injection System__\\nOffers an optional [dependency injection](https://ai.pydantic.dev/dependencies/) system to provide data and services to your agent's [system prompts](https://ai.pydantic.dev/agents/#system-prompts), [tools](https://ai.pydantic.dev/tools/) and [result validators](https://ai.pydantic.dev/results/#result-validators-functions).\\nThis is useful for testing and eval-driven iterative development.\\n\\n* __Streamed Responses__\\nProvides the ability to [stream](https://ai.pydantic.dev/results/#streamed-results) LLM outputs continuously, with immediate validation, ensuring rapid and accurate results.\\n\\n* __Graph Support__\\n[Pydantic Graph](https://ai.pydantic.dev/graph) provides a powerful way to define graphs using typing hints, this is useful in complex applications where standard control flow can degrade to spaghetti code.\", 'metadata': {'file_path': 'pydantic-ai/README.md', 'section_title': 'Why use PydanticAI'}}, {'source_file': 'pydantic-ai/README.md', 'title': 'In Beta!', 'content': \"## In Beta!\\n\\nPydanticAI is in early beta, the API is still subject to change and there's a lot more to do.\\n[Feedback](https://github.com/pydantic/pydantic-ai/issues) is very welcome!\", 'metadata': {'file_path': 'pydantic-ai/README.md', 'section_title': 'In Beta!'}}, {'source_file': 'pydantic-ai/README.md', 'title': 'Hello World Example', 'content': \"## Hello World Example\\n\\nHere's a minimal example of PydanticAI:\\n\\n```python\\nfrom pydantic_ai import Agent\", 'metadata': {'file_path': 'pydantic-ai/README.md', 'section_title': 'Hello World Example'}}, {'source_file': 'pydantic-ai/README.md', 'title': 'Define a very simple agent including the model to use, you can also set the model when running the agent.', 'content': \"# Define a very simple agent including the model to use, you can also set the model when running the agent.\\nagent = Agent(\\n    'gemini-1.5-flash',\\n    # Register a static system prompt using a keyword argument to the agent.\\n    # For more complex dynamically-generated system prompts, see the example below.\\n    system_prompt='Be concise, reply with one sentence.',\\n)\", 'metadata': {'file_path': 'pydantic-ai/README.md', 'section_title': 'Define a very simple agent including the model to use, you can also set the model when running the agent.'}}, {'source_file': 'pydantic-ai/README.md', 'title': 'Run the agent synchronously, conducting a conversation with the LLM.', 'content': '# Run the agent synchronously, conducting a conversation with the LLM.', 'metadata': {'file_path': 'pydantic-ai/README.md', 'section_title': 'Run the agent synchronously, conducting a conversation with the LLM.'}}, {'source_file': 'pydantic-ai/README.md', 'title': 'Here the exchange should be very short: PydanticAI will send the system prompt and the user query to the LLM,', 'content': '# Here the exchange should be very short: PydanticAI will send the system prompt and the user query to the LLM,', 'metadata': {'file_path': 'pydantic-ai/README.md', 'section_title': 'Here the exchange should be very short: PydanticAI will send the system prompt and the user query to the LLM,'}}, {'source_file': 'pydantic-ai/README.md', 'title': 'the model will return a text response. See below for a more complex run.', 'content': '# the model will return a text response. See below for a more complex run.\\nresult = agent.run_sync(\\'Where does \"hello world\" come from?\\')\\nprint(result.data)\\n\"\"\"\\nThe first known use of \"hello, world\" was in a 1974 textbook about the C programming language.\\n\"\"\"\\n```\\n\\n_(This example is complete, it can be run \"as is\")_\\n\\nNot very interesting yet, but we can easily add \"tools\", dynamic system prompts, and structured responses to build more powerful agents.', 'metadata': {'file_path': 'pydantic-ai/README.md', 'section_title': 'the model will return a text response. See below for a more complex run.'}}, {'source_file': 'pydantic-ai/README.md', 'title': 'Tools & Dependency Injection Example', 'content': '## Tools & Dependency Injection Example\\n\\nHere is a concise example using PydanticAI to build a support agent for a bank:\\n\\n**(Better documented example [in the docs](https://ai.pydantic.dev/#tools-dependency-injection-example))**\\n\\n```python\\nfrom dataclasses import dataclass\\n\\nfrom pydantic import BaseModel, Field\\nfrom pydantic_ai import Agent, RunContext\\n\\nfrom bank_database import DatabaseConn', 'metadata': {'file_path': 'pydantic-ai/README.md', 'section_title': 'Tools & Dependency Injection Example'}}, {'source_file': 'pydantic-ai/README.md', 'title': 'SupportDependencies is used to pass data, connections, and logic into the model that will be needed when running', 'content': '# SupportDependencies is used to pass data, connections, and logic into the model that will be needed when running', 'metadata': {'file_path': 'pydantic-ai/README.md', 'section_title': 'SupportDependencies is used to pass data, connections, and logic into the model that will be needed when running'}}, {'source_file': 'pydantic-ai/README.md', 'title': 'system prompt and tool functions. Dependency injection provides a type-safe way to customise the behavior of your agents.', 'content': '# system prompt and tool functions. Dependency injection provides a type-safe way to customise the behavior of your agents.\\n@dataclass\\nclass SupportDependencies:\\n    customer_id: int\\n    db: DatabaseConn', 'metadata': {'file_path': 'pydantic-ai/README.md', 'section_title': 'system prompt and tool functions. Dependency injection provides a type-safe way to customise the behavior of your agents.'}}, {'source_file': 'pydantic-ai/README.md', 'title': 'This pydantic model defines the structure of the result returned by the agent.', 'content': '# This pydantic model defines the structure of the result returned by the agent.\\nclass SupportResult(BaseModel):\\n    support_advice: str = Field(description=\\'Advice returned to the customer\\')\\n    block_card: bool = Field(description=\"Whether to block the customer\\'s card\")\\n    risk: int = Field(description=\\'Risk level of query\\', ge=0, le=10)', 'metadata': {'file_path': 'pydantic-ai/README.md', 'section_title': 'This pydantic model defines the structure of the result returned by the agent.'}}, {'source_file': 'pydantic-ai/README.md', 'title': 'This agent will act as first-tier support in a bank.', 'content': '# This agent will act as first-tier support in a bank.', 'metadata': {'file_path': 'pydantic-ai/README.md', 'section_title': 'This agent will act as first-tier support in a bank.'}}, {'source_file': 'pydantic-ai/README.md', 'title': 'Agents are generic in the type of dependencies they accept and the type of result they return.', 'content': '# Agents are generic in the type of dependencies they accept and the type of result they return.', 'metadata': {'file_path': 'pydantic-ai/README.md', 'section_title': 'Agents are generic in the type of dependencies they accept and the type of result they return.'}}, {'source_file': 'pydantic-ai/README.md', 'title': 'In this case, the support agent has type `Agent[SupportDependencies, SupportResult]`.', 'content': \"# In this case, the support agent has type `Agent[SupportDependencies, SupportResult]`.\\nsupport_agent = Agent(\\n    'openai:gpt-4o',\\n    deps_type=SupportDependencies,\\n    # The response from the agent will, be guaranteed to be a SupportResult,\\n    # if validation fails the agent is prompted to try again.\\n    result_type=SupportResult,\\n    system_prompt=(\\n        'You are a support agent in our bank, give the '\\n        'customer support and judge the risk level of their query.'\\n    ),\\n)\", 'metadata': {'file_path': 'pydantic-ai/README.md', 'section_title': 'In this case, the support agent has type `Agent[SupportDependencies, SupportResult]`.'}}, {'source_file': 'pydantic-ai/README.md', 'title': 'Dynamic system prompts can make use of dependency injection.', 'content': '# Dynamic system prompts can make use of dependency injection.', 'metadata': {'file_path': 'pydantic-ai/README.md', 'section_title': 'Dynamic system prompts can make use of dependency injection.'}}, {'source_file': 'pydantic-ai/README.md', 'title': 'Dependencies are carried via the `RunContext` argument, which is parameterized with the `deps_type` from above.', 'content': '# Dependencies are carried via the `RunContext` argument, which is parameterized with the `deps_type` from above.', 'metadata': {'file_path': 'pydantic-ai/README.md', 'section_title': 'Dependencies are carried via the `RunContext` argument, which is parameterized with the `deps_type` from above.'}}, {'source_file': 'pydantic-ai/README.md', 'title': 'If the type annotation here is wrong, static type checkers will catch it.', 'content': '# If the type annotation here is wrong, static type checkers will catch it.\\n@support_agent.system_prompt\\nasync def add_customer_name(ctx: RunContext[SupportDependencies]) -> str:\\n    customer_name = await ctx.deps.db.customer_name(id=ctx.deps.customer_id)\\n    return f\"The customer\\'s name is {customer_name!r}\"', 'metadata': {'file_path': 'pydantic-ai/README.md', 'section_title': 'If the type annotation here is wrong, static type checkers will catch it.'}}, {'source_file': 'pydantic-ai/README.md', 'title': '`tool` let you register functions which the LLM may call while responding to a user.', 'content': '# `tool` let you register functions which the LLM may call while responding to a user.', 'metadata': {'file_path': 'pydantic-ai/README.md', 'section_title': '`tool` let you register functions which the LLM may call while responding to a user.'}}, {'source_file': 'pydantic-ai/README.md', 'title': 'Again, dependencies are carried via `RunContext`, any other arguments become the tool schema passed to the LLM.', 'content': '# Again, dependencies are carried via `RunContext`, any other arguments become the tool schema passed to the LLM.', 'metadata': {'file_path': 'pydantic-ai/README.md', 'section_title': 'Again, dependencies are carried via `RunContext`, any other arguments become the tool schema passed to the LLM.'}}, {'source_file': 'pydantic-ai/README.md', 'title': 'Pydantic is used to validate these arguments, and errors are passed back to the LLM so it can retry.', 'content': '# Pydantic is used to validate these arguments, and errors are passed back to the LLM so it can retry.\\n@support_agent.tool\\nasync def customer_balance(\\n    ctx: RunContext[SupportDependencies], include_pending: bool\\n) -> float:\\n    \"\"\"Returns the customer\\'s current account balance.\"\"\"\\n    # The docstring of a tool is also passed to the LLM as the description of the tool.\\n    # Parameter descriptions are extracted from the docstring and added to the parameter schema sent to the LLM.\\n    balance = await ctx.deps.db.customer_balance(\\n        id=ctx.deps.customer_id,\\n        include_pending=include_pending,\\n    )\\n    return balance\\n\\n\\n...  # In a real use case, you\\'d add more tools and a longer system prompt\\n\\n\\nasync def main():\\n    deps = SupportDependencies(customer_id=123, db=DatabaseConn())\\n    # Run the agent asynchronously, conducting a conversation with the LLM until a final response is reached.\\n    # Even in this fairly simple case, the agent will exchange multiple messages with the LLM as tools are called to retrieve a result.\\n    result = await support_agent.run(\\'What is my balance?\\', deps=deps)\\n    # The result will be validated with Pydantic to guarantee it is a `SupportResult`, since the agent is generic,\\n    # it\\'ll also be typed as a `SupportResult` to aid with static type checking.\\n    print(result.data)\\n    \"\"\"\\n    support_advice=\\'Hello John, your current account balance, including pending transactions, is $123.45.\\' block_card=False risk=1\\n    \"\"\"\\n\\n    result = await support_agent.run(\\'I just lost my card!\\', deps=deps)\\n    print(result.data)\\n    \"\"\"\\n    support_advice=\"I\\'m sorry to hear that, John. We are temporarily blocking your card to prevent unauthorized transactions.\" block_card=True risk=8\\n    \"\"\"\\n```', 'metadata': {'file_path': 'pydantic-ai/README.md', 'section_title': 'Pydantic is used to validate these arguments, and errors are passed back to the LLM so it can retry.'}}, {'source_file': 'pydantic-ai/README.md', 'title': 'Next Steps', 'content': \"## Next Steps\\n\\nTo try PydanticAI yourself, follow the instructions [in the examples](https://ai.pydantic.dev/examples/).\\n\\nRead the [docs](https://ai.pydantic.dev/agents/) to learn more about building applications with PydanticAI.\\n\\nRead the [API Reference](https://ai.pydantic.dev/api/agent/) to understand PydanticAI's interface.\", 'metadata': {'file_path': 'pydantic-ai/README.md', 'section_title': 'Next Steps'}}, {'source_file': 'pydantic-ai/pydantic_ai_slim/README.md', 'title': 'PydanticAI Slim', 'content': '# PydanticAI Slim\\n\\n[![CI](https://github.com/pydantic/pydantic-ai/actions/workflows/ci.yml/badge.svg?event=push)](https://github.com/pydantic/pydantic-ai/actions/workflows/ci.yml?query=branch%3Amain)\\n[![Coverage](https://coverage-badge.samuelcolvin.workers.dev/pydantic/pydantic-ai.svg)](https://coverage-badge.samuelcolvin.workers.dev/redirect/pydantic/pydantic-ai)\\n[![PyPI](https://img.shields.io/pypi/v/pydantic-ai.svg)](https://pypi.python.org/pypi/pydantic-ai)\\n[![versions](https://img.shields.io/pypi/pyversions/pydantic-ai.svg)](https://github.com/pydantic/pydantic-ai)\\n[![license](https://img.shields.io/github/license/pydantic/pydantic-ai.svg?v)](https://github.com/pydantic/pydantic-ai/blob/main/LICENSE)\\n\\nPydanticAI core logic with minimal required dependencies.\\n\\nFor more information on how to use this package see [ai.pydantic.dev/install](https://ai.pydantic.dev/install/).', 'metadata': {'file_path': 'pydantic-ai/pydantic_ai_slim/README.md', 'section_title': 'PydanticAI Slim'}}, {'source_file': 'pydantic-ai/tests/example_modules/README.md', 'title': 'docs examples imports', 'content': '# docs examples imports\\n\\nThis directory is added to `sys.path` in `tests/test_examples.py::test_docs_examples` to augment some of the examples.', 'metadata': {'file_path': 'pydantic-ai/tests/example_modules/README.md', 'section_title': 'docs examples imports'}}, {'source_file': 'pydantic-ai/docs/troubleshooting.md', 'title': 'Troubleshooting', 'content': \"# Troubleshooting\\n\\nBelow are suggestions on how to fix some common errors you might encounter while using PydanticAI. If the issue you're experiencing is not listed below or addressed in the documentation, please feel free to ask in the [Pydantic Slack](help.md) or create an issue on [GitHub](https://github.com/pydantic/pydantic-ai/issues).\", 'metadata': {'file_path': 'pydantic-ai/docs/troubleshooting.md', 'section_title': 'Troubleshooting'}}, {'source_file': 'pydantic-ai/docs/troubleshooting.md', 'title': 'Jupyter Notebook Errors', 'content': '## Jupyter Notebook Errors\\n\\n### `RuntimeError: This event loop is already running`\\n\\nThis error is caused by conflicts between the event loops in Jupyter notebook and PydanticAI\\'s. One way to manage these conflicts is by using [`nest-asyncio`](https://pypi.org/project/nest-asyncio/). Namely, before you execute any agent runs, do the following:\\n```python {test=\"skip\"}\\nimport nest_asyncio\\n\\nnest_asyncio.apply()\\n```\\nNote: This fix also applies to Google Colab.', 'metadata': {'file_path': 'pydantic-ai/docs/troubleshooting.md', 'section_title': 'Jupyter Notebook Errors'}}, {'source_file': 'pydantic-ai/docs/troubleshooting.md', 'title': 'API Key Configuration', 'content': \"## API Key Configuration\\n\\n### `UserError: API key must be provided or set in the [MODEL]_API_KEY environment variable`\\n\\nIf you're running into issues with setting the API key for your model, visit the [Models](models.md) page to learn more about how to set an environment variable and/or pass in an `api_key` argument.\", 'metadata': {'file_path': 'pydantic-ai/docs/troubleshooting.md', 'section_title': 'API Key Configuration'}}, {'source_file': 'pydantic-ai/docs/dependencies.md', 'title': 'Dependencies', 'content': '# Dependencies\\n\\nPydanticAI uses a dependency injection system to provide data and services to your agent\\'s [system prompts](agents.md#system-prompts), [tools](tools.md) and [result validators](results.md#result-validators-functions).\\n\\nMatching PydanticAI\\'s design philosophy, our dependency system tries to use existing best practice in Python development rather than inventing esoteric \"magic\", this should make dependencies type-safe, understandable easier to test and ultimately easier to deploy in production.', 'metadata': {'file_path': 'pydantic-ai/docs/dependencies.md', 'section_title': 'Dependencies'}}, {'source_file': 'pydantic-ai/docs/dependencies.md', 'title': 'Defining Dependencies', 'content': '## Defining Dependencies\\n\\nDependencies can be any python type. While in simple cases you might be able to pass a single object as a dependency (e.g. an HTTP connection), [dataclasses][] are generally a convenient container when your dependencies included multiple objects.\\n\\nHere\\'s an example of defining an agent that requires dependencies.\\n\\n(**Note:** dependencies aren\\'t actually used in this example, see [Accessing Dependencies](#accessing-dependencies) below)\\n\\n```python {title=\"unused_dependencies.py\"}\\nfrom dataclasses import dataclass\\n\\nimport httpx\\n\\nfrom pydantic_ai import Agent\\n\\n\\n@dataclass\\nclass MyDeps:  # (1)!\\n    api_key: str\\n    http_client: httpx.AsyncClient\\n\\n\\nagent = Agent(\\n    \\'openai:gpt-4o\\',\\n    deps_type=MyDeps,  # (2)!\\n)\\n\\n\\nasync def main():\\n    async with httpx.AsyncClient() as client:\\n        deps = MyDeps(\\'foobar\\', client)\\n        result = await agent.run(\\n            \\'Tell me a joke.\\',\\n            deps=deps,  # (3)!\\n        )\\n        print(result.data)\\n        #> Did you hear about the toothpaste scandal? They called it Colgate.\\n```\\n\\n1. Define a dataclass to hold dependencies.\\n2. Pass the dataclass type to the `deps_type` argument of the [`Agent` constructor][pydantic_ai.Agent.__init__]. **Note**: we\\'re passing the type here, NOT an instance, this parameter is not actually used at runtime, it\\'s here so we can get full type checking of the agent.\\n3. When running the agent, pass an instance of the dataclass to the `deps` parameter.\\n\\n_(This example is complete, it can be run \"as is\" — you\\'ll need to add `asyncio.run(main())` to run `main`)_', 'metadata': {'file_path': 'pydantic-ai/docs/dependencies.md', 'section_title': 'Defining Dependencies'}}, {'source_file': 'pydantic-ai/docs/dependencies.md', 'title': 'Accessing Dependencies', 'content': '## Accessing Dependencies\\n\\nDependencies are accessed through the [`RunContext`][pydantic_ai.tools.RunContext] type, this should be the first parameter of system prompt functions etc.\\n\\n\\n```python {title=\"system_prompt_dependencies.py\" hl_lines=\"20-27\"}\\nfrom dataclasses import dataclass\\n\\nimport httpx\\n\\nfrom pydantic_ai import Agent, RunContext\\n\\n\\n@dataclass\\nclass MyDeps:\\n    api_key: str\\n    http_client: httpx.AsyncClient\\n\\n\\nagent = Agent(\\n    \\'openai:gpt-4o\\',\\n    deps_type=MyDeps,\\n)\\n\\n\\n@agent.system_prompt  # (1)!\\nasync def get_system_prompt(ctx: RunContext[MyDeps]) -> str:  # (2)!\\n    response = await ctx.deps.http_client.get(  # (3)!\\n        \\'https://example.com\\',\\n        headers={\\'Authorization\\': f\\'Bearer {ctx.deps.api_key}\\'},  # (4)!\\n    )\\n    response.raise_for_status()\\n    return f\\'Prompt: {response.text}\\'\\n\\n\\nasync def main():\\n    async with httpx.AsyncClient() as client:\\n        deps = MyDeps(\\'foobar\\', client)\\n        result = await agent.run(\\'Tell me a joke.\\', deps=deps)\\n        print(result.data)\\n        #> Did you hear about the toothpaste scandal? They called it Colgate.\\n```\\n\\n1. [`RunContext`][pydantic_ai.tools.RunContext] may optionally be passed to a [`system_prompt`][pydantic_ai.Agent.system_prompt] function as the only argument.\\n2. [`RunContext`][pydantic_ai.tools.RunContext] is parameterized with the type of the dependencies, if this type is incorrect, static type checkers will raise an error.\\n3. Access dependencies through the [`.deps`][pydantic_ai.tools.RunContext.deps] attribute.\\n4. Access dependencies through the [`.deps`][pydantic_ai.tools.RunContext.deps] attribute.\\n\\n_(This example is complete, it can be run \"as is\" — you\\'ll need to add `asyncio.run(main())` to run `main`)_\\n\\n### Asynchronous vs. Synchronous dependencies\\n\\n[System prompt functions](agents.md#system-prompts), [function tools](tools.md) and [result validators](results.md#result-validators-functions) are all run in the async context of an agent run.\\n\\nIf these functions are not coroutines (e.g. `async def`) they are called with\\n[`run_in_executor`][asyncio.loop.run_in_executor] in a thread pool, it\\'s therefore marginally preferable\\nto use `async` methods where dependencies perform IO, although synchronous dependencies should work fine too.\\n\\n!!! note \"`run` vs. `run_sync` and Asynchronous vs. Synchronous dependencies\"\\n    Whether you use synchronous or asynchronous dependencies, is completely independent of whether you use `run` or `run_sync` — `run_sync` is just a wrapper around `run` and agents are always run in an async context.\\n\\nHere\\'s the same example as above, but with a synchronous dependency:\\n\\n```python {title=\"sync_dependencies.py\"}\\nfrom dataclasses import dataclass\\n\\nimport httpx\\n\\nfrom pydantic_ai import Agent, RunContext\\n\\n\\n@dataclass\\nclass MyDeps:\\n    api_key: str\\n    http_client: httpx.Client  # (1)!\\n\\n\\nagent = Agent(\\n    \\'openai:gpt-4o\\',\\n    deps_type=MyDeps,\\n)\\n\\n\\n@agent.system_prompt\\ndef get_system_prompt(ctx: RunContext[MyDeps]) -> str:  # (2)!\\n    response = ctx.deps.http_client.get(\\n        \\'https://example.com\\', headers={\\'Authorization\\': f\\'Bearer {ctx.deps.api_key}\\'}\\n    )\\n    response.raise_for_status()\\n    return f\\'Prompt: {response.text}\\'\\n\\n\\nasync def main():\\n    deps = MyDeps(\\'foobar\\', httpx.Client())\\n    result = await agent.run(\\n        \\'Tell me a joke.\\',\\n        deps=deps,\\n    )\\n    print(result.data)\\n    #> Did you hear about the toothpaste scandal? They called it Colgate.\\n```\\n\\n1. Here we use a synchronous `httpx.Client` instead of an asynchronous `httpx.AsyncClient`.\\n2. To match the synchronous dependency, the system prompt function is now a plain function, not a coroutine.\\n\\n_(This example is complete, it can be run \"as is\" — you\\'ll need to add `asyncio.run(main())` to run `main`)_', 'metadata': {'file_path': 'pydantic-ai/docs/dependencies.md', 'section_title': 'Accessing Dependencies'}}, {'source_file': 'pydantic-ai/docs/dependencies.md', 'title': 'Full Example', 'content': '## Full Example\\n\\nAs well as system prompts, dependencies can be used in [tools](tools.md) and [result validators](results.md#result-validators-functions).\\n\\n```python {title=\"full_example.py\" hl_lines=\"27-35 38-48\"}\\nfrom dataclasses import dataclass\\n\\nimport httpx\\n\\nfrom pydantic_ai import Agent, ModelRetry, RunContext\\n\\n\\n@dataclass\\nclass MyDeps:\\n    api_key: str\\n    http_client: httpx.AsyncClient\\n\\n\\nagent = Agent(\\n    \\'openai:gpt-4o\\',\\n    deps_type=MyDeps,\\n)\\n\\n\\n@agent.system_prompt\\nasync def get_system_prompt(ctx: RunContext[MyDeps]) -> str:\\n    response = await ctx.deps.http_client.get(\\'https://example.com\\')\\n    response.raise_for_status()\\n    return f\\'Prompt: {response.text}\\'\\n\\n\\n@agent.tool  # (1)!\\nasync def get_joke_material(ctx: RunContext[MyDeps], subject: str) -> str:\\n    response = await ctx.deps.http_client.get(\\n        \\'https://example.com#jokes\\',\\n        params={\\'subject\\': subject},\\n        headers={\\'Authorization\\': f\\'Bearer {ctx.deps.api_key}\\'},\\n    )\\n    response.raise_for_status()\\n    return response.text\\n\\n\\n@agent.result_validator  # (2)!\\nasync def validate_result(ctx: RunContext[MyDeps], final_response: str) -> str:\\n    response = await ctx.deps.http_client.post(\\n        \\'https://example.com#validate\\',\\n        headers={\\'Authorization\\': f\\'Bearer {ctx.deps.api_key}\\'},\\n        params={\\'query\\': final_response},\\n    )\\n    if response.status_code == 400:\\n        raise ModelRetry(f\\'invalid response: {response.text}\\')\\n    response.raise_for_status()\\n    return final_response\\n\\n\\nasync def main():\\n    async with httpx.AsyncClient() as client:\\n        deps = MyDeps(\\'foobar\\', client)\\n        result = await agent.run(\\'Tell me a joke.\\', deps=deps)\\n        print(result.data)\\n        #> Did you hear about the toothpaste scandal? They called it Colgate.\\n```\\n\\n1. To pass `RunContext` to a tool, use the [`tool`][pydantic_ai.Agent.tool] decorator.\\n2. `RunContext` may optionally be passed to a [`result_validator`][pydantic_ai.Agent.result_validator] function as the first argument.\\n\\n_(This example is complete, it can be run \"as is\" — you\\'ll need to add `asyncio.run(main())` to run `main`)_', 'metadata': {'file_path': 'pydantic-ai/docs/dependencies.md', 'section_title': 'Full Example'}}, {'source_file': 'pydantic-ai/docs/dependencies.md', 'title': 'Overriding Dependencies', 'content': '## Overriding Dependencies\\n\\nWhen testing agents, it\\'s useful to be able to customise dependencies.\\n\\nWhile this can sometimes be done by calling the agent directly within unit tests, we can also override dependencies\\nwhile calling application code which in turn calls the agent.\\n\\nThis is done via the [`override`][pydantic_ai.Agent.override] method on the agent.\\n\\n```python {title=\"joke_app.py\"}\\nfrom dataclasses import dataclass\\n\\nimport httpx\\n\\nfrom pydantic_ai import Agent, RunContext\\n\\n\\n@dataclass\\nclass MyDeps:\\n    api_key: str\\n    http_client: httpx.AsyncClient\\n\\n    async def system_prompt_factory(self) -> str:  # (1)!\\n        response = await self.http_client.get(\\'https://example.com\\')\\n        response.raise_for_status()\\n        return f\\'Prompt: {response.text}\\'\\n\\n\\njoke_agent = Agent(\\'openai:gpt-4o\\', deps_type=MyDeps)\\n\\n\\n@joke_agent.system_prompt\\nasync def get_system_prompt(ctx: RunContext[MyDeps]) -> str:\\n    return await ctx.deps.system_prompt_factory()  # (2)!\\n\\n\\nasync def application_code(prompt: str) -> str:  # (3)!\\n    ...\\n    ...\\n    # now deep within application code we call our agent\\n    async with httpx.AsyncClient() as client:\\n        app_deps = MyDeps(\\'foobar\\', client)\\n        result = await joke_agent.run(prompt, deps=app_deps)  # (4)!\\n    return result.data\\n```\\n\\n1. Define a method on the dependency to make the system prompt easier to customise.\\n2. Call the system prompt factory from within the system prompt function.\\n3. Application code that calls the agent, in a real application this might be an API endpoint.\\n4. Call the agent from within the application code, in a real application this call might be deep within a call stack. Note `app_deps` here will NOT be used when deps are overridden.\\n\\n_(This example is complete, it can be run \"as is\")_\\n\\n```python {title=\"test_joke_app.py\" hl_lines=\"10-12\" call_name=\"test_application_code\"}\\nfrom joke_app import MyDeps, application_code, joke_agent\\n\\n\\nclass TestMyDeps(MyDeps):  # (1)!\\n    async def system_prompt_factory(self) -> str:\\n        return \\'test prompt\\'\\n\\n\\nasync def test_application_code():\\n    test_deps = TestMyDeps(\\'test_key\\', None)  # (2)!\\n    with joke_agent.override(deps=test_deps):  # (3)!\\n        joke = await application_code(\\'Tell me a joke.\\')  # (4)!\\n    assert joke.startswith(\\'Did you hear about the toothpaste scandal?\\')\\n```\\n\\n1. Define a subclass of `MyDeps` in tests to customise the system prompt factory.\\n2. Create an instance of the test dependency, we don\\'t need to pass an `http_client` here as it\\'s not used.\\n3. Override the dependencies of the agent for the duration of the `with` block, `test_deps` will be used when the agent is run.\\n4. Now we can safely call our application code, the agent will use the overridden dependencies.', 'metadata': {'file_path': 'pydantic-ai/docs/dependencies.md', 'section_title': 'Overriding Dependencies'}}, {'source_file': 'pydantic-ai/docs/dependencies.md', 'title': 'Examples', 'content': '## Examples\\n\\nThe following examples demonstrate how to use dependencies in PydanticAI:\\n\\n- [Weather Agent](examples/weather-agent.md)\\n- [SQL Generation](examples/sql-gen.md)\\n- [RAG](examples/rag.md)', 'metadata': {'file_path': 'pydantic-ai/docs/dependencies.md', 'section_title': 'Examples'}}, {'source_file': 'pydantic-ai/docs/install.md', 'title': 'Installation', 'content': '# Installation\\n\\nPydanticAI is available on PyPI as [`pydantic-ai`](https://pypi.org/project/pydantic-ai/) so installation is as simple as:\\n\\n```bash\\npip/uv-add pydantic-ai\\n```\\n\\n(Requires Python 3.9+)\\n\\nThis installs the `pydantic_ai` package, core dependencies, and libraries required to use all the models\\nincluded in PydanticAI. If you want to use a specific model, you can install the [\"slim\"](#slim-install) version of PydanticAI.', 'metadata': {'file_path': 'pydantic-ai/docs/install.md', 'section_title': 'Installation'}}, {'source_file': 'pydantic-ai/docs/install.md', 'title': 'Use with Pydantic Logfire', 'content': \"## Use with Pydantic Logfire\\n\\nPydanticAI has an excellent (but completely optional) integration with [Pydantic Logfire](https://pydantic.dev/logfire) to help you view and understand agent runs.\\n\\nTo use Logfire with PydanticAI, install `pydantic-ai` or `pydantic-ai-slim` with the `logfire` optional group:\\n\\n```bash\\npip/uv-add 'pydantic-ai[logfire]'\\n```\\n\\nFrom there, follow the [Logfire setup docs](logfire.md#using-logfire) to configure Logfire.\", 'metadata': {'file_path': 'pydantic-ai/docs/install.md', 'section_title': 'Use with Pydantic Logfire'}}, {'source_file': 'pydantic-ai/docs/install.md', 'title': 'Running Examples', 'content': \"## Running Examples\\n\\nWe distribute the [`pydantic_ai_examples`](https://github.com/pydantic/pydantic-ai/tree/main/pydantic_ai_examples) directory as a separate PyPI package ([`pydantic-ai-examples`](https://pypi.org/project/pydantic-ai-examples/)) to make examples extremely easy to customize and run.\\n\\nTo install examples, use the `examples` optional group:\\n\\n```bash\\npip/uv-add 'pydantic-ai[examples]'\\n```\\n\\nTo run the examples, follow instructions in the [examples docs](examples/index.md).\", 'metadata': {'file_path': 'pydantic-ai/docs/install.md', 'section_title': 'Running Examples'}}, {'source_file': 'pydantic-ai/docs/install.md', 'title': 'Slim Install', 'content': '## Slim Install\\n\\nIf you know which model you\\'re going to use and want to avoid installing superfluous packages, you can use the [`pydantic-ai-slim`](https://pypi.org/project/pydantic-ai-slim/) package.\\nFor example, if you\\'re using just [`OpenAIModel`][pydantic_ai.models.openai.OpenAIModel], you would run:\\n\\n```bash\\npip/uv-add \\'pydantic-ai-slim[openai]\\'\\n```\\n\\n`pydantic-ai-slim` has the following optional groups:\\n\\n* `logfire` — installs [`logfire`](logfire.md) [PyPI ↗](https://pypi.org/project/logfire){:target=\"_blank\"}\\n* `graph` - installs [`pydantic-graph`](graph.md) [PyPI ↗](https://pypi.org/project/pydantic-graph){:target=\"_blank\"}\\n* `openai` — installs `openai` [PyPI ↗](https://pypi.org/project/openai){:target=\"_blank\"}\\n* `vertexai` — installs `google-auth` [PyPI ↗](https://pypi.org/project/google-auth){:target=\"_blank\"} and `requests` [PyPI ↗](https://pypi.org/project/requests){:target=\"_blank\"}\\n* `anthropic` — installs `anthropic` [PyPI ↗](https://pypi.org/project/anthropic){:target=\"_blank\"}\\n* `groq` — installs `groq` [PyPI ↗](https://pypi.org/project/groq){:target=\"_blank\"}\\n* `mistral` — installs `mistralai` [PyPI ↗](https://pypi.org/project/mistralai){:target=\"_blank\"}\\n\\nSee the [models](models.md) documentation for information on which optional dependencies are required for each model.\\n\\nYou can also install dependencies for multiple models and use cases, for example:\\n\\n```bash\\npip/uv-add \\'pydantic-ai-slim[openai,vertexai,logfire]\\'\\n```', 'metadata': {'file_path': 'pydantic-ai/docs/install.md', 'section_title': 'Slim Install'}}, {'source_file': 'pydantic-ai/docs/help.md', 'title': 'Getting Help', 'content': '# Getting Help\\n\\nIf you need help getting started with PydanticAI or with advanced usage, the following sources may be useful.', 'metadata': {'file_path': 'pydantic-ai/docs/help.md', 'section_title': 'Getting Help'}}, {'source_file': 'pydantic-ai/docs/help.md', 'title': ':simple-slack: Slack', 'content': \"## :simple-slack: Slack\\n\\nJoin the `#pydantic-ai` channel in the [Pydantic Slack][slack] to ask questions, get help, and chat about PydanticAI. There's also channels for Pydantic, Logfire, and FastUI.\\n\\nIf you're on a [Logfire][logfire] Pro plan, you can also get a dedicated private slack collab channel with us.\", 'metadata': {'file_path': 'pydantic-ai/docs/help.md', 'section_title': ':simple-slack: Slack'}}, {'source_file': 'pydantic-ai/docs/help.md', 'title': ':simple-github: GitHub Issues', 'content': '## :simple-github: GitHub Issues\\n\\nThe [PydanticAI GitHub Issues][github-issues] are a great place to ask questions and give us feedback.\\n\\n[slack]: https://join.slack.com/t/pydanticlogfire/shared_invite/zt-2war8jrjq-w_nWG6ZX7Zm~gnzY7cXSog\\n[github-issues]: https://github.com/pydantic/pydantic-ai/issues\\n[logfire]: https://pydantic.dev/logfire', 'metadata': {'file_path': 'pydantic-ai/docs/help.md', 'section_title': ':simple-github: GitHub Issues'}}, {'source_file': 'pydantic-ai/docs/message-history.md', 'title': 'Messages and chat history', 'content': '# Messages and chat history\\n\\nPydanticAI provides access to messages exchanged during an agent run. These messages can be used both to continue a coherent conversation, and to understand how an agent performed.\\n\\n### Accessing Messages from Results\\n\\nAfter running an agent, you can access the messages exchanged during that run from the `result` object.\\n\\nBoth [`RunResult`][pydantic_ai.result.RunResult]\\n(returned by [`Agent.run`][pydantic_ai.Agent.run], [`Agent.run_sync`][pydantic_ai.Agent.run_sync])\\nand [`StreamedRunResult`][pydantic_ai.result.StreamedRunResult] (returned by [`Agent.run_stream`][pydantic_ai.Agent.run_stream]) have the following methods:\\n\\n* [`all_messages()`][pydantic_ai.result.RunResult.all_messages]: returns all messages, including messages from prior runs. There\\'s also a variant that returns JSON bytes, [`all_messages_json()`][pydantic_ai.result.RunResult.all_messages_json].\\n* [`new_messages()`][pydantic_ai.result.RunResult.new_messages]: returns only the messages from the current run. There\\'s also a variant that returns JSON bytes, [`new_messages_json()`][pydantic_ai.result.RunResult.new_messages_json].\\n\\n!!! info \"StreamedRunResult and complete messages\"\\n    On [`StreamedRunResult`][pydantic_ai.result.StreamedRunResult], the messages returned from these methods will only include the final result message once the stream has finished.\\n\\n    E.g. you\\'ve awaited one of the following coroutines:\\n\\n    * [`StreamedRunResult.stream()`][pydantic_ai.result.StreamedRunResult.stream]\\n    * [`StreamedRunResult.stream_text()`][pydantic_ai.result.StreamedRunResult.stream_text]\\n    * [`StreamedRunResult.stream_structured()`][pydantic_ai.result.StreamedRunResult.stream_structured]\\n    * [`StreamedRunResult.get_data()`][pydantic_ai.result.StreamedRunResult.get_data]\\n\\n    **Note:** The final result message will NOT be added to result messages if you use [`.stream_text(delta=True)`][pydantic_ai.result.StreamedRunResult.stream_text] since in this case the result content is never built as one string.\\n\\nExample of accessing methods on a [`RunResult`][pydantic_ai.result.RunResult] :\\n\\n```python {title=\"run_result_messages.py\" hl_lines=\"10 28\"}\\nfrom pydantic_ai import Agent\\n\\nagent = Agent(\\'openai:gpt-4o\\', system_prompt=\\'Be a helpful assistant.\\')\\n\\nresult = agent.run_sync(\\'Tell me a joke.\\')\\nprint(result.data)\\n#> Did you hear about the toothpaste scandal? They called it Colgate.', 'metadata': {'file_path': 'pydantic-ai/docs/message-history.md', 'section_title': 'Messages and chat history'}}, {'source_file': 'pydantic-ai/docs/message-history.md', 'title': 'all messages from the run', 'content': '# all messages from the run\\nprint(result.all_messages())\\n\"\"\"\\n[\\n    ModelRequest(\\n        parts=[\\n            SystemPromptPart(\\n                content=\\'Be a helpful assistant.\\',\\n                dynamic_ref=None,\\n                part_kind=\\'system-prompt\\',\\n            ),\\n            UserPromptPart(\\n                content=\\'Tell me a joke.\\',\\n                timestamp=datetime.datetime(...),\\n                part_kind=\\'user-prompt\\',\\n            ),\\n        ],\\n        kind=\\'request\\',\\n    ),\\n    ModelResponse(\\n        parts=[\\n            TextPart(\\n                content=\\'Did you hear about the toothpaste scandal? They called it Colgate.\\',\\n                part_kind=\\'text\\',\\n            )\\n        ],\\n        timestamp=datetime.datetime(...),\\n        kind=\\'response\\',\\n    ),\\n]\\n\"\"\"\\n```\\n_(This example is complete, it can be run \"as is\")_\\n\\nExample of accessing methods on a [`StreamedRunResult`][pydantic_ai.result.StreamedRunResult] :\\n\\n```python {title=\"streamed_run_result_messages.py\" hl_lines=\"9 31\"}\\nfrom pydantic_ai import Agent\\n\\nagent = Agent(\\'openai:gpt-4o\\', system_prompt=\\'Be a helpful assistant.\\')\\n\\n\\nasync def main():\\n    async with agent.run_stream(\\'Tell me a joke.\\') as result:\\n        # incomplete messages before the stream finishes\\n        print(result.all_messages())\\n        \"\"\"\\n        [\\n            ModelRequest(\\n                parts=[\\n                    SystemPromptPart(\\n                        content=\\'Be a helpful assistant.\\',\\n                        dynamic_ref=None,\\n                        part_kind=\\'system-prompt\\',\\n                    ),\\n                    UserPromptPart(\\n                        content=\\'Tell me a joke.\\',\\n                        timestamp=datetime.datetime(...),\\n                        part_kind=\\'user-prompt\\',\\n                    ),\\n                ],\\n                kind=\\'request\\',\\n            )\\n        ]\\n        \"\"\"\\n\\n        async for text in result.stream_text():\\n            print(text)\\n            #> Did you hear\\n            #> Did you hear about the toothpaste\\n            #> Did you hear about the toothpaste scandal? They called\\n            #> Did you hear about the toothpaste scandal? They called it Colgate.\\n\\n        # complete messages once the stream finishes\\n        print(result.all_messages())\\n        \"\"\"\\n        [\\n            ModelRequest(\\n                parts=[\\n                    SystemPromptPart(\\n                        content=\\'Be a helpful assistant.\\',\\n                        dynamic_ref=None,\\n                        part_kind=\\'system-prompt\\',\\n                    ),\\n                    UserPromptPart(\\n                        content=\\'Tell me a joke.\\',\\n                        timestamp=datetime.datetime(...),\\n                        part_kind=\\'user-prompt\\',\\n                    ),\\n                ],\\n                kind=\\'request\\',\\n            ),\\n            ModelResponse(\\n                parts=[\\n                    TextPart(\\n                        content=\\'Did you hear about the toothpaste scandal? They called it Colgate.\\',\\n                        part_kind=\\'text\\',\\n                    )\\n                ],\\n                timestamp=datetime.datetime(...),\\n                kind=\\'response\\',\\n            ),\\n        ]\\n        \"\"\"\\n```\\n_(This example is complete, it can be run \"as is\" — you\\'ll need to add `asyncio.run(main())` to run `main`)_\\n\\n### Using Messages as Input for Further Agent Runs\\n\\nThe primary use of message histories in PydanticAI is to maintain context across multiple agent runs.\\n\\nTo use existing messages in a run, pass them to the `message_history` parameter of\\n[`Agent.run`][pydantic_ai.Agent.run], [`Agent.run_sync`][pydantic_ai.Agent.run_sync] or\\n[`Agent.run_stream`][pydantic_ai.Agent.run_stream].\\n\\nIf `message_history` is set and not empty, a new system prompt is not generated — we assume the existing message history includes a system prompt.\\n\\n```python {title=\"Reusing messages in a conversation\" hl_lines=\"9 13\"}\\nfrom pydantic_ai import Agent\\n\\nagent = Agent(\\'openai:gpt-4o\\', system_prompt=\\'Be a helpful assistant.\\')\\n\\nresult1 = agent.run_sync(\\'Tell me a joke.\\')\\nprint(result1.data)\\n#> Did you hear about the toothpaste scandal? They called it Colgate.\\n\\nresult2 = agent.run_sync(\\'Explain?\\', message_history=result1.new_messages())\\nprint(result2.data)\\n#> This is an excellent joke invent by Samuel Colvin, it needs no explanation.\\n\\nprint(result2.all_messages())\\n\"\"\"\\n[\\n    ModelRequest(\\n        parts=[\\n            SystemPromptPart(\\n                content=\\'Be a helpful assistant.\\',\\n                dynamic_ref=None,\\n                part_kind=\\'system-prompt\\',\\n            ),\\n            UserPromptPart(\\n                content=\\'Tell me a joke.\\',\\n                timestamp=datetime.datetime(...),\\n                part_kind=\\'user-prompt\\',\\n            ),\\n        ],\\n        kind=\\'request\\',\\n    ),\\n    ModelResponse(\\n        parts=[\\n            TextPart(\\n                content=\\'Did you hear about the toothpaste scandal? They called it Colgate.\\',\\n                part_kind=\\'text\\',\\n            )\\n        ],\\n        timestamp=datetime.datetime(...),\\n        kind=\\'response\\',\\n    ),\\n    ModelRequest(\\n        parts=[\\n            UserPromptPart(\\n                content=\\'Explain?\\',\\n                timestamp=datetime.datetime(...),\\n                part_kind=\\'user-prompt\\',\\n            )\\n        ],\\n        kind=\\'request\\',\\n    ),\\n    ModelResponse(\\n        parts=[\\n            TextPart(\\n                content=\\'This is an excellent joke invent by Samuel Colvin, it needs no explanation.\\',\\n                part_kind=\\'text\\',\\n            )\\n        ],\\n        timestamp=datetime.datetime(...),\\n        kind=\\'response\\',\\n    ),\\n]\\n\"\"\"\\n```\\n_(This example is complete, it can be run \"as is\")_', 'metadata': {'file_path': 'pydantic-ai/docs/message-history.md', 'section_title': 'all messages from the run'}}, {'source_file': 'pydantic-ai/docs/message-history.md', 'title': 'Other ways of using messages', 'content': '## Other ways of using messages\\n\\nSince messages are defined by simple dataclasses, you can manually create and manipulate, e.g. for testing.\\n\\nThe message format is independent of the model used, so you can use messages in different agents, or the same agent with different models.\\n\\n```python\\nfrom pydantic_ai import Agent\\n\\nagent = Agent(\\'openai:gpt-4o\\', system_prompt=\\'Be a helpful assistant.\\')\\n\\nresult1 = agent.run_sync(\\'Tell me a joke.\\')\\nprint(result1.data)\\n#> Did you hear about the toothpaste scandal? They called it Colgate.\\n\\nresult2 = agent.run_sync(\\n    \\'Explain?\\', model=\\'gemini-1.5-pro\\', message_history=result1.new_messages()\\n)\\nprint(result2.data)\\n#> This is an excellent joke invent by Samuel Colvin, it needs no explanation.\\n\\nprint(result2.all_messages())\\n\"\"\"\\n[\\n    ModelRequest(\\n        parts=[\\n            SystemPromptPart(\\n                content=\\'Be a helpful assistant.\\',\\n                dynamic_ref=None,\\n                part_kind=\\'system-prompt\\',\\n            ),\\n            UserPromptPart(\\n                content=\\'Tell me a joke.\\',\\n                timestamp=datetime.datetime(...),\\n                part_kind=\\'user-prompt\\',\\n            ),\\n        ],\\n        kind=\\'request\\',\\n    ),\\n    ModelResponse(\\n        parts=[\\n            TextPart(\\n                content=\\'Did you hear about the toothpaste scandal? They called it Colgate.\\',\\n                part_kind=\\'text\\',\\n            )\\n        ],\\n        timestamp=datetime.datetime(...),\\n        kind=\\'response\\',\\n    ),\\n    ModelRequest(\\n        parts=[\\n            UserPromptPart(\\n                content=\\'Explain?\\',\\n                timestamp=datetime.datetime(...),\\n                part_kind=\\'user-prompt\\',\\n            )\\n        ],\\n        kind=\\'request\\',\\n    ),\\n    ModelResponse(\\n        parts=[\\n            TextPart(\\n                content=\\'This is an excellent joke invent by Samuel Colvin, it needs no explanation.\\',\\n                part_kind=\\'text\\',\\n            )\\n        ],\\n        timestamp=datetime.datetime(...),\\n        kind=\\'response\\',\\n    ),\\n]\\n\"\"\"\\n```', 'metadata': {'file_path': 'pydantic-ai/docs/message-history.md', 'section_title': 'Other ways of using messages'}}, {'source_file': 'pydantic-ai/docs/message-history.md', 'title': 'Examples', 'content': '## Examples\\n\\nFor a more complete example of using messages in conversations, see the [chat app](examples/chat-app.md) example.', 'metadata': {'file_path': 'pydantic-ai/docs/message-history.md', 'section_title': 'Examples'}}, {'source_file': 'pydantic-ai/docs/testing-evals.md', 'title': 'Testing and Evals', 'content': \"# Testing and Evals\\n\\nWith PydanticAI and LLM integrations in general, there are two distinct kinds of test:\\n\\n1. **Unit tests** — tests of your application code, and whether it's behaving correctly\\n2. **Evals** — tests of the LLM, and how good or bad its responses are\\n\\nFor the most part, these two kinds of tests have pretty separate goals and considerations.\", 'metadata': {'file_path': 'pydantic-ai/docs/testing-evals.md', 'section_title': 'Testing and Evals'}}, {'source_file': 'pydantic-ai/docs/testing-evals.md', 'title': 'Unit tests', 'content': '## Unit tests\\n\\nUnit tests for PydanticAI code are just like unit tests for any other Python code.\\n\\nBecause for the most part they\\'re nothing new, we have pretty well established tools and patterns for writing and running these kinds of tests.\\n\\nUnless you\\'re really sure you know better, you\\'ll probably want to follow roughly this strategy:\\n\\n* Use [`pytest`](https://docs.pytest.org/en/stable/) as your test harness\\n* If you find yourself typing out long assertions, use [inline-snapshot](https://15r10nk.github.io/inline-snapshot/latest/)\\n* Similarly, [dirty-equals](https://dirty-equals.helpmanual.io/latest/) can be useful for comparing large data structures\\n* Use [`TestModel`][pydantic_ai.models.test.TestModel] or [`FunctionModel`][pydantic_ai.models.function.FunctionModel] in place of your actual model to avoid the usage, latency and variability of real LLM calls\\n* Use [`Agent.override`][pydantic_ai.agent.Agent.override] to replace your model inside your application logic\\n* Set [`ALLOW_MODEL_REQUESTS=False`][pydantic_ai.models.ALLOW_MODEL_REQUESTS] globally to block any requests from being made to non-test models accidentally\\n\\n### Unit testing with `TestModel`\\n\\nThe simplest and fastest way to exercise most of your application code is using [`TestModel`][pydantic_ai.models.test.TestModel], this will (by default) call all tools in the agent, then return either plain text or a structured response depending on the return type of the agent.\\n\\n!!! note \"`TestModel` is not magic\"\\n    The \"clever\" (but not too clever) part of `TestModel` is that it will attempt to generate valid structured data for [function tools](tools.md) and [result types](results.md#structured-result-validation) based on the schema of the registered tools.\\n\\n    There\\'s no ML or AI in `TestModel`, it\\'s just plain old procedural Python code that tries to generate data that satisfies the JSON schema of a tool.\\n\\n    The resulting data won\\'t look pretty or relevant, but it should pass Pydantic\\'s validation in most cases.\\n    If you want something more sophisticated, use [`FunctionModel`][pydantic_ai.models.function.FunctionModel] and write your own data generation logic.\\n\\nLet\\'s write unit tests for the following application code:\\n\\n```python {title=\"weather_app.py\"}\\nimport asyncio\\nfrom datetime import date\\n\\nfrom pydantic_ai import Agent, RunContext\\n\\nfrom fake_database import DatabaseConn  # (1)!\\nfrom weather_service import WeatherService  # (2)!\\n\\nweather_agent = Agent(\\n    \\'openai:gpt-4o\\',\\n    deps_type=WeatherService,\\n    system_prompt=\\'Providing a weather forecast at the locations the user provides.\\',\\n)\\n\\n\\n@weather_agent.tool\\ndef weather_forecast(\\n    ctx: RunContext[WeatherService], location: str, forecast_date: date\\n) -> str:\\n    if forecast_date < date.today():  # (3)!\\n        return ctx.deps.get_historic_weather(location, forecast_date)\\n    else:\\n        return ctx.deps.get_forecast(location, forecast_date)\\n\\n\\nasync def run_weather_forecast(  # (4)!\\n    user_prompts: list[tuple[str, int]], conn: DatabaseConn\\n):\\n    \"\"\"Run weather forecast for a list of user prompts and save.\"\"\"\\n    async with WeatherService() as weather_service:\\n\\n        async def run_forecast(prompt: str, user_id: int):\\n            result = await weather_agent.run(prompt, deps=weather_service)\\n            await conn.store_forecast(user_id, result.data)\\n\\n        # run all prompts in parallel\\n        await asyncio.gather(\\n            *(run_forecast(prompt, user_id) for (prompt, user_id) in user_prompts)\\n        )\\n```\\n\\n1. `DatabaseConn` is a class that holds a database connection\\n2. `WeatherService` has methods to get weather forecasts and historic data about the weather\\n3. We need to call a different endpoint depending on whether the date is in the past or the future, you\\'ll see why this nuance is important below\\n4. This function is the code we want to test, together with the agent it uses\\n\\nHere we have a function that takes a list of `#!python (user_prompt, user_id)` tuples, gets a weather forecast for each prompt, and stores the result in the database.\\n\\n**We want to test this code without having to mock certain objects or modify our code so we can pass test objects in.**\\n\\nHere\\'s how we would write tests using [`TestModel`][pydantic_ai.models.test.TestModel]:\\n\\n```python {title=\"test_weather_app.py\" call_name=\"test_forecast\"}\\nfrom datetime import timezone\\nimport pytest\\n\\nfrom dirty_equals import IsNow\\n\\nfrom pydantic_ai import models, capture_run_messages\\nfrom pydantic_ai.models.test import TestModel\\nfrom pydantic_ai.messages import (\\n    ArgsDict,\\n    ModelResponse,\\n    SystemPromptPart,\\n    TextPart,\\n    ToolCallPart,\\n    ToolReturnPart,\\n    UserPromptPart,\\n    ModelRequest,\\n)\\n\\nfrom fake_database import DatabaseConn\\nfrom weather_app import run_weather_forecast, weather_agent\\n\\npytestmark = pytest.mark.anyio  # (1)!\\nmodels.ALLOW_MODEL_REQUESTS = False  # (2)!\\n\\n\\nasync def test_forecast():\\n    conn = DatabaseConn()\\n    user_id = 1\\n    with capture_run_messages() as messages:\\n        with weather_agent.override(model=TestModel()):  # (3)!\\n            prompt = \\'What will the weather be like in London on 2024-11-28?\\'\\n            await run_weather_forecast([(prompt, user_id)], conn)  # (4)!\\n\\n    forecast = await conn.get_forecast(user_id)\\n    assert forecast == \\'{\"weather_forecast\":\"Sunny with a chance of rain\"}\\'  # (5)!\\n\\n    assert messages == [  # (6)!\\n        ModelRequest(\\n            parts=[\\n                SystemPromptPart(\\n                    content=\\'Providing a weather forecast at the locations the user provides.\\',\\n                ),\\n                UserPromptPart(\\n                    content=\\'What will the weather be like in London on 2024-11-28?\\',\\n                    timestamp=IsNow(tz=timezone.utc),  # (7)!\\n                ),\\n            ]\\n        ),\\n        ModelResponse(\\n            parts=[\\n                ToolCallPart(\\n                    tool_name=\\'weather_forecast\\',\\n                    args=ArgsDict(\\n                        args_dict={\\n                            \\'location\\': \\'a\\',\\n                            \\'forecast_date\\': \\'2024-01-01\\',  # (8)!\\n                        }\\n                    ),\\n                    tool_call_id=None,\\n                )\\n            ],\\n            timestamp=IsNow(tz=timezone.utc),\\n        ),\\n        ModelRequest(\\n            parts=[\\n                ToolReturnPart(\\n                    tool_name=\\'weather_forecast\\',\\n                    content=\\'Sunny with a chance of rain\\',\\n                    tool_call_id=None,\\n                    timestamp=IsNow(tz=timezone.utc),\\n                ),\\n            ],\\n        ),\\n        ModelResponse(\\n            parts=[\\n                TextPart(\\n                    content=\\'{\"weather_forecast\":\"Sunny with a chance of rain\"}\\',\\n                )\\n            ],\\n            timestamp=IsNow(tz=timezone.utc),\\n        ),\\n    ]\\n```\\n\\n1. We\\'re using [anyio](https://anyio.readthedocs.io/en/stable/) to run async tests.\\n2. This is a safety measure to make sure we don\\'t accidentally make real requests to the LLM while testing, see [`ALLOW_MODEL_REQUESTS`][pydantic_ai.models.ALLOW_MODEL_REQUESTS] for more details.\\n3. We\\'re using [`Agent.override`][pydantic_ai.agent.Agent.override] to replace the agent\\'s model with [`TestModel`][pydantic_ai.models.test.TestModel], the nice thing about `override` is that we can replace the model inside agent without needing access to the agent `run*` methods call site.\\n4. Now we call the function we want to test inside the `override` context manager.\\n5. But default, `TestModel` will return a JSON string summarising the tools calls made, and what was returned. If you wanted to customise the response to something more closely aligned with the domain, you could add [`custom_result_text=\\'Sunny\\'`][pydantic_ai.models.test.TestModel.custom_result_text] when defining `TestModel`.\\n6. So far we don\\'t actually know which tools were called and with which values, we can use [`capture_run_messages`][pydantic_ai.capture_run_messages] to inspect messages from the most recent run and assert the exchange between the agent and the model occurred as expected.\\n7. The [`IsNow`][dirty_equals.IsNow] helper allows us to use declarative asserts even with data which will contain timestamps that change over time.\\n8. `TestModel` isn\\'t doing anything clever to extract values from the prompt, so these values are hardcoded.\\n\\n### Unit testing with `FunctionModel`\\n\\nThe above tests are a great start, but careful readers will notice that the `WeatherService.get_forecast` is never called since `TestModel` calls `weather_forecast` with a date in the past.\\n\\nTo fully exercise `weather_forecast`, we need to use [`FunctionModel`][pydantic_ai.models.function.FunctionModel] to customise how the tools is called.\\n\\nHere\\'s an example of using `FunctionModel` to test the `weather_forecast` tool with custom inputs\\n\\n```python {title=\"test_weather_app2.py\" call_name=\"test_forecast_future\"}\\nimport re\\n\\nimport pytest\\n\\nfrom pydantic_ai import models\\nfrom pydantic_ai.messages import (\\n    ModelMessage,\\n    ModelResponse,\\n    ToolCallPart,\\n)\\nfrom pydantic_ai.models.function import AgentInfo, FunctionModel\\n\\nfrom fake_database import DatabaseConn\\nfrom weather_app import run_weather_forecast, weather_agent\\n\\npytestmark = pytest.mark.anyio\\nmodels.ALLOW_MODEL_REQUESTS = False\\n\\n\\ndef call_weather_forecast(  # (1)!\\n    messages: list[ModelMessage], info: AgentInfo\\n) -> ModelResponse:\\n    if len(messages) == 1:\\n        # first call, call the weather forecast tool\\n        user_prompt = messages[0].parts[-1]\\n        m = re.search(r\\'\\\\d{4}-\\\\d{2}-\\\\d{2}\\', user_prompt.content)\\n        assert m is not None\\n        args = {\\'location\\': \\'London\\', \\'forecast_date\\': m.group()}  # (2)!\\n        return ModelResponse(\\n            parts=[ToolCallPart.from_raw_args(\\'weather_forecast\\', args)]\\n        )\\n    else:\\n        # second call, return the forecast\\n        msg = messages[-1].parts[0]\\n        assert msg.part_kind == \\'tool-return\\'\\n        return ModelResponse.from_text(f\\'The forecast is: {msg.content}\\')\\n\\n\\nasync def test_forecast_future():\\n    conn = DatabaseConn()\\n    user_id = 1\\n    with weather_agent.override(model=FunctionModel(call_weather_forecast)):  # (3)!\\n        prompt = \\'What will the weather be like in London on 2032-01-01?\\'\\n        await run_weather_forecast([(prompt, user_id)], conn)\\n\\n    forecast = await conn.get_forecast(user_id)\\n    assert forecast == \\'The forecast is: Rainy with a chance of sun\\'\\n```\\n\\n1. We define a function `call_weather_forecast` that will be called by `FunctionModel` in place of the LLM, this function has access to the list of [`ModelMessage`][pydantic_ai.messages.ModelMessage]s that make up the run, and [`AgentInfo`][pydantic_ai.models.function.AgentInfo] which contains information about the agent and the function tools and return tools.\\n2. Our function is slightly intelligent in that it tries to extract a date from the prompt, but just hard codes the location.\\n3. We use [`FunctionModel`][pydantic_ai.models.function.FunctionModel] to replace the agent\\'s model with our custom function.\\n\\n### Overriding model via pytest fixtures\\n\\nIf you\\'re writing lots of tests that all require model to be overridden, you can use [pytest fixtures](https://docs.pytest.org/en/6.2.x/fixture.html) to override the model with [`TestModel`][pydantic_ai.models.test.TestModel] or [`FunctionModel`][pydantic_ai.models.function.FunctionModel] in a reusable way.\\n\\nHere\\'s an example of a fixture that overrides the model with `TestModel`:\\n\\n```python {title=\"tests.py\"}\\nimport pytest\\nfrom weather_app import weather_agent\\n\\nfrom pydantic_ai.models.test import TestModel\\n\\n\\n@pytest.fixture\\ndef override_weather_agent():\\n    with weather_agent.override(model=TestModel()):\\n        yield\\n\\n\\nasync def test_forecast(override_weather_agent: None):\\n    ...\\n    # test code here\\n```', 'metadata': {'file_path': 'pydantic-ai/docs/testing-evals.md', 'section_title': 'Unit tests'}}, {'source_file': 'pydantic-ai/docs/testing-evals.md', 'title': 'Evals', 'content': '## Evals\\n\\n\"Evals\" refers to evaluating a models performance for a specific application.\\n\\n!!! danger \"Warning\"\\n    Unlike unit tests, evals are an emerging art/science; anyone who claims to know for sure exactly how your evals should be defined can safely be ignored.\\n\\nEvals are generally more like benchmarks than unit tests, they never \"pass\" although they do \"fail\"; you care mostly about how they change over time.\\n\\nSince evals need to be run against the real model, then can be slow and expensive to run, you generally won\\'t want to run them in CI for every commit.\\n\\n### Measuring performance\\n\\nThe hardest part of evals is measuring how well the model has performed.\\n\\nIn some cases (e.g. an agent to generate SQL) there are simple, easy to run tests that can be used to measure performance (e.g. is the SQL valid? Does it return the right results? Does it return just the right results?).\\n\\nIn other cases (e.g. an agent that gives advice on quitting smoking) it can be very hard or impossible to make quantitative measures of performance — in the smoking case you\\'d really need to run a double-blind trial over months, then wait 40 years and observe health outcomes to know if changes to your prompt were an improvement.\\n\\nThere are a few different strategies you can use to measure performance:\\n\\n* **End to end, self-contained tests** — like the SQL example, we can test the final result of the agent near-instantly\\n* **Synthetic self-contained tests** — writing unit test style checks that the output is as expected, checks like `#!python \\'chewing gum\\' in response`, while these checks might seem simplistic they can be helpful, one nice characteristic is that it\\'s easy to tell what\\'s wrong when they fail\\n* **LLMs evaluating LLMs** — using another models, or even the same model with a different prompt to evaluate the performance of the agent (like when the class marks each other\\'s homework because the teacher has a hangover), while the downsides and complexities of this approach are obvious, some think it can be a useful tool in the right circumstances\\n* **Evals in prod** — measuring the end results of the agent in production, then creating a quantitative measure of performance, so you can easily measure changes over time as you change the prompt or model used, [logfire](logfire.md) can be extremely useful in this case since you can write a custom query to measure the performance of your agent\\n\\n### System prompt customization\\n\\nThe system prompt is the developer\\'s primary tool in controlling an agent\\'s behavior, so it\\'s often useful to be able to customise the system prompt and see how performance changes. This is particularly relevant when the system prompt contains a list of examples and you want to understand how changing that list affects the model\\'s performance.\\n\\nLet\\'s assume we have the following app for running SQL generated from a user prompt (this examples omits a lot of details for brevity, see the [SQL gen](examples/sql-gen.md) example for a more complete code):\\n\\n```python {title=\"sql_app.py\"}\\nimport json\\nfrom pathlib import Path\\nfrom typing import Union\\n\\nfrom pydantic_ai import Agent, RunContext\\n\\nfrom fake_database import DatabaseConn\\n\\n\\nclass SqlSystemPrompt:  # (1)!\\n    def __init__(\\n        self, examples: Union[list[dict[str, str]], None] = None, db: str = \\'PostgreSQL\\'\\n    ):\\n        if examples is None:\\n            # if examples aren\\'t provided, load them from file, this is the default\\n            with Path(\\'examples.json\\').open(\\'rb\\') as f:\\n                self.examples = json.load(f)\\n        else:\\n            self.examples = examples\\n\\n        self.db = db\\n\\n    def build_prompt(self) -> str:  # (2)!\\n        return f\"\"\"\\\\\\nGiven the following {self.db} table of records, your job is to\\nwrite a SQL query that suits the user\\'s request.\\n\\nDatabase schema:\\nCREATE TABLE records (\\n  ...\\n);\\n\\n{\\'\\'.join(self.format_example(example) for example in self.examples)}\\n\"\"\"\\n\\n    @staticmethod\\n    def format_example(example: dict[str, str]) -> str:  # (3)!\\n        return f\"\"\"\\\\\\n<example>\\n  <request>{example[\\'request\\']}</request>\\n  <sql>{example[\\'sql\\']}</sql>\\n</example>\\n\"\"\"\\n\\n\\nsql_agent = Agent(\\n    \\'gemini-1.5-flash\\',\\n    deps_type=SqlSystemPrompt,\\n)\\n\\n\\n@sql_agent.system_prompt\\nasync def system_prompt(ctx: RunContext[SqlSystemPrompt]) -> str:\\n    return ctx.deps.build_prompt()\\n\\n\\nasync def user_search(user_prompt: str) -> list[dict[str, str]]:\\n    \"\"\"Search the database based on the user\\'s prompts.\"\"\"\\n    ...  # (4)!\\n    result = await sql_agent.run(user_prompt, deps=SqlSystemPrompt())\\n    conn = DatabaseConn()\\n    return await conn.execute(result.data)\\n```\\n\\n1. The `SqlSystemPrompt` class is used to build the system prompt, it can be customised with a list of examples and a database type. We implement this as a separate class passed as a dep to the agent so we can override both the inputs and the logic during evals via dependency injection.\\n2. The `build_prompt` method constructs the system prompt from the examples and the database type.\\n3. Some people think that LLMs are more likely to generate good responses if examples are formatted as XML as it\\'s to identify the end of a string, see [#93](https://github.com/pydantic/pydantic-ai/issues/93).\\n4. In reality, you would have more logic here, making it impractical to run the agent independently of the wider application.\\n\\n`examples.json` looks something like this:\\n\\n\\n    request: show me error records with the tag \"foobar\"\\n    response: SELECT * FROM records WHERE level = \\'error\\' and \\'foobar\\' = ANY(tags)\\n\\n```json {title=\"examples.json\"}\\n{\\n  \"examples\": [\\n    {\\n      \"request\": \"Show me all records\",\\n      \"sql\": \"SELECT * FROM records;\"\\n    },\\n    {\\n      \"request\": \"Show me all records from 2021\",\\n      \"sql\": \"SELECT * FROM records WHERE date_trunc(\\'year\\', date) = \\'2021-01-01\\';\"\\n    },\\n    {\\n      \"request\": \"show me error records with the tag \\'foobar\\'\",\\n      \"sql\": \"SELECT * FROM records WHERE level = \\'error\\' and \\'foobar\\' = ANY(tags);\"\\n    },\\n    ...\\n  ]\\n}\\n```\\n\\nNow we want a way to quantify the success of the SQL generation so we can judge how changes to the agent affect its performance.\\n\\nWe can use [`Agent.override`][pydantic_ai.agent.Agent.override] to replace the system prompt with a custom one that uses a subset of examples, and then run the application code (in this case `user_search`). We also run the actual SQL from the examples and compare the \"correct\" result from the example SQL to the SQL generated by the agent. (We compare the results of running the SQL rather than the SQL itself since the SQL might be semantically equivalent but written in a different way).\\n\\nTo get a quantitative measure of performance, we assign points to each run as follows:\\n* **-100** points if the generated SQL is invalid\\n* **-1** point for each row returned by the agent (so returning lots of results is discouraged)\\n* **+5** points for each row returned by the agent that matches the expected result\\n\\nWe use 5-fold cross-validation to judge the performance of the agent using our existing set of examples.\\n\\n```python {title=\"sql_app_evals.py\"}\\nimport json\\nimport statistics\\nfrom pathlib import Path\\nfrom itertools import chain\\n\\nfrom fake_database import DatabaseConn, QueryError\\nfrom sql_app import sql_agent, SqlSystemPrompt, user_search\\n\\n\\nasync def main():\\n    with Path(\\'examples.json\\').open(\\'rb\\') as f:\\n        examples = json.load(f)\\n\\n    # split examples into 5 folds\\n    fold_size = len(examples) // 5\\n    folds = [examples[i : i + fold_size] for i in range(0, len(examples), fold_size)]\\n    conn = DatabaseConn()\\n    scores = []\\n\\n    for i, fold in enumerate(folds, start=1):\\n        fold_score = 0\\n        # build all other folds into a list of examples\\n        other_folds = list(chain(*(f for j, f in enumerate(folds) if j != i)))\\n        # create a new system prompt with the other fold examples\\n        system_prompt = SqlSystemPrompt(examples=other_folds)\\n\\n        # override the system prompt with the new one\\n        with sql_agent.override(deps=system_prompt):\\n            for case in fold:\\n                try:\\n                    agent_results = await user_search(case[\\'request\\'])\\n                except QueryError as e:\\n                    print(f\\'Fold {i} {case}: {e}\\')\\n                    fold_score -= 100\\n                else:\\n                    # get the expected results using the SQL from this case\\n                    expected_results = await conn.execute(case[\\'sql\\'])\\n\\n                agent_ids = [r[\\'id\\'] for r in agent_results]\\n                # each returned value has a score of -1\\n                fold_score -= len(agent_ids)\\n                expected_ids = {r[\\'id\\'] for r in expected_results}\\n\\n                # each return value that matches the expected value has a score of 3\\n                fold_score += 5 * len(set(agent_ids) & expected_ids)\\n\\n        scores.append(fold_score)\\n\\n    overall_score = statistics.mean(scores)\\n    print(f\\'Overall score: {overall_score:0.2f}\\')\\n    #> Overall score: 12.00\\n```\\n\\nWe can then change the prompt, the model, or the examples and see how the score changes over time.', 'metadata': {'file_path': 'pydantic-ai/docs/testing-evals.md', 'section_title': 'Evals'}}, {'source_file': 'pydantic-ai/docs/multi-agent-applications.md', 'title': 'Multi-agent Applications', 'content': '# Multi-agent Applications\\n\\nThere are roughly four levels of complexity when building applications with PydanticAI:\\n\\n1. Single agent workflows — what most of the `pydantic_ai` documentation covers\\n2. [Agent delegation](#agent-delegation) — agents using another agent via tools\\n3. [Programmatic agent hand-off](#programmatic-agent-hand-off) — one agent runs, then application code calls another agent\\n4. [Graph based control flow](graph.md) — for the most complex cases, a graph-based state machine can be used to control the execution of multiple agents\\n\\nOf course, you can combine multiple strategies in a single application.', 'metadata': {'file_path': 'pydantic-ai/docs/multi-agent-applications.md', 'section_title': 'Multi-agent Applications'}}, {'source_file': 'pydantic-ai/docs/multi-agent-applications.md', 'title': 'Agent delegation', 'content': '## Agent delegation\\n\\n\"Agent delegation\" refers to the scenario where an agent delegates work to another agent, then takes back control when the delegate agent (the agent called from within a tool) finishes.\\n\\nSince agents are stateless and designed to be global, you do not need to include the agent itself in agent [dependencies](dependencies.md).\\n\\nYou\\'ll generally want to pass [`ctx.usage`][pydantic_ai.RunContext.usage] to the [`usage`][pydantic_ai.Agent.run] keyword argument of the delegate agent run so usage within that run counts towards the total usage of the parent agent run.\\n\\n!!! note \"Multiple models\"\\n    Agent delegation doesn\\'t need to use the same model for each agent. If you choose to use different models within a run, calculating the monetary cost from the final [`result.usage()`][pydantic_ai.result.RunResult.usage] of the run will not be possible, but you can still use [`UsageLimits`][pydantic_ai.usage.UsageLimits] to avoid unexpected costs.\\n\\n```python {title=\"agent_delegation_simple.py\"}\\nfrom pydantic_ai import Agent, RunContext\\nfrom pydantic_ai.usage import UsageLimits\\n\\njoke_selection_agent = Agent(  # (1)!\\n    \\'openai:gpt-4o\\',\\n    system_prompt=(\\n        \\'Use the `joke_factory` to generate some jokes, then choose the best. \\'\\n        \\'You must return just a single joke.\\'\\n    ),\\n)\\njoke_generation_agent = Agent(\\'gemini-1.5-flash\\', result_type=list[str])  # (2)!\\n\\n\\n@joke_selection_agent.tool\\nasync def joke_factory(ctx: RunContext[None], count: int) -> list[str]:\\n    r = await joke_generation_agent.run(  # (3)!\\n        f\\'Please generate {count} jokes.\\',\\n        usage=ctx.usage,  # (4)!\\n    )\\n    return r.data  # (5)!\\n\\n\\nresult = joke_selection_agent.run_sync(\\n    \\'Tell me a joke.\\',\\n    usage_limits=UsageLimits(request_limit=5, total_tokens_limit=300),\\n)\\nprint(result.data)\\n#> Did you hear about the toothpaste scandal? They called it Colgate.\\nprint(result.usage())\\n\"\"\"\\nUsage(\\n    requests=3, request_tokens=204, response_tokens=24, total_tokens=228, details=None\\n)\\n\"\"\"\\n```\\n\\n1. The \"parent\" or controlling agent.\\n2. The \"delegate\" agent, which is called from within a tool of the parent agent.\\n3. Call the delegate agent from within a tool of the parent agent.\\n4. Pass the usage from the parent agent to the delegate agent so the final [`result.usage()`][pydantic_ai.result.RunResult.usage] includes the usage from both agents.\\n5. Since the function returns `#!python list[str]`, and the `result_type` of `joke_generation_agent` is also `#!python list[str]`, we can simply return `#!python r.data` from the tool.\\n\\n_(This example is complete, it can be run \"as is\")_\\n\\nThe control flow for this example is pretty simple and can be summarised as follows:\\n\\n```mermaid\\ngraph TD\\n  START --> joke_selection_agent\\n  joke_selection_agent --> joke_factory[\"joke_factory (tool)\"]\\n  joke_factory --> joke_generation_agent\\n  joke_generation_agent --> joke_factory\\n  joke_factory --> joke_selection_agent\\n  joke_selection_agent --> END\\n```\\n\\n### Agent delegation and dependencies\\n\\nGenerally the delegate agent needs to either have the same [dependencies](dependencies.md) as the calling agent, or dependencies which are a subset of the calling agent\\'s dependencies.\\n\\n!!! info \"Initializing dependencies\"\\n    We say \"generally\" above since there\\'s nothing to stop you initializing dependencies within a tool call and therefore using interdependencies in a delegate agent that are not available on the parent, this should often be avoided since it can be significantly slower than reusing connections etc. from the parent agent.\\n\\n```python {title=\"agent_delegation_deps.py\"}\\nfrom dataclasses import dataclass\\n\\nimport httpx\\n\\nfrom pydantic_ai import Agent, RunContext\\n\\n\\n@dataclass\\nclass ClientAndKey:  # (1)!\\n    http_client: httpx.AsyncClient\\n    api_key: str\\n\\n\\njoke_selection_agent = Agent(\\n    \\'openai:gpt-4o\\',\\n    deps_type=ClientAndKey,  # (2)!\\n    system_prompt=(\\n        \\'Use the `joke_factory` tool to generate some jokes on the given subject, \\'\\n        \\'then choose the best. You must return just a single joke.\\'\\n    ),\\n)\\njoke_generation_agent = Agent(\\n    \\'gemini-1.5-flash\\',\\n    deps_type=ClientAndKey,  # (4)!\\n    result_type=list[str],\\n    system_prompt=(\\n        \\'Use the \"get_jokes\" tool to get some jokes on the given subject, \\'\\n        \\'then extract each joke into a list.\\'\\n    ),\\n)\\n\\n\\n@joke_selection_agent.tool\\nasync def joke_factory(ctx: RunContext[ClientAndKey], count: int) -> list[str]:\\n    r = await joke_generation_agent.run(\\n        f\\'Please generate {count} jokes.\\',\\n        deps=ctx.deps,  # (3)!\\n        usage=ctx.usage,\\n    )\\n    return r.data\\n\\n\\n@joke_generation_agent.tool  # (5)!\\nasync def get_jokes(ctx: RunContext[ClientAndKey], count: int) -> str:\\n    response = await ctx.deps.http_client.get(\\n        \\'https://example.com\\',\\n        params={\\'count\\': count},\\n        headers={\\'Authorization\\': f\\'Bearer {ctx.deps.api_key}\\'},\\n    )\\n    response.raise_for_status()\\n    return response.text\\n\\n\\nasync def main():\\n    async with httpx.AsyncClient() as client:\\n        deps = ClientAndKey(client, \\'foobar\\')\\n        result = await joke_selection_agent.run(\\'Tell me a joke.\\', deps=deps)\\n        print(result.data)\\n        #> Did you hear about the toothpaste scandal? They called it Colgate.\\n        print(result.usage())  # (6)!\\n        \"\"\"\\n        Usage(\\n            requests=4,\\n            request_tokens=309,\\n            response_tokens=32,\\n            total_tokens=341,\\n            details=None,\\n        )\\n        \"\"\"\\n```\\n\\n1. Define a dataclass to hold the client and API key dependencies.\\n2. Set the `deps_type` of the calling agent — `joke_selection_agent` here.\\n3. Pass the dependencies to the delegate agent\\'s run method within the tool call.\\n4. Also set the `deps_type` of the delegate agent — `joke_generation_agent` here.\\n5. Define a tool on the delegate agent that uses the dependencies to make an HTTP request.\\n6. Usage now includes 4 requests — 2 from the calling agent and 2 from the delegate agent.\\n\\n_(This example is complete, it can be run \"as is\" — you\\'ll need to add `asyncio.run(main())` to run `main`)_\\n\\nThis example shows how even a fairly simple agent delegation can lead to a complex control flow:\\n\\n```mermaid\\ngraph TD\\n  START --> joke_selection_agent\\n  joke_selection_agent --> joke_factory[\"joke_factory (tool)\"]\\n  joke_factory --> joke_generation_agent\\n  joke_generation_agent --> get_jokes[\"get_jokes (tool)\"]\\n  get_jokes --> http_request[\"HTTP request\"]\\n  http_request --> get_jokes\\n  get_jokes --> joke_generation_agent\\n  joke_generation_agent --> joke_factory\\n  joke_factory --> joke_selection_agent\\n  joke_selection_agent --> END\\n```', 'metadata': {'file_path': 'pydantic-ai/docs/multi-agent-applications.md', 'section_title': 'Agent delegation'}}, {'source_file': 'pydantic-ai/docs/multi-agent-applications.md', 'title': 'Programmatic agent hand-off', 'content': '## Programmatic agent hand-off\\n\\n\"Programmatic agent hand-off\" refers to the scenario where multiple agents are called in succession, with application code and/or a human in the loop responsible for deciding which agent to call next.\\n\\nHere agents don\\'t need to use the same deps.\\n\\nHere we show two agents used in succession, the first to find a flight and the second to extract the user\\'s seat preference.\\n\\n```python {title=\"programmatic_handoff.py\"}\\nfrom typing import Literal, Union\\n\\nfrom pydantic import BaseModel, Field\\nfrom rich.prompt import Prompt\\n\\nfrom pydantic_ai import Agent, RunContext\\nfrom pydantic_ai.messages import ModelMessage\\nfrom pydantic_ai.usage import Usage, UsageLimits\\n\\n\\nclass FlightDetails(BaseModel):\\n    flight_number: str\\n\\n\\nclass Failed(BaseModel):\\n    \"\"\"Unable to find a satisfactory choice.\"\"\"\\n\\n\\nflight_search_agent = Agent[None, Union[FlightDetails, Failed]](  # (1)!\\n    \\'openai:gpt-4o\\',\\n    result_type=Union[FlightDetails, Failed],  # type: ignore\\n    system_prompt=(\\n        \\'Use the \"flight_search\" tool to find a flight \\'\\n        \\'from the given origin to the given destination.\\'\\n    ),\\n)\\n\\n\\n@flight_search_agent.tool  # (2)!\\nasync def flight_search(\\n    ctx: RunContext[None], origin: str, destination: str\\n) -> Union[FlightDetails, None]:\\n    # in reality, this would call a flight search API or\\n    # use a browser to scrape a flight search website\\n    return FlightDetails(flight_number=\\'AK456\\')\\n\\n\\nusage_limits = UsageLimits(request_limit=15)  # (3)!\\n\\n\\nasync def find_flight(usage: Usage) -> Union[FlightDetails, None]:  # (4)!\\n    message_history: Union[list[ModelMessage], None] = None\\n    for _ in range(3):\\n        prompt = Prompt.ask(\\n            \\'Where would you like to fly from and to?\\',\\n        )\\n        result = await flight_search_agent.run(\\n            prompt,\\n            message_history=message_history,\\n            usage=usage,\\n            usage_limits=usage_limits,\\n        )\\n        if isinstance(result.data, FlightDetails):\\n            return result.data\\n        else:\\n            message_history = result.all_messages(\\n                result_tool_return_content=\\'Please try again.\\'\\n            )\\n\\n\\nclass SeatPreference(BaseModel):\\n    row: int = Field(ge=1, le=30)\\n    seat: Literal[\\'A\\', \\'B\\', \\'C\\', \\'D\\', \\'E\\', \\'F\\']', 'metadata': {'file_path': 'pydantic-ai/docs/multi-agent-applications.md', 'section_title': 'Programmatic agent hand-off'}}, {'source_file': 'pydantic-ai/docs/multi-agent-applications.md', 'title': \"This agent is responsible for extracting the user's seat selection\", 'content': '# This agent is responsible for extracting the user\\'s seat selection\\nseat_preference_agent = Agent[None, Union[SeatPreference, Failed]](  # (5)!\\n    \\'openai:gpt-4o\\',\\n    result_type=Union[SeatPreference, Failed],  # type: ignore\\n    system_prompt=(\\n        \"Extract the user\\'s seat preference. \"\\n        \\'Seats A and F are window seats. \\'\\n        \\'Row 1 is the front row and has extra leg room. \\'\\n        \\'Rows 14, and 20 also have extra leg room. \\'\\n    ),\\n)\\n\\n\\nasync def find_seat(usage: Usage) -> SeatPreference:  # (6)!\\n    message_history: Union[list[ModelMessage], None] = None\\n    while True:\\n        answer = Prompt.ask(\\'What seat would you like?\\')\\n\\n        result = await seat_preference_agent.run(\\n            answer,\\n            message_history=message_history,\\n            usage=usage,\\n            usage_limits=usage_limits,\\n        )\\n        if isinstance(result.data, SeatPreference):\\n            return result.data\\n        else:\\n            print(\\'Could not understand seat preference. Please try again.\\')\\n            message_history = result.all_messages()\\n\\n\\nasync def main():  # (7)!\\n    usage: Usage = Usage()\\n\\n    opt_flight_details = await find_flight(usage)\\n    if opt_flight_details is not None:\\n        print(f\\'Flight found: {opt_flight_details.flight_number}\\')\\n        #> Flight found: AK456\\n        seat_preference = await find_seat(usage)\\n        print(f\\'Seat preference: {seat_preference}\\')\\n        #> Seat preference: row=1 seat=\\'A\\'\\n```\\n\\n1. Define the first agent, which finds a flight. We use an explicit type annotation until [PEP-747](https://peps.python.org/pep-0747/) lands, see [structured results](results.md#structured-result-validation). We use a union as the result type so the model can communicate if it\\'s unable to find a satisfactory choice; internally, each member of the union will be registered as a separate tool.\\n2. Define a tool on the agent to find a flight. In this simple case we could dispense with the tool and just define the agent to return structured data, then search for a flight, but in more complex scenarios the tool would be necessary.\\n3. Define usage limits for the entire app.\\n4. Define a function to find a flight, which asks the user for their preferences and then calls the agent to find a flight.\\n5. As with `flight_search_agent` above, we use an explicit type annotation to define the agent.\\n6. Define a function to find the user\\'s seat preference, which asks the user for their seat preference and then calls the agent to extract the seat preference.\\n7. Now that we\\'ve put our logic for running each agent into separate functions, our main app becomes very simple.\\n\\n_(This example is complete, it can be run \"as is\" — you\\'ll need to add `asyncio.run(main())` to run `main`)_\\n\\nThe control flow for this example can be summarised as follows:\\n\\n```mermaid\\ngraph TB\\n  START --> ask_user_flight[\"ask user for flight\"]\\n\\n  subgraph find_flight\\n    flight_search_agent --> ask_user_flight\\n    ask_user_flight --> flight_search_agent\\n  end\\n\\n  flight_search_agent --> ask_user_seat[\"ask user for seat\"]\\n  flight_search_agent --> END\\n\\n  subgraph find_seat\\n    seat_preference_agent --> ask_user_seat\\n    ask_user_seat --> seat_preference_agent\\n  end\\n\\n  seat_preference_agent --> END\\n```', 'metadata': {'file_path': 'pydantic-ai/docs/multi-agent-applications.md', 'section_title': \"This agent is responsible for extracting the user's seat selection\"}}, {'source_file': 'pydantic-ai/docs/multi-agent-applications.md', 'title': 'Pydantic Graphs', 'content': '## Pydantic Graphs\\n\\nSee the [graph](graph.md) documentation on when and how to use graphs.', 'metadata': {'file_path': 'pydantic-ai/docs/multi-agent-applications.md', 'section_title': 'Pydantic Graphs'}}, {'source_file': 'pydantic-ai/docs/multi-agent-applications.md', 'title': 'Examples', 'content': '## Examples\\n\\nThe following examples demonstrate how to use dependencies in PydanticAI:\\n\\n- [Flight booking](examples/flight-booking.md)', 'metadata': {'file_path': 'pydantic-ai/docs/multi-agent-applications.md', 'section_title': 'Examples'}}, {'source_file': 'pydantic-ai/docs/results.md', 'title': 'No Header', 'content': 'Results are the final values returned from [running an agent](agents.md#running-agents).\\nThe result values are wrapped in [`RunResult`][pydantic_ai.result.RunResult] and [`StreamedRunResult`][pydantic_ai.result.StreamedRunResult] so you can access other data like [usage][pydantic_ai.usage.Usage] of the run and [message history](message-history.md#accessing-messages-from-results)\\n\\nBoth `RunResult` and `StreamedRunResult` are generic in the data they wrap, so typing information about the data returned by the agent is preserved.\\n\\n```python {title=\"olympics.py\"}\\nfrom pydantic import BaseModel\\n\\nfrom pydantic_ai import Agent\\n\\n\\nclass CityLocation(BaseModel):\\n    city: str\\n    country: str\\n\\n\\nagent = Agent(\\'gemini-1.5-flash\\', result_type=CityLocation)\\nresult = agent.run_sync(\\'Where were the olympics held in 2012?\\')\\nprint(result.data)\\n#> city=\\'London\\' country=\\'United Kingdom\\'\\nprint(result.usage())\\n\"\"\"\\nUsage(requests=1, request_tokens=57, response_tokens=8, total_tokens=65, details=None)\\n\"\"\"\\n```\\n\\n_(This example is complete, it can be run \"as is\")_\\n\\nRuns end when either a plain text response is received or the model calls a tool associated with one of the structured result types. We will add limits to make sure a run doesn\\'t go on indefinitely, see [#70](https://github.com/pydantic/pydantic-ai/issues/70).', 'metadata': {'file_path': 'pydantic-ai/docs/results.md', 'section_title': 'No Header'}}, {'source_file': 'pydantic-ai/docs/results.md', 'title': 'Result data {#structured-result-validation}', 'content': '## Result data {#structured-result-validation}\\n\\nWhen the result type is `str`, or a union including `str`, plain text responses are enabled on the model, and the raw text response from the model is used as the response data.\\n\\nIf the result type is a union with multiple members (after remove `str` from the members), each member is registered as a separate tool with the model in order to reduce the complexity of the tool schemas and maximise the chances a model will respond correctly.\\n\\nIf the result type schema is not of type `\"object\"`, the result type is wrapped in a single element object, so the schema of all tools registered with the model are object schemas.\\n\\nStructured results (like tools) use Pydantic to build the JSON schema used for the tool, and to validate the data returned by the model.\\n\\n!!! note \"Bring on PEP-747\"\\n    Until [PEP-747](https://peps.python.org/pep-0747/) \"Annotating Type Forms\" lands, unions are not valid as `type`s in Python.\\n\\n    When creating the agent we need to `# type: ignore` the `result_type` argument, and add a type hint to tell type checkers about the type of the agent.\\n\\nHere\\'s an example of returning either text or a structured value\\n\\n```python {title=\"box_or_error.py\"}\\nfrom typing import Union\\n\\nfrom pydantic import BaseModel\\n\\nfrom pydantic_ai import Agent\\n\\n\\nclass Box(BaseModel):\\n    width: int\\n    height: int\\n    depth: int\\n    units: str\\n\\n\\nagent: Agent[None, Union[Box, str]] = Agent(\\n    \\'openai:gpt-4o-mini\\',\\n    result_type=Union[Box, str],  # type: ignore\\n    system_prompt=(\\n        \"Extract me the dimensions of a box, \"\\n        \"if you can\\'t extract all data, ask the user to try again.\"\\n    ),\\n)\\n\\nresult = agent.run_sync(\\'The box is 10x20x30\\')\\nprint(result.data)\\n#> Please provide the units for the dimensions (e.g., cm, in, m).\\n\\nresult = agent.run_sync(\\'The box is 10x20x30 cm\\')\\nprint(result.data)\\n#> width=10 height=20 depth=30 units=\\'cm\\'\\n```\\n\\n_(This example is complete, it can be run \"as is\")_\\n\\nHere\\'s an example of using a union return type which registered multiple tools, and wraps non-object schemas in an object:\\n\\n```python {title=\"colors_or_sizes.py\"}\\nfrom typing import Union\\n\\nfrom pydantic_ai import Agent\\n\\nagent: Agent[None, Union[list[str], list[int]]] = Agent(\\n    \\'openai:gpt-4o-mini\\',\\n    result_type=Union[list[str], list[int]],  # type: ignore\\n    system_prompt=\\'Extract either colors or sizes from the shapes provided.\\',\\n)\\n\\nresult = agent.run_sync(\\'red square, blue circle, green triangle\\')\\nprint(result.data)\\n#> [\\'red\\', \\'blue\\', \\'green\\']\\n\\nresult = agent.run_sync(\\'square size 10, circle size 20, triangle size 30\\')\\nprint(result.data)\\n#> [10, 20, 30]\\n```\\n\\n_(This example is complete, it can be run \"as is\")_\\n\\n### Result validators functions\\n\\nSome validation is inconvenient or impossible to do in Pydantic validators, in particular when the validation requires IO and is asynchronous. PydanticAI provides a way to add validation functions via the [`agent.result_validator`][pydantic_ai.Agent.result_validator] decorator.\\n\\nHere\\'s a simplified variant of the [SQL Generation example](examples/sql-gen.md):\\n\\n```python {title=\"sql_gen.py\"}\\nfrom typing import Union\\n\\nfrom fake_database import DatabaseConn, QueryError\\nfrom pydantic import BaseModel\\n\\nfrom pydantic_ai import Agent, RunContext, ModelRetry\\n\\n\\nclass Success(BaseModel):\\n    sql_query: str\\n\\n\\nclass InvalidRequest(BaseModel):\\n    error_message: str\\n\\n\\nResponse = Union[Success, InvalidRequest]\\nagent: Agent[DatabaseConn, Response] = Agent(\\n    \\'gemini-1.5-flash\\',\\n    result_type=Response,  # type: ignore\\n    deps_type=DatabaseConn,\\n    system_prompt=\\'Generate PostgreSQL flavored SQL queries based on user input.\\',\\n)\\n\\n\\n@agent.result_validator\\nasync def validate_result(ctx: RunContext[DatabaseConn], result: Response) -> Response:\\n    if isinstance(result, InvalidRequest):\\n        return result\\n    try:\\n        await ctx.deps.execute(f\\'EXPLAIN {result.sql_query}\\')\\n    except QueryError as e:\\n        raise ModelRetry(f\\'Invalid query: {e}\\') from e\\n    else:\\n        return result\\n\\n\\nresult = agent.run_sync(\\n    \\'get me uses who were last active yesterday.\\', deps=DatabaseConn()\\n)\\nprint(result.data)\\n#> sql_query=\\'SELECT * FROM users WHERE last_active::date = today() - interval 1 day\\'\\n```\\n\\n_(This example is complete, it can be run \"as is\")_', 'metadata': {'file_path': 'pydantic-ai/docs/results.md', 'section_title': 'Result data {#structured-result-validation}'}}, {'source_file': 'pydantic-ai/docs/results.md', 'title': 'Streamed Results', 'content': '## Streamed Results\\n\\nThere two main challenges with streamed results:\\n\\n1. Validating structured responses before they\\'re complete, this is achieved by \"partial validation\" which was recently added to Pydantic in [pydantic/pydantic#10748](https://github.com/pydantic/pydantic/pull/10748).\\n2. When receiving a response, we don\\'t know if it\\'s the final response without starting to stream it and peeking at the content. PydanticAI streams just enough of the response to sniff out if it\\'s a tool call or a result, then streams the whole thing and calls tools, or returns the stream as a [`StreamedRunResult`][pydantic_ai.result.StreamedRunResult].\\n\\n### Streaming Text\\n\\nExample of streamed text result:\\n\\n```python {title=\"streamed_hello_world.py\" line_length=\"120\"}\\nfrom pydantic_ai import Agent\\n\\nagent = Agent(\\'gemini-1.5-flash\\')  # (1)!\\n\\n\\nasync def main():\\n    async with agent.run_stream(\\'Where does \"hello world\" come from?\\') as result:  # (2)!\\n        async for message in result.stream_text():  # (3)!\\n            print(message)\\n            #> The first known\\n            #> The first known use of \"hello,\\n            #> The first known use of \"hello, world\" was in\\n            #> The first known use of \"hello, world\" was in a 1974 textbook\\n            #> The first known use of \"hello, world\" was in a 1974 textbook about the C\\n            #> The first known use of \"hello, world\" was in a 1974 textbook about the C programming language.\\n```\\n\\n1. Streaming works with the standard [`Agent`][pydantic_ai.Agent] class, and doesn\\'t require any special setup, just a model that supports streaming (currently all models support streaming).\\n2. The [`Agent.run_stream()`][pydantic_ai.Agent.run_stream] method is used to start a streamed run, this method returns a context manager so the connection can be closed when the stream completes.\\n3. Each item yield by [`StreamedRunResult.stream_text()`][pydantic_ai.result.StreamedRunResult.stream_text] is the complete text response, extended as new data is received.\\n\\n_(This example is complete, it can be run \"as is\" — you\\'ll need to add `asyncio.run(main())` to run `main`)_\\n\\nWe can also stream text as deltas rather than the entire text in each item:\\n\\n```python {title=\"streamed_delta_hello_world.py\"}\\nfrom pydantic_ai import Agent\\n\\nagent = Agent(\\'gemini-1.5-flash\\')\\n\\n\\nasync def main():\\n    async with agent.run_stream(\\'Where does \"hello world\" come from?\\') as result:\\n        async for message in result.stream_text(delta=True):  # (1)!\\n            print(message)\\n            #> The first known\\n            #> use of \"hello,\\n            #> world\" was in\\n            #> a 1974 textbook\\n            #> about the C\\n            #> programming language.\\n```\\n\\n1. [`stream_text`][pydantic_ai.result.StreamedRunResult.stream_text] will error if the response is not text\\n\\n_(This example is complete, it can be run \"as is\" — you\\'ll need to add `asyncio.run(main())` to run `main`)_\\n\\n!!! warning \"Result message not included in `messages`\"\\n    The final result message will **NOT** be added to result messages if you use `.stream_text(delta=True)`,\\n    see [Messages and chat history](message-history.md) for more information.\\n\\n### Streaming Structured Responses\\n\\nNot all types are supported with partial validation in Pydantic, see [pydantic/pydantic#10748](https://github.com/pydantic/pydantic/pull/10748), generally for model-like structures it\\'s currently best to use `TypeDict`.\\n\\nHere\\'s an example of streaming a use profile as it\\'s built:\\n\\n```python {title=\"streamed_user_profile.py\" line_length=\"120\"}\\nfrom datetime import date\\n\\nfrom typing_extensions import TypedDict\\n\\nfrom pydantic_ai import Agent\\n\\n\\nclass UserProfile(TypedDict, total=False):\\n    name: str\\n    dob: date\\n    bio: str\\n\\n\\nagent = Agent(\\n    \\'openai:gpt-4o\\',\\n    result_type=UserProfile,\\n    system_prompt=\\'Extract a user profile from the input\\',\\n)\\n\\n\\nasync def main():\\n    user_input = \\'My name is Ben, I was born on January 28th 1990, I like the chain the dog and the pyramid.\\'\\n    async with agent.run_stream(user_input) as result:\\n        async for profile in result.stream():\\n            print(profile)\\n            #> {\\'name\\': \\'Ben\\'}\\n            #> {\\'name\\': \\'Ben\\'}\\n            #> {\\'name\\': \\'Ben\\', \\'dob\\': date(1990, 1, 28), \\'bio\\': \\'Likes\\'}\\n            #> {\\'name\\': \\'Ben\\', \\'dob\\': date(1990, 1, 28), \\'bio\\': \\'Likes the chain the \\'}\\n            #> {\\'name\\': \\'Ben\\', \\'dob\\': date(1990, 1, 28), \\'bio\\': \\'Likes the chain the dog and the pyr\\'}\\n            #> {\\'name\\': \\'Ben\\', \\'dob\\': date(1990, 1, 28), \\'bio\\': \\'Likes the chain the dog and the pyramid\\'}\\n            #> {\\'name\\': \\'Ben\\', \\'dob\\': date(1990, 1, 28), \\'bio\\': \\'Likes the chain the dog and the pyramid\\'}\\n```\\n\\n_(This example is complete, it can be run \"as is\" — you\\'ll need to add `asyncio.run(main())` to run `main`)_\\n\\nIf you want fine-grained control of validation, particularly catching validation errors, you can use the following pattern:\\n\\n```python {title=\"streamed_user_profile.py\" line_length=\"120\"}\\nfrom datetime import date\\n\\nfrom pydantic import ValidationError\\nfrom typing_extensions import TypedDict\\n\\nfrom pydantic_ai import Agent\\n\\n\\nclass UserProfile(TypedDict, total=False):\\n    name: str\\n    dob: date\\n    bio: str\\n\\n\\nagent = Agent(\\'openai:gpt-4o\\', result_type=UserProfile)\\n\\n\\nasync def main():\\n    user_input = \\'My name is Ben, I was born on January 28th 1990, I like the chain the dog and the pyramid.\\'\\n    async with agent.run_stream(user_input) as result:\\n        async for message, last in result.stream_structured(debounce_by=0.01):  # (1)!\\n            try:\\n                profile = await result.validate_structured_result(  # (2)!\\n                    message,\\n                    allow_partial=not last,\\n                )\\n            except ValidationError:\\n                continue\\n            print(profile)\\n            #> {\\'name\\': \\'Ben\\'}\\n            #> {\\'name\\': \\'Ben\\'}\\n            #> {\\'name\\': \\'Ben\\', \\'dob\\': date(1990, 1, 28), \\'bio\\': \\'Likes\\'}\\n            #> {\\'name\\': \\'Ben\\', \\'dob\\': date(1990, 1, 28), \\'bio\\': \\'Likes the chain the \\'}\\n            #> {\\'name\\': \\'Ben\\', \\'dob\\': date(1990, 1, 28), \\'bio\\': \\'Likes the chain the dog and the pyr\\'}\\n            #> {\\'name\\': \\'Ben\\', \\'dob\\': date(1990, 1, 28), \\'bio\\': \\'Likes the chain the dog and the pyramid\\'}\\n            #> {\\'name\\': \\'Ben\\', \\'dob\\': date(1990, 1, 28), \\'bio\\': \\'Likes the chain the dog and the pyramid\\'}\\n```\\n\\n1. [`stream_structured`][pydantic_ai.result.StreamedRunResult.stream_structured] streams the data as [`ModelResponse`][pydantic_ai.messages.ModelResponse] objects, thus iteration can\\'t fail with a `ValidationError`.\\n2. [`validate_structured_result`][pydantic_ai.result.StreamedRunResult.validate_structured_result] validates the data, `allow_partial=True` enables pydantic\\'s [`experimental_allow_partial` flag on `TypeAdapter`][pydantic.type_adapter.TypeAdapter.validate_json].\\n\\n_(This example is complete, it can be run \"as is\" — you\\'ll need to add `asyncio.run(main())` to run `main`)_', 'metadata': {'file_path': 'pydantic-ai/docs/results.md', 'section_title': 'Streamed Results'}}, {'source_file': 'pydantic-ai/docs/results.md', 'title': 'Examples', 'content': '## Examples\\n\\nThe following examples demonstrate how to use streamed responses in PydanticAI:\\n\\n- [Stream markdown](examples/stream-markdown.md)\\n- [Stream Whales](examples/stream-whales.md)', 'metadata': {'file_path': 'pydantic-ai/docs/results.md', 'section_title': 'Examples'}}, {'source_file': 'pydantic-ai/docs/index.md', 'title': 'Introduction {.hide}', 'content': '# Introduction {.hide}\\n\\n--8<-- \"docs/.partials/index-header.html\"\\n\\nPydanticAI is a Python Agent Framework designed to make it less painful to\\nbuild production grade applications with Generative AI.\\n\\nFastAPI revolutionized web development by offering an innovative and ergonomic design, built on the foundation of [Pydantic](https://docs.pydantic.dev).\\n\\nSimilarly, virtually every agent framework and LLM library in Python uses Pydantic, yet when we began to use LLMs in [Pydantic Logfire](https://pydantic.dev/logfire), we couldn\\'t find anything that gave us the same feeling.\\n\\nWe built PydanticAI with one simple aim: to bring that FastAPI feeling to GenAI app development.', 'metadata': {'file_path': 'pydantic-ai/docs/index.md', 'section_title': 'Introduction {.hide}'}}, {'source_file': 'pydantic-ai/docs/index.md', 'title': 'Why use PydanticAI', 'content': '## Why use PydanticAI\\n\\n:material-account-group:{ .md .middle .team-blue }&nbsp;<strong class=\"vertical-middle\">Built by the Pydantic Team</strong><br>\\nBuilt by the team behind [Pydantic](https://docs.pydantic.dev/latest/) (the validation layer of the OpenAI SDK, the Anthropic SDK, LangChain, LlamaIndex, AutoGPT, Transformers, CrewAI, Instructor and many more).\\n\\n:fontawesome-solid-shapes:{ .md .middle .shapes-orange }&nbsp;<strong class=\"vertical-middle\">Model-agnostic</strong><br>\\nSupports OpenAI, Anthropic, Gemini, Ollama, Groq, and Mistral, and there is a simple interface to implement support for [other models](models.md).\\n\\n:logfire-logo:{ .md .middle }&nbsp;<strong class=\"vertical-middle\">Pydantic Logfire Integration</strong><br>\\nSeamlessly [integrates](logfire.md) with [Pydantic Logfire](https://pydantic.dev/logfire) for real-time debugging, performance monitoring, and behavior tracking of your LLM-powered applications.\\n\\n:material-shield-check:{ .md .middle .secure-green }&nbsp;<strong class=\"vertical-middle\">Type-safe</strong><br>\\nDesigned to make [type checking](agents.md#static-type-checking) as powerful and informative as possible for you.\\n\\n:snake:{ .md .middle }&nbsp;<strong class=\"vertical-middle\">Python-centric Design</strong><br>\\nLeverages Python\\'s familiar control flow and agent composition to build your AI-driven projects, making it easy to apply standard Python best practices you\\'d use in any other (non-AI) project.\\n\\n:simple-pydantic:{ .md .middle .pydantic-pink }&nbsp;<strong class=\"vertical-middle\">Structured Responses</strong><br>\\nHarnesses the power of [Pydantic](https://docs.pydantic.dev/latest/) to [validate and structure](results.md#structured-result-validation) model outputs, ensuring responses are consistent across runs.\\n\\n:material-puzzle-plus:{ .md .middle .puzzle-purple }&nbsp;<strong class=\"vertical-middle\">Dependency Injection System</strong><br>\\nOffers an optional [dependency injection](dependencies.md) system to provide data and services to your agent\\'s [system prompts](agents.md#system-prompts), [tools](tools.md) and [result validators](results.md#result-validators-functions).\\nThis is useful for testing and eval-driven iterative development.\\n\\n:material-sine-wave:{ .md .middle }&nbsp;<strong class=\"vertical-middle\">Streamed Responses</strong><br>\\nProvides the ability to [stream](results.md#streamed-results) LLM outputs continuously, with immediate validation, ensuring rapid and accurate results.\\n\\n:material-graph:{ .md .middle .graph-green }&nbsp;<strong class=\"vertical-middle\">Graph Support</strong><br>\\n[Pydantic Graph](graph.md) provides a powerful way to define graphs using typing hints, this is useful in complex applications where standard control flow can degrade to spaghetti code.\\n\\n!!! example \"In Beta\"\\n    PydanticAI is in early beta, the API is still subject to change and there\\'s a lot more to do.\\n    [Feedback](https://github.com/pydantic/pydantic-ai/issues) is very welcome!', 'metadata': {'file_path': 'pydantic-ai/docs/index.md', 'section_title': 'Why use PydanticAI'}}, {'source_file': 'pydantic-ai/docs/index.md', 'title': 'Hello World Example', 'content': '## Hello World Example\\n\\nHere\\'s a minimal example of PydanticAI:\\n\\n```python {title=\"hello_world.py\"}\\nfrom pydantic_ai import Agent\\n\\nagent = Agent(  # (1)!\\n    \\'gemini-1.5-flash\\',\\n    system_prompt=\\'Be concise, reply with one sentence.\\',  # (2)!\\n)\\n\\nresult = agent.run_sync(\\'Where does \"hello world\" come from?\\')  # (3)!\\nprint(result.data)\\n\"\"\"\\nThe first known use of \"hello, world\" was in a 1974 textbook about the C programming language.\\n\"\"\"\\n```\\n\\n1. We configure the agent to use [Gemini 1.5\\'s Flash](api/models/gemini.md) model, but you can also set the model when running the agent.\\n2. Register a static [system prompt](agents.md#system-prompts) using a keyword argument to the agent.\\n3. [Run the agent](agents.md#running-agents) synchronously, conducting a conversation with the LLM.\\n\\n_(This example is complete, it can be run \"as is\")_\\n\\nThe exchange should be very short: PydanticAI will send the system prompt and the user query to the LLM, the model will return a text response.\\n\\nNot very interesting yet, but we can easily add \"tools\", dynamic system prompts, and structured responses to build more powerful agents.', 'metadata': {'file_path': 'pydantic-ai/docs/index.md', 'section_title': 'Hello World Example'}}, {'source_file': 'pydantic-ai/docs/index.md', 'title': 'Tools & Dependency Injection Example', 'content': '## Tools & Dependency Injection Example\\n\\nHere is a concise example using PydanticAI to build a support agent for a bank:\\n\\n```python {title=\"bank_support.py\"}\\nfrom dataclasses import dataclass\\n\\nfrom pydantic import BaseModel, Field\\nfrom pydantic_ai import Agent, RunContext\\n\\nfrom bank_database import DatabaseConn\\n\\n\\n@dataclass\\nclass SupportDependencies:  # (3)!\\n    customer_id: int\\n    db: DatabaseConn  # (12)!\\n\\n\\nclass SupportResult(BaseModel):  # (13)!\\n    support_advice: str = Field(description=\\'Advice returned to the customer\\')\\n    block_card: bool = Field(description=\"Whether to block the customer\\'s card\")\\n    risk: int = Field(description=\\'Risk level of query\\', ge=0, le=10)\\n\\n\\nsupport_agent = Agent(  # (1)!\\n    \\'openai:gpt-4o\\',  # (2)!\\n    deps_type=SupportDependencies,\\n    result_type=SupportResult,  # (9)!\\n    system_prompt=(  # (4)!\\n        \\'You are a support agent in our bank, give the \\'\\n        \\'customer support and judge the risk level of their query.\\'\\n    ),\\n)\\n\\n\\n@support_agent.system_prompt  # (5)!\\nasync def add_customer_name(ctx: RunContext[SupportDependencies]) -> str:\\n    customer_name = await ctx.deps.db.customer_name(id=ctx.deps.customer_id)\\n    return f\"The customer\\'s name is {customer_name!r}\"\\n\\n\\n@support_agent.tool  # (6)!\\nasync def customer_balance(\\n    ctx: RunContext[SupportDependencies], include_pending: bool\\n) -> float:\\n    \"\"\"Returns the customer\\'s current account balance.\"\"\"  # (7)!\\n    return await ctx.deps.db.customer_balance(\\n        id=ctx.deps.customer_id,\\n        include_pending=include_pending,\\n    )\\n\\n\\n...  # (11)!\\n\\n\\nasync def main():\\n    deps = SupportDependencies(customer_id=123, db=DatabaseConn())\\n    result = await support_agent.run(\\'What is my balance?\\', deps=deps)  # (8)!\\n    print(result.data)  # (10)!\\n    \"\"\"\\n    support_advice=\\'Hello John, your current account balance, including pending transactions, is $123.45.\\' block_card=False risk=1\\n    \"\"\"\\n\\n    result = await support_agent.run(\\'I just lost my card!\\', deps=deps)\\n    print(result.data)\\n    \"\"\"\\n    support_advice=\"I\\'m sorry to hear that, John. We are temporarily blocking your card to prevent unauthorized transactions.\" block_card=True risk=8\\n    \"\"\"\\n```\\n\\n1. This [agent](agents.md) will act as first-tier support in a bank. Agents are generic in the type of dependencies they accept and the type of result they return. In this case, the support agent has type `#!python Agent[SupportDependencies, SupportResult]`.\\n2. Here we configure the agent to use [OpenAI\\'s GPT-4o model](api/models/openai.md), you can also set the model when running the agent.\\n3. The `SupportDependencies` dataclass is used to pass data, connections, and logic into the model that will be needed when running [system prompt](agents.md#system-prompts) and [tool](tools.md) functions. PydanticAI\\'s system of dependency injection provides a [type-safe](agents.md#static-type-checking) way to customise the behavior of your agents, and can be especially useful when running [unit tests](testing-evals.md) and evals.\\n4. Static [system prompts](agents.md#system-prompts) can be registered with the [`system_prompt` keyword argument][pydantic_ai.Agent.__init__] to the agent.\\n5. Dynamic [system prompts](agents.md#system-prompts) can be registered with the [`@agent.system_prompt`][pydantic_ai.Agent.system_prompt] decorator, and can make use of dependency injection. Dependencies are carried via the [`RunContext`][pydantic_ai.tools.RunContext] argument, which is parameterized with the `deps_type` from above. If the type annotation here is wrong, static type checkers will catch it.\\n6. [`tool`](tools.md) let you register functions which the LLM may call while responding to a user. Again, dependencies are carried via [`RunContext`][pydantic_ai.tools.RunContext], any other arguments become the tool schema passed to the LLM. Pydantic is used to validate these arguments, and errors are passed back to the LLM so it can retry.\\n7. The docstring of a tool is also passed to the LLM as the description of the tool. Parameter descriptions are [extracted](tools.md#function-tools-and-schema) from the docstring and added to the parameter schema sent to the LLM.\\n8. [Run the agent](agents.md#running-agents) asynchronously, conducting a conversation with the LLM until a final response is reached. Even in this fairly simple case, the agent will exchange multiple messages with the LLM as tools are called to retrieve a result.\\n9. The response from the agent will, be guaranteed to be a `SupportResult`, if validation fails [reflection](agents.md#reflection-and-self-correction) will mean the agent is prompted to try again.\\n10. The result will be validated with Pydantic to guarantee it is a `SupportResult`, since the agent is generic, it\\'ll also be typed as a `SupportResult` to aid with static type checking.\\n11. In a real use case, you\\'d add more tools and a longer system prompt to the agent to extend the context it\\'s equipped with and support it can provide.\\n12. This is a simple sketch of a database connection, used to keep the example short and readable. In reality, you\\'d be connecting to an external database (e.g. PostgreSQL) to get information about customers.\\n13. This [Pydantic](https://docs.pydantic.dev) model is used to constrain the structured data returned by the agent. From this simple definition, Pydantic builds the JSON Schema that tells the LLM how to return the data, and performs validation to guarantee the data is correct at the end of the run.\\n\\n!!! tip \"Complete `bank_support.py` example\"\\n    The code included here is incomplete for the sake of brevity (the definition of `DatabaseConn` is missing); you can find the complete `bank_support.py` example [here](examples/bank-support.md).', 'metadata': {'file_path': 'pydantic-ai/docs/index.md', 'section_title': 'Tools & Dependency Injection Example'}}, {'source_file': 'pydantic-ai/docs/index.md', 'title': 'Instrumentation with Pydantic Logfire', 'content': '## Instrumentation with Pydantic Logfire\\n\\nTo understand the flow of the above runs, we can watch the agent in action using Pydantic Logfire.\\n\\nTo do this, we need to set up logfire, and add the following to our code:\\n\\n```python {title=\"bank_support_with_logfire.py\" hl_lines=\"4-6\" test=\"skip\" lint=\"skip\"}\\n...\\nfrom bank_database import DatabaseConn\\n\\nimport logfire\\nlogfire.configure()  # (1)!\\nlogfire.instrument_asyncpg()  # (2)!\\n...\\n```\\n\\n1. Configure logfire, this will fail if not project is set up.\\n2. In our demo, `DatabaseConn` uses [`asyncpg`]() to connect to a PostgreSQL database, so [`logfire.instrument_asyncpg()`](https://magicstack.github.io/asyncpg/current/) is used to log the database queries.\\n\\nThat\\'s enough to get the following view of your agent in action:\\n\\n{{ video(\\'9078b98c4f75d01f912a0368bbbdb97a\\', 25, 55) }}\\n\\nSee [Monitoring and Performance](logfire.md) to learn more.', 'metadata': {'file_path': 'pydantic-ai/docs/index.md', 'section_title': 'Instrumentation with Pydantic Logfire'}}, {'source_file': 'pydantic-ai/docs/index.md', 'title': 'Next Steps', 'content': \"## Next Steps\\n\\nTo try PydanticAI yourself, follow the instructions [in the examples](examples/index.md).\\n\\nRead the [docs](agents.md) to learn more about building applications with PydanticAI.\\n\\nRead the [API Reference](api/agent.md) to understand PydanticAI's interface.\", 'metadata': {'file_path': 'pydantic-ai/docs/index.md', 'section_title': 'Next Steps'}}, {'source_file': 'pydantic-ai/docs/models.md', 'title': 'No Header', 'content': 'PydanticAI is Model-agnostic and has built in support for the following model providers:\\n\\n* [OpenAI](#openai)\\n* [Anthropic](#anthropic)\\n* Gemini via two different APIs: [Generative Language API](#gemini) and [VertexAI API](#gemini-via-vertexai)\\n* [Ollama](#ollama)\\n* [Groq](#groq)\\n* [Mistral](#mistral)\\n\\nSee [OpenAI-compatible models](#openai-compatible-models) for more examples on how to use models such as [OpenRouter](#openrouter), [Grok (xAI)](#grok-xai) and [DeepSeek](#deepseek) that support the OpenAI SDK.\\n\\nYou can also [add support for other models](#implementing-custom-models).\\n\\nPydanticAI also comes with [`TestModel`](api/models/test.md) and [`FunctionModel`](api/models/function.md) for testing and development.\\n\\nTo use each model provider, you need to configure your local environment and make sure you have the right packages installed.', 'metadata': {'file_path': 'pydantic-ai/docs/models.md', 'section_title': 'No Header'}}, {'source_file': 'pydantic-ai/docs/models.md', 'title': 'OpenAI', 'content': '## OpenAI\\n\\n### Install\\n\\nTo use OpenAI models, you need to either install [`pydantic-ai`](install.md), or install [`pydantic-ai-slim`](install.md#slim-install) with the `openai` optional group:\\n\\n```bash\\npip/uv-add \\'pydantic-ai-slim[openai]\\'\\n```\\n\\n### Configuration\\n\\nTo use [`OpenAIModel`][pydantic_ai.models.openai.OpenAIModel] through their main API, go to [platform.openai.com](https://platform.openai.com/) and follow your nose until you find the place to generate an API key.\\n\\n### Environment variable\\n\\nOnce you have the API key, you can set it as an environment variable:\\n\\n```bash\\nexport OPENAI_API_KEY=\\'your-api-key\\'\\n```\\n\\nYou can then use [`OpenAIModel`][pydantic_ai.models.openai.OpenAIModel] by name:\\n\\n```python {title=\"openai_model_by_name.py\"}\\nfrom pydantic_ai import Agent\\n\\nagent = Agent(\\'openai:gpt-4o\\')\\n...\\n```\\n\\nOr initialise the model directly with just the model name:\\n\\n```python {title=\"openai_model_init.py\"}\\nfrom pydantic_ai import Agent\\nfrom pydantic_ai.models.openai import OpenAIModel\\n\\nmodel = OpenAIModel(\\'gpt-4o\\')\\nagent = Agent(model)\\n...\\n```\\n\\n### `api_key` argument\\n\\nIf you don\\'t want to or can\\'t set the environment variable, you can pass it at runtime via the [`api_key` argument][pydantic_ai.models.openai.OpenAIModel.__init__]:\\n\\n```python {title=\"openai_model_api_key.py\"}\\nfrom pydantic_ai import Agent\\nfrom pydantic_ai.models.openai import OpenAIModel\\n\\nmodel = OpenAIModel(\\'gpt-4o\\', api_key=\\'your-api-key\\')\\nagent = Agent(model)\\n...\\n```\\n\\n### Custom OpenAI Client\\n\\n`OpenAIModel` also accepts a custom `AsyncOpenAI` client via the [`openai_client` parameter][pydantic_ai.models.openai.OpenAIModel.__init__],\\nso you can customise the `organization`, `project`, `base_url` etc. as defined in the [OpenAI API docs](https://platform.openai.com/docs/api-reference).\\n\\nYou could also use the [`AsyncAzureOpenAI`](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/switching-endpoints) client to use the Azure OpenAI API.\\n\\n```python {title=\"openai_azure.py\"}\\nfrom openai import AsyncAzureOpenAI\\n\\nfrom pydantic_ai import Agent\\nfrom pydantic_ai.models.openai import OpenAIModel\\n\\nclient = AsyncAzureOpenAI(\\n    azure_endpoint=\\'...\\',\\n    api_version=\\'2024-07-01-preview\\',\\n    api_key=\\'your-api-key\\',\\n)\\n\\nmodel = OpenAIModel(\\'gpt-4o\\', openai_client=client)\\nagent = Agent(model)\\n...\\n```', 'metadata': {'file_path': 'pydantic-ai/docs/models.md', 'section_title': 'OpenAI'}}, {'source_file': 'pydantic-ai/docs/models.md', 'title': 'Anthropic', 'content': '## Anthropic\\n\\n### Install\\n\\nTo use [`AnthropicModel`][pydantic_ai.models.anthropic.AnthropicModel] models, you need to either install [`pydantic-ai`](install.md), or install [`pydantic-ai-slim`](install.md#slim-install) with the `anthropic` optional group:\\n\\n```bash\\npip/uv-add \\'pydantic-ai-slim[anthropic]\\'\\n```\\n\\n### Configuration\\n\\nTo use [Anthropic](https://anthropic.com) through their API, go to [console.anthropic.com/settings/keys](https://console.anthropic.com/settings/keys) to generate an API key.\\n\\n[`AnthropicModelName`][pydantic_ai.models.anthropic.AnthropicModelName] contains a list of available Anthropic models.\\n\\n### Environment variable\\n\\nOnce you have the API key, you can set it as an environment variable:\\n\\n```bash\\nexport ANTHROPIC_API_KEY=\\'your-api-key\\'\\n```\\n\\nYou can then use [`AnthropicModel`][pydantic_ai.models.anthropic.AnthropicModel] by name:\\n\\n```py title=\"anthropic_model_by_name.py\"\\nfrom pydantic_ai import Agent\\n\\nagent = Agent(\\'claude-3-5-sonnet-latest\\')\\n...\\n```\\n\\nOr initialise the model directly with just the model name:\\n\\n```py title=\"anthropic_model_init.py\"\\nfrom pydantic_ai import Agent\\nfrom pydantic_ai.models.anthropic import AnthropicModel\\n\\nmodel = AnthropicModel(\\'claude-3-5-sonnet-latest\\')\\nagent = Agent(model)\\n...\\n```\\n\\n### `api_key` argument\\n\\nIf you don\\'t want to or can\\'t set the environment variable, you can pass it at runtime via the [`api_key` argument][pydantic_ai.models.anthropic.AnthropicModel.__init__]:\\n\\n```py title=\"anthropic_model_api_key.py\"\\nfrom pydantic_ai import Agent\\nfrom pydantic_ai.models.anthropic import AnthropicModel\\n\\nmodel = AnthropicModel(\\'claude-3-5-sonnet-latest\\', api_key=\\'your-api-key\\')\\nagent = Agent(model)\\n...\\n```', 'metadata': {'file_path': 'pydantic-ai/docs/models.md', 'section_title': 'Anthropic'}}, {'source_file': 'pydantic-ai/docs/models.md', 'title': 'Gemini', 'content': '## Gemini\\n\\n!!! warning \"For prototyping only\"\\n    Google themselves refer to this API as the \"hobby\" API, I\\'ve received 503 responses from it a number of times.\\n    The API is easy to use and useful for prototyping and simple demos, but I would not rely on it in production.\\n\\n    If you want to run Gemini models in production, you should use the [VertexAI API](#gemini-via-vertexai) described below.\\n\\n### Install\\n\\nTo use [`GeminiModel`][pydantic_ai.models.gemini.GeminiModel] models, you just need to install [`pydantic-ai`](install.md) or [`pydantic-ai-slim`](install.md#slim-install), no extra dependencies are required.\\n\\n### Configuration\\n\\n[`GeminiModel`][pydantic_ai.models.gemini.GeminiModel] let\\'s you use the Google\\'s Gemini models through their [Generative Language API](https://ai.google.dev/api/all-methods), `generativelanguage.googleapis.com`.\\n\\n[`GeminiModelName`][pydantic_ai.models.gemini.GeminiModelName] contains a list of available Gemini models that can be used through this interface.\\n\\nTo use `GeminiModel`, go to [aistudio.google.com](https://aistudio.google.com/) and follow your nose until you find the place to generate an API key.\\n\\n### Environment variable\\n\\nOnce you have the API key, you can set it as an environment variable:\\n\\n```bash\\nexport GEMINI_API_KEY=your-api-key\\n```\\n\\nYou can then use [`GeminiModel`][pydantic_ai.models.gemini.GeminiModel] by name:\\n\\n```python {title=\"gemini_model_by_name.py\"}\\nfrom pydantic_ai import Agent\\n\\nagent = Agent(\\'google-gla:gemini-1.5-flash\\')\\n...\\n```\\n\\n!!! note\\n    The `google-gla` provider prefix represents the [Google **G**enerative **L**anguage **A**PI](https://ai.google.dev/api/all-methods) for `GeminiModel`s.\\n    `google-vertex` is used with [Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models) for `VertexAIModel`s.\\n\\nOr initialise the model directly with just the model name:\\n\\n```python {title=\"gemini_model_init.py\"}\\nfrom pydantic_ai import Agent\\nfrom pydantic_ai.models.gemini import GeminiModel\\n\\nmodel = GeminiModel(\\'gemini-1.5-flash\\')\\nagent = Agent(model)\\n...\\n```\\n\\n### `api_key` argument\\n\\nIf you don\\'t want to or can\\'t set the environment variable, you can pass it at runtime via the [`api_key` argument][pydantic_ai.models.gemini.GeminiModel.__init__]:\\n\\n```python {title=\"gemini_model_api_key.py\"}\\nfrom pydantic_ai import Agent\\nfrom pydantic_ai.models.gemini import GeminiModel\\n\\nmodel = GeminiModel(\\'gemini-1.5-flash\\', api_key=\\'your-api-key\\')\\nagent = Agent(model)\\n...\\n```', 'metadata': {'file_path': 'pydantic-ai/docs/models.md', 'section_title': 'Gemini'}}, {'source_file': 'pydantic-ai/docs/models.md', 'title': 'Gemini via VertexAI', 'content': '## Gemini via VertexAI\\n\\nTo run Google\\'s Gemini models in production, you should use [`VertexAIModel`][pydantic_ai.models.vertexai.VertexAIModel] which uses the `*-aiplatform.googleapis.com` API.\\n\\n[`GeminiModelName`][pydantic_ai.models.gemini.GeminiModelName] contains a list of available Gemini models that can be used through this interface.\\n\\n### Install\\n\\nTo use [`VertexAIModel`][pydantic_ai.models.vertexai.VertexAIModel], you need to either install [`pydantic-ai`](install.md), or install [`pydantic-ai-slim`](install.md#slim-install) with the `vertexai` optional group:\\n\\n```bash\\npip/uv-add \\'pydantic-ai-slim[vertexai]\\'\\n```\\n\\n### Configuration\\n\\nThis interface has a number of advantages over `generativelanguage.googleapis.com` documented above:\\n\\n1. The VertexAI API is more reliably and marginally lower latency in our experience.\\n2. You can\\n   [purchase provisioned throughput](https://cloud.google.com/vertex-ai/generative-ai/docs/provisioned-throughput#purchase-provisioned-throughput)\\n   with VertexAI to guarantee capacity.\\n3. If you\\'re running PydanticAI inside GCP, you don\\'t need to set up authentication, it should \"just work\".\\n4. You can decide which region to use, which might be important from a regulatory perspective,\\n   and might improve latency.\\n\\nThe big disadvantage is that for local development you may need to create and configure a \"service account\", which I\\'ve found extremely painful to get right in the past.\\n\\nWhichever way you authenticate, you\\'ll need to have VertexAI enabled in your GCP account.\\n\\n### Application default credentials\\n\\nLuckily if you\\'re running PydanticAI inside GCP, or you have the [`gcloud` CLI](https://cloud.google.com/sdk/gcloud) installed and configured, you should be able to use `VertexAIModel` without any additional setup.\\n\\nTo use `VertexAIModel`, with [application default credentials](https://cloud.google.com/docs/authentication/application-default-credentials) configured (e.g. with `gcloud`), you can simply use:\\n\\n```python {title=\"vertexai_application_default_credentials.py\"}\\nfrom pydantic_ai import Agent\\nfrom pydantic_ai.models.vertexai import VertexAIModel\\n\\nmodel = VertexAIModel(\\'gemini-1.5-flash\\')\\nagent = Agent(model)\\n...\\n```\\n\\nInternally this uses [`google.auth.default()`](https://google-auth.readthedocs.io/en/master/reference/google.auth.html) from the `google-auth` package to obtain credentials.\\n\\n!!! note \"Won\\'t fail until `agent.run()`\"\\n    Because `google.auth.default()` requires network requests and can be slow, it\\'s not run until you call `agent.run()`. Meaning any configuration or permissions error will only be raised when you try to use the model. To for this check to be run, call [`await model.ainit()`][pydantic_ai.models.vertexai.VertexAIModel.ainit].\\n\\nYou may also need to pass the [`project_id` argument to `VertexAIModel`][pydantic_ai.models.vertexai.VertexAIModel.__init__] if application default credentials don\\'t set a project, if you pass `project_id` and it conflicts with the project set by application default credentials, an error is raised.\\n\\n### Service account\\n\\nIf instead of application default credentials, you want to authenticate with a service account, you\\'ll need to create a service account, add it to your GCP project (note: AFAIK this step is necessary even if you created the service account within the project), give that service account the \"Vertex AI Service Agent\" role, and download the service account JSON file.\\n\\nOnce you have the JSON file, you can use it thus:\\n\\n```python {title=\"vertexai_service_account.py\"}\\nfrom pydantic_ai import Agent\\nfrom pydantic_ai.models.vertexai import VertexAIModel\\n\\nmodel = VertexAIModel(\\n    \\'gemini-1.5-flash\\',\\n    service_account_file=\\'path/to/service-account.json\\',\\n)\\nagent = Agent(model)\\n...\\n```\\n\\n### Customising region\\n\\nWhichever way you authenticate, you can specify which region requests will be sent to via the [`region` argument][pydantic_ai.models.vertexai.VertexAIModel.__init__].\\n\\nUsing a region close to your application can improve latency and might be important from a regulatory perspective.\\n\\n```python {title=\"vertexai_region.py\"}\\nfrom pydantic_ai import Agent\\nfrom pydantic_ai.models.vertexai import VertexAIModel\\n\\nmodel = VertexAIModel(\\'gemini-1.5-flash\\', region=\\'asia-east1\\')\\nagent = Agent(model)\\n...\\n```\\n\\n[`VertexAiRegion`][pydantic_ai.models.vertexai.VertexAiRegion] contains a list of available regions.', 'metadata': {'file_path': 'pydantic-ai/docs/models.md', 'section_title': 'Gemini via VertexAI'}}, {'source_file': 'pydantic-ai/docs/models.md', 'title': 'Ollama', 'content': \"## Ollama\\n\\n### Install\\n\\nTo use [`OllamaModel`][pydantic_ai.models.ollama.OllamaModel], you need to either install [`pydantic-ai`](install.md), or install [`pydantic-ai-slim`](install.md#slim-install) with the `openai` optional group:\\n\\n```bash\\npip/uv-add 'pydantic-ai-slim[openai]'\\n```\\n\\n**This is because internally, `OllamaModel` uses the OpenAI API.**\\n\\n### Configuration\\n\\nTo use [Ollama](https://ollama.com/), you must first download the Ollama client, and then download a model using the [Ollama model library](https://ollama.com/library).\\n\\nYou must also ensure the Ollama server is running when trying to make requests to it. For more information, please see the [Ollama documentation](https://github.com/ollama/ollama/tree/main/docs).\\n\\nFor detailed setup and example, please see the [Ollama setup documentation](https://github.com/pydantic/pydantic-ai/blob/main/docs/api/models/ollama.md).\", 'metadata': {'file_path': 'pydantic-ai/docs/models.md', 'section_title': 'Ollama'}}, {'source_file': 'pydantic-ai/docs/models.md', 'title': 'Groq', 'content': '## Groq\\n\\n### Install\\n\\nTo use [`GroqModel`][pydantic_ai.models.groq.GroqModel], you need to either install [`pydantic-ai`](install.md), or install [`pydantic-ai-slim`](install.md#slim-install) with the `groq` optional group:\\n\\n```bash\\npip/uv-add \\'pydantic-ai-slim[groq]\\'\\n```\\n\\n### Configuration\\n\\nTo use [Groq](https://groq.com/) through their API, go to [console.groq.com/keys](https://console.groq.com/keys) and follow your nose until you find the place to generate an API key.\\n\\n[`GroqModelName`][pydantic_ai.models.groq.GroqModelName] contains a list of available Groq models.\\n\\n### Environment variable\\n\\nOnce you have the API key, you can set it as an environment variable:\\n\\n```bash\\nexport GROQ_API_KEY=\\'your-api-key\\'\\n```\\n\\nYou can then use [`GroqModel`][pydantic_ai.models.groq.GroqModel] by name:\\n\\n```python {title=\"groq_model_by_name.py\"}\\nfrom pydantic_ai import Agent\\n\\nagent = Agent(\\'groq:llama-3.1-70b-versatile\\')\\n...\\n```\\n\\nOr initialise the model directly with just the model name:\\n\\n```python {title=\"groq_model_init.py\"}\\nfrom pydantic_ai import Agent\\nfrom pydantic_ai.models.groq import GroqModel\\n\\nmodel = GroqModel(\\'llama-3.1-70b-versatile\\')\\nagent = Agent(model)\\n...\\n```\\n\\n### `api_key` argument\\n\\nIf you don\\'t want to or can\\'t set the environment variable, you can pass it at runtime via the [`api_key` argument][pydantic_ai.models.groq.GroqModel.__init__]:\\n\\n```python {title=\"groq_model_api_key.py\"}\\nfrom pydantic_ai import Agent\\nfrom pydantic_ai.models.groq import GroqModel\\n\\nmodel = GroqModel(\\'llama-3.1-70b-versatile\\', api_key=\\'your-api-key\\')\\nagent = Agent(model)\\n...\\n```', 'metadata': {'file_path': 'pydantic-ai/docs/models.md', 'section_title': 'Groq'}}, {'source_file': 'pydantic-ai/docs/models.md', 'title': 'Mistral', 'content': '## Mistral\\n\\n### Install\\n\\nTo use [`MistralModel`][pydantic_ai.models.mistral.MistralModel], you need to either install [`pydantic-ai`](install.md), or install [`pydantic-ai-slim`](install.md#slim-install) with the `mistral` optional group:\\n\\n```bash\\npip/uv-add \\'pydantic-ai-slim[mistral]\\'\\n```\\n\\n### Configuration\\n\\nTo use [Mistral](https://mistral.ai) through their API, go to [console.mistral.ai/api-keys/](https://console.mistral.ai/api-keys/) and follow your nose until you find the place to generate an API key.\\n\\n[`NamedMistralModels`][pydantic_ai.models.mistral.NamedMistralModels] contains a list of the most popular Mistral models.\\n\\n### Environment variable\\n\\nOnce you have the API key, you can set it as an environment variable:\\n\\n```bash\\nexport MISTRAL_API_KEY=\\'your-api-key\\'\\n```\\n\\nYou can then use [`MistralModel`][pydantic_ai.models.mistral.MistralModel] by name:\\n\\n```python {title=\"mistral_model_by_name.py\"}\\nfrom pydantic_ai import Agent\\n\\nagent = Agent(\\'mistral:mistral-large-latest\\')\\n...\\n```\\n\\nOr initialise the model directly with just the model name:\\n\\n```python {title=\"mistral_model_init.py\"}\\nfrom pydantic_ai import Agent\\nfrom pydantic_ai.models.mistral import MistralModel\\n\\nmodel = MistralModel(\\'mistral-small-latest\\')\\nagent = Agent(model)\\n...\\n```\\n\\n### `api_key` argument\\n\\nIf you don\\'t want to or can\\'t set the environment variable, you can pass it at runtime via the [`api_key` argument][pydantic_ai.models.mistral.MistralModel.__init__]:\\n\\n```python {title=\"mistral_model_api_key.py\"}\\nfrom pydantic_ai import Agent\\nfrom pydantic_ai.models.mistral import MistralModel\\n\\nmodel = MistralModel(\\'mistral-small-latest\\', api_key=\\'your-api-key\\')\\nagent = Agent(model)\\n...\\n```', 'metadata': {'file_path': 'pydantic-ai/docs/models.md', 'section_title': 'Mistral'}}, {'source_file': 'pydantic-ai/docs/models.md', 'title': 'OpenAI-compatible Models', 'content': '## OpenAI-compatible Models\\n\\nMany of the models are compatible with OpenAI API, and thus can be used with [`OpenAIModel`][pydantic_ai.models.openai.OpenAIModel] in PydanticAI.\\nBefore getting started, check the [OpenAI](#openai) section for installation and configuration instructions.\\n\\nTo use another OpenAI-compatible API, you can make use of the [`base_url`][pydantic_ai.models.openai.OpenAIModel.__init__] and [`api_key`][pydantic_ai.models.openai.OpenAIModel.__init__] arguments:\\n\\n```python {title=\"openai_model_base_url.py\" hl_lines=\"5-6\"}\\nfrom pydantic_ai.models.openai import OpenAIModel\\n\\nmodel = OpenAIModel(\\n    \\'model_name\\',\\n    base_url=\\'https://<openai-compatible-api-endpoint>.com\\',\\n    api_key=\\'your-api-key\\',\\n)\\n...\\n```\\n\\n### OpenRouter\\n\\nTo use [OpenRouter](https://openrouter.ai), first create an API key at [openrouter.ai/keys](https://openrouter.ai/keys).\\n\\nOnce you have the API key, you can pass it to [`OpenAIModel`][pydantic_ai.models.openai.OpenAIModel] as the `api_key` argument:\\n\\n```python {title=\"openrouter_model_init.py\"}\\nfrom pydantic_ai import Agent\\nfrom pydantic_ai.models.openai import OpenAIModel\\n\\nmodel = OpenAIModel(\\n    \\'anthropic/claude-3.5-sonnet\\',\\n    base_url=\\'https://openrouter.ai/api/v1\\',\\n    api_key=\\'your-openrouter-api-key\\',\\n)\\nagent = Agent(model)\\n...\\n```\\n\\n### Grok (xAI)\\n\\nGo to [xAI API Console](https://console.x.ai/) and create an API key.\\nOnce you have the API key, follow the [xAI API Documentation](https://docs.x.ai/docs/overview), and set the `base_url` and `api_key` arguments appropriately:\\n\\n```python {title=\"grok_model_init.py\"}\\nfrom pydantic_ai import Agent\\nfrom pydantic_ai.models.openai import OpenAIModel\\n\\nmodel = OpenAIModel(\\n    \\'grok-2-1212\\',\\n    base_url=\\'https://api.x.ai/v1\\',\\n    api_key=\\'your-xai-api-key\\',\\n)\\nagent = Agent(model)\\n...\\n```\\n\\n### DeepSeek\\n\\nGo to [DeepSeek API Platform](https://platform.deepseek.com/api_keys) and create an API key.\\nOnce you have the API key, follow the [DeepSeek API Documentation](https://platform.deepseek.com/docs/api/overview), and set the `base_url` and `api_key` arguments appropriately:\\n\\n```python {title=\"deepseek_model_init.py\"}\\nfrom pydantic_ai import Agent\\nfrom pydantic_ai.models.openai import OpenAIModel\\n\\nmodel = OpenAIModel(\\n    \\'deepseek-chat\\',\\n    base_url=\\'https://api.deepseek.com\\',\\n    api_key=\\'your-deepseek-api-key\\',\\n)\\nagent = Agent(model)\\n...\\n```', 'metadata': {'file_path': 'pydantic-ai/docs/models.md', 'section_title': 'OpenAI-compatible Models'}}, {'source_file': 'pydantic-ai/docs/models.md', 'title': 'Implementing Custom Models', 'content': \"## Implementing Custom Models\\n\\nTo implement support for models not already supported, you will need to subclass the [`Model`][pydantic_ai.models.Model] abstract base class.\\n\\nThis in turn will require you to implement the following other abstract base classes:\\n\\n* [`AgentModel`][pydantic_ai.models.AgentModel]\\n* [`StreamedResponse`][pydantic_ai.models.StreamedResponse]\\n\\nThe best place to start is to review the source code for existing implementations, e.g. [`OpenAIModel`](https://github.com/pydantic/pydantic-ai/blob/main/pydantic_ai_slim/pydantic_ai/models/openai.py).\\n\\nFor details on when we'll accept contributions adding new models to PydanticAI, see the [contributing guidelines](contributing.md#new-model-rules).\", 'metadata': {'file_path': 'pydantic-ai/docs/models.md', 'section_title': 'Implementing Custom Models'}}, {'source_file': 'pydantic-ai/docs/contributing.md', 'title': 'No Header', 'content': \"We'd love you to contribute to PydanticAI!\", 'metadata': {'file_path': 'pydantic-ai/docs/contributing.md', 'section_title': 'No Header'}}, {'source_file': 'pydantic-ai/docs/contributing.md', 'title': 'Installation and Setup', 'content': '## Installation and Setup\\n\\nClone your fork and cd into the repo directory\\n\\n```bash\\ngit clone git@github.com:<your username>/pydantic-ai.git\\ncd pydantic-ai\\n```\\n\\nInstall `uv` (version 0.4.30 or later) and `pre-commit`\\n\\nWe use pipx here, for other options see:\\n\\n* [`uv` install docs](https://docs.astral.sh/uv/getting-started/installation/)\\n* [`pre-commit` install docs](https://pre-commit.com/#install)\\n\\nTo get `pipx` itself, see [these docs](https://pypa.github.io/pipx/)\\n\\n```bash\\npipx install uv pre-commit\\n```\\n\\nInstall `pydantic-ai`, all dependencies and pre-commit hooks\\n\\n```bash\\nmake install\\n```', 'metadata': {'file_path': 'pydantic-ai/docs/contributing.md', 'section_title': 'Installation and Setup'}}, {'source_file': 'pydantic-ai/docs/contributing.md', 'title': 'Running Tests etc.', 'content': \"## Running Tests etc.\\n\\nWe use `make` to manage most commands you'll need to run.\\n\\nFor details on available commands, run:\\n\\n```bash\\nmake help\\n```\\n\\nTo run code formatting, linting, static type checks, and tests with coverage report generation, run:\\n\\n```bash\\nmake\\n```\", 'metadata': {'file_path': 'pydantic-ai/docs/contributing.md', 'section_title': 'Running Tests etc.'}}, {'source_file': 'pydantic-ai/docs/contributing.md', 'title': 'Documentation Changes', 'content': '## Documentation Changes\\n\\nTo run the documentation page locally, run:\\n\\n```bash\\nuv run mkdocs serve\\n```', 'metadata': {'file_path': 'pydantic-ai/docs/contributing.md', 'section_title': 'Documentation Changes'}}, {'source_file': 'pydantic-ai/docs/contributing.md', 'title': 'Rules for adding new models to PydanticAI {#new-model-rules}', 'content': \"## Rules for adding new models to PydanticAI {#new-model-rules}\\n\\nTo avoid an excessive workload for the maintainers of PydanticAI, we can't accept all model contributions, so we're setting the following rules for when we'll accept new models and when we won't. This should hopefully reduce the chances of disappointment and wasted work.\\n\\n* To add a new model with an extra dependency, that dependency needs > 500k monthly downloads from PyPI consistently over 3 months or more\\n* To add a new model which uses another models logic internally and has no extra dependencies, that model's GitHub org needs > 20k stars in total\\n* For any other model that's just a custom URL and API key, we're happy to add a one-paragraph description with a link and instructions on the URL to use\\n* For any other model that requires more logic, we recommend you release your own Python package `pydantic-ai-xxx`, which depends on [`pydantic-ai-slim`](install.md#slim-install) and implements a model that inherits from our [`Model`][pydantic_ai.models.Model] ABC\\n\\nIf you're unsure about adding a model, please [create an issue](https://github.com/pydantic/pydantic-ai/issues).\", 'metadata': {'file_path': 'pydantic-ai/docs/contributing.md', 'section_title': 'Rules for adding new models to PydanticAI {#new-model-rules}'}}, {'source_file': 'pydantic-ai/docs/agents.md', 'title': 'Introduction', 'content': '## Introduction\\n\\nAgents are PydanticAI\\'s primary interface for interacting with LLMs.\\n\\nIn some use cases a single Agent will control an entire application or component,\\nbut multiple agents can also interact to embody more complex workflows.\\n\\nThe [`Agent`][pydantic_ai.Agent] class has full API documentation, but conceptually you can think of an agent as a container for:\\n\\n| **Component**                                   | **Description**                                                                                          |\\n|-------------------------------------------------|----------------------------------------------------------------------------------------------------------|\\n| [System prompt(s)](#system-prompts)             | A set of instructions for the LLM written by the developer.                                              |\\n| [Function tool(s)](tools.md)                    | Functions that the LLM may call to get information while generating a response.                          |\\n| [Structured result type](results.md)            | The structured datatype the LLM must return at the end of a run, if specified.                           |\\n| [Dependency type constraint](dependencies.md)   | System prompt functions, tools, and result validators may all use dependencies when they\\'re run.         |\\n| [LLM model](api/models/base.md)                 | Optional default LLM model associated with the agent. Can also be specified when running the agent.      |\\n| [Model Settings](#additional-configuration)     | Optional default model settings to help fine tune requests. Can also be specified when running the agent.|\\n\\nIn typing terms, agents are generic in their dependency and result types, e.g., an agent which required dependencies of type `#!python Foobar` and returned results of type `#!python list[str]` would have type `Agent[Foobar, list[str]]`. In practice, you shouldn\\'t need to care about this, it should just mean your IDE can tell you when you have the right type, and if you choose to use [static type checking](#static-type-checking) it should work well with PydanticAI.\\n\\nHere\\'s a toy example of an agent that simulates a roulette wheel:\\n\\n```python {title=\"roulette_wheel.py\"}\\nfrom pydantic_ai import Agent, RunContext\\n\\nroulette_agent = Agent(  # (1)!\\n    \\'openai:gpt-4o\\',\\n    deps_type=int,\\n    result_type=bool,\\n    system_prompt=(\\n        \\'Use the `roulette_wheel` function to see if the \\'\\n        \\'customer has won based on the number they provide.\\'\\n    ),\\n)\\n\\n\\n@roulette_agent.tool\\nasync def roulette_wheel(ctx: RunContext[int], square: int) -> str:  # (2)!\\n    \"\"\"check if the square is a winner\"\"\"\\n    return \\'winner\\' if square == ctx.deps else \\'loser\\'', 'metadata': {'file_path': 'pydantic-ai/docs/agents.md', 'section_title': 'Introduction'}}, {'source_file': 'pydantic-ai/docs/agents.md', 'title': 'Run the agent', 'content': '# Run the agent\\nsuccess_number = 18  # (3)!\\nresult = roulette_agent.run_sync(\\'Put my money on square eighteen\\', deps=success_number)\\nprint(result.data)  # (4)!\\n#> True\\n\\nresult = roulette_agent.run_sync(\\'I bet five is the winner\\', deps=success_number)\\nprint(result.data)\\n#> False\\n```\\n\\n1. Create an agent, which expects an integer dependency and returns a boolean result. This agent will have type `#!python Agent[int, bool]`.\\n2. Define a tool that checks if the square is a winner. Here [`RunContext`][pydantic_ai.tools.RunContext] is parameterized with the dependency type `int`; if you got the dependency type wrong you\\'d get a typing error.\\n3. In reality, you might want to use a random number here e.g. `random.randint(0, 36)`.\\n4. `result.data` will be a boolean indicating if the square is a winner. Pydantic performs the result validation, it\\'ll be typed as a `bool` since its type is derived from the `result_type` generic parameter of the agent.\\n\\n\\n!!! tip \"Agents are designed for reuse, like FastAPI Apps\"\\n    Agents are intended to be instantiated once (frequently as module globals) and reused throughout your application, similar to a small [FastAPI][fastapi.FastAPI] app or an [APIRouter][fastapi.APIRouter].', 'metadata': {'file_path': 'pydantic-ai/docs/agents.md', 'section_title': 'Run the agent'}}, {'source_file': 'pydantic-ai/docs/agents.md', 'title': 'Running Agents', 'content': '## Running Agents\\n\\nThere are three ways to run an agent:\\n\\n1. [`agent.run()`][pydantic_ai.Agent.run] — a coroutine which returns a [`RunResult`][pydantic_ai.result.RunResult] containing a completed response\\n2. [`agent.run_sync()`][pydantic_ai.Agent.run_sync] — a plain, synchronous function which returns a [`RunResult`][pydantic_ai.result.RunResult] containing a completed response (internally, this just calls `loop.run_until_complete(self.run())`)\\n3. [`agent.run_stream()`][pydantic_ai.Agent.run_stream] — a coroutine which returns a [`StreamedRunResult`][pydantic_ai.result.StreamedRunResult], which contains methods to stream a response as an async iterable\\n\\nHere\\'s a simple example demonstrating all three:\\n\\n```python {title=\"run_agent.py\"}\\nfrom pydantic_ai import Agent\\n\\nagent = Agent(\\'openai:gpt-4o\\')\\n\\nresult_sync = agent.run_sync(\\'What is the capital of Italy?\\')\\nprint(result_sync.data)\\n#> Rome\\n\\n\\nasync def main():\\n    result = await agent.run(\\'What is the capital of France?\\')\\n    print(result.data)\\n    #> Paris\\n\\n    async with agent.run_stream(\\'What is the capital of the UK?\\') as response:\\n        print(await response.get_data())\\n        #> London\\n```\\n_(This example is complete, it can be run \"as is\" — you\\'ll need to add `asyncio.run(main())` to run `main`)_\\n\\nYou can also pass messages from previous runs to continue a conversation or provide context, as described in [Messages and Chat History](message-history.md).\\n\\n\\n### Additional Configuration\\n\\n#### Usage Limits\\n\\nPydanticAI offers a [`UsageLimits`][pydantic_ai.usage.UsageLimits] structure to help you limit your\\nusage (tokens and/or requests) on model runs.\\n\\nYou can apply these settings by passing the `usage_limits` argument to the `run{_sync,_stream}` functions.\\n\\nConsider the following example, where we limit the number of response tokens:\\n\\n```py\\nfrom pydantic_ai import Agent\\nfrom pydantic_ai.exceptions import UsageLimitExceeded\\nfrom pydantic_ai.usage import UsageLimits\\n\\nagent = Agent(\\'claude-3-5-sonnet-latest\\')\\n\\nresult_sync = agent.run_sync(\\n    \\'What is the capital of Italy? Answer with just the city.\\',\\n    usage_limits=UsageLimits(response_tokens_limit=10),\\n)\\nprint(result_sync.data)\\n#> Rome\\nprint(result_sync.usage())\\n\"\"\"\\nUsage(requests=1, request_tokens=62, response_tokens=1, total_tokens=63, details=None)\\n\"\"\"\\n\\ntry:\\n    result_sync = agent.run_sync(\\n        \\'What is the capital of Italy? Answer with a paragraph.\\',\\n        usage_limits=UsageLimits(response_tokens_limit=10),\\n    )\\nexcept UsageLimitExceeded as e:\\n    print(e)\\n    #> Exceeded the response_tokens_limit of 10 (response_tokens=32)\\n```\\n\\nRestricting the number of requests can be useful in preventing infinite loops or excessive tool calling:\\n\\n```py\\nfrom typing_extensions import TypedDict\\n\\nfrom pydantic_ai import Agent, ModelRetry\\nfrom pydantic_ai.exceptions import UsageLimitExceeded\\nfrom pydantic_ai.usage import UsageLimits\\n\\n\\nclass NeverResultType(TypedDict):\\n    \"\"\"\\n    Never ever coerce data to this type.\\n    \"\"\"\\n\\n    never_use_this: str\\n\\n\\nagent = Agent(\\n    \\'claude-3-5-sonnet-latest\\',\\n    result_type=NeverResultType,\\n    system_prompt=\\'Any time you get a response, call the `infinite_retry_tool` to produce another response.\\',\\n)\\n\\n\\n@agent.tool_plain(retries=5)  # (1)!\\ndef infinite_retry_tool() -> int:\\n    raise ModelRetry(\\'Please try again.\\')\\n\\n\\ntry:\\n    result_sync = agent.run_sync(\\n        \\'Begin infinite retry loop!\\', usage_limits=UsageLimits(request_limit=3)  # (2)!\\n    )\\nexcept UsageLimitExceeded as e:\\n    print(e)\\n    #> The next request would exceed the request_limit of 3\\n```\\n\\n1. This tool has the ability to retry 5 times before erroring, simulating a tool that might get stuck in a loop.\\n2. This run will error after 3 requests, preventing the infinite tool calling.\\n\\n!!! note\\n    This is especially relevant if you\\'re registered a lot of tools, `request_limit` can be used to prevent the model from choosing to make too many of these calls.\\n\\n#### Model (Run) Settings\\n\\nPydanticAI offers a [`settings.ModelSettings`][pydantic_ai.settings.ModelSettings] structure to help you fine tune your requests.\\nThis structure allows you to configure common parameters that influence the model\\'s behavior, such as `temperature`, `max_tokens`,\\n`timeout`, and more.\\n\\nThere are two ways to apply these settings:\\n1. Passing to `run{_sync,_stream}` functions via the `model_settings` argument. This allows for fine-tuning on a per-request basis.\\n2. Setting during [`Agent`][pydantic_ai.agent.Agent] initialization via the `model_settings` argument. These settings will be applied by default to all subsequent run calls using said agent. However, `model_settings` provided during a specific run call will override the agent\\'s default settings.\\n\\nFor example, if you\\'d like to set the `temperature` setting to `0.0` to ensure less random behavior,\\nyou can do the following:\\n\\n```py\\nfrom pydantic_ai import Agent\\n\\nagent = Agent(\\'openai:gpt-4o\\')\\n\\nresult_sync = agent.run_sync(\\n    \\'What is the capital of Italy?\\', model_settings={\\'temperature\\': 0.0}\\n)\\nprint(result_sync.data)\\n#> Rome\\n```', 'metadata': {'file_path': 'pydantic-ai/docs/agents.md', 'section_title': 'Running Agents'}}, {'source_file': 'pydantic-ai/docs/agents.md', 'title': 'Runs vs. Conversations', 'content': '## Runs vs. Conversations\\n\\nAn agent **run** might represent an entire conversation — there\\'s no limit to how many messages can be exchanged in a single run. However, a **conversation** might also be composed of multiple runs, especially if you need to maintain state between separate interactions or API calls.\\n\\nHere\\'s an example of a conversation comprised of multiple runs:\\n\\n```python {title=\"conversation_example.py\" hl_lines=\"13\"}\\nfrom pydantic_ai import Agent\\n\\nagent = Agent(\\'openai:gpt-4o\\')', 'metadata': {'file_path': 'pydantic-ai/docs/agents.md', 'section_title': 'Runs vs. Conversations'}}, {'source_file': 'pydantic-ai/docs/agents.md', 'title': 'First run', 'content': \"# First run\\nresult1 = agent.run_sync('Who was Albert Einstein?')\\nprint(result1.data)\\n#> Albert Einstein was a German-born theoretical physicist.\", 'metadata': {'file_path': 'pydantic-ai/docs/agents.md', 'section_title': 'First run'}}, {'source_file': 'pydantic-ai/docs/agents.md', 'title': 'Second run, passing previous messages', 'content': '# Second run, passing previous messages\\nresult2 = agent.run_sync(\\n    \\'What was his most famous equation?\\',\\n    message_history=result1.new_messages(),  # (1)!\\n)\\nprint(result2.data)\\n#> Albert Einstein\\'s most famous equation is (E = mc^2).\\n```\\n\\n1. Continue the conversation; without `message_history` the model would not know who \"his\" was referring to.\\n\\n_(This example is complete, it can be run \"as is\")_', 'metadata': {'file_path': 'pydantic-ai/docs/agents.md', 'section_title': 'Second run, passing previous messages'}}, {'source_file': 'pydantic-ai/docs/agents.md', 'title': 'Type safe by design {#static-type-checking}', 'content': '## Type safe by design {#static-type-checking}\\n\\nPydanticAI is designed to work well with static type checkers, like mypy and pyright.\\n\\n!!! tip \"Typing is (somewhat) optional\"\\n    PydanticAI is designed to make type checking as useful as possible for you if you choose to use it, but you don\\'t have to use types everywhere all the time.\\n\\n    That said, because PydanticAI uses Pydantic, and Pydantic uses type hints as the definition for schema and validation, some types (specifically type hints on parameters to tools, and the `result_type` arguments to [`Agent`][pydantic_ai.Agent]) are used at runtime.\\n\\n    We (the library developers) have messed up if type hints are confusing you more than helping you, if you find this, please create an [issue](https://github.com/pydantic/pydantic-ai/issues) explaining what\\'s annoying you!\\n\\nIn particular, agents are generic in both the type of their dependencies and the type of results they return, so you can use the type hints to ensure you\\'re using the right types.\\n\\nConsider the following script with type mistakes:\\n\\n```python {title=\"type_mistakes.py\" hl_lines=\"18 28\"}\\nfrom dataclasses import dataclass\\n\\nfrom pydantic_ai import Agent, RunContext\\n\\n\\n@dataclass\\nclass User:\\n    name: str\\n\\n\\nagent = Agent(\\n    \\'test\\',\\n    deps_type=User,  # (1)!\\n    result_type=bool,\\n)\\n\\n\\n@agent.system_prompt\\ndef add_user_name(ctx: RunContext[str]) -> str:  # (2)!\\n    return f\"The user\\'s name is {ctx.deps}.\"\\n\\n\\ndef foobar(x: bytes) -> None:\\n    pass\\n\\n\\nresult = agent.run_sync(\\'Does their name start with \"A\"?\\', deps=User(\\'Anne\\'))\\nfoobar(result.data)  # (3)!\\n```\\n\\n1. The agent is defined as expecting an instance of `User` as `deps`.\\n2. But here `add_user_name` is defined as taking a `str` as the dependency, not a `User`.\\n3. Since the agent is defined as returning a `bool`, this will raise a type error since `foobar` expects `bytes`.\\n\\nRunning `mypy` on this will give the following output:\\n\\n```bash\\n➤ uv run mypy type_mistakes.py\\ntype_mistakes.py:18: error: Argument 1 to \"system_prompt\" of \"Agent\" has incompatible type \"Callable[[RunContext[str]], str]\"; expected \"Callable[[RunContext[User]], str]\"  [arg-type]\\ntype_mistakes.py:28: error: Argument 1 to \"foobar\" has incompatible type \"bool\"; expected \"bytes\"  [arg-type]\\nFound 2 errors in 1 file (checked 1 source file)\\n```\\n\\nRunning `pyright` would identify the same issues.', 'metadata': {'file_path': 'pydantic-ai/docs/agents.md', 'section_title': 'Type safe by design {#static-type-checking}'}}, {'source_file': 'pydantic-ai/docs/agents.md', 'title': 'System Prompts', 'content': '## System Prompts\\n\\nSystem prompts might seem simple at first glance since they\\'re just strings (or sequences of strings that are concatenated), but crafting the right system prompt is key to getting the model to behave as you want.\\n\\nGenerally, system prompts fall into two categories:\\n\\n1. **Static system prompts**: These are known when writing the code and can be defined via the `system_prompt` parameter of the [`Agent` constructor][pydantic_ai.Agent.__init__].\\n2. **Dynamic system prompts**: These depend in some way on context that isn\\'t known until runtime, and should be defined via functions decorated with [`@agent.system_prompt`][pydantic_ai.Agent.system_prompt].\\n\\nYou can add both to a single agent; they\\'re appended in the order they\\'re defined at runtime.\\n\\nHere\\'s an example using both types of system prompts:\\n\\n```python {title=\"system_prompts.py\"}\\nfrom datetime import date\\n\\nfrom pydantic_ai import Agent, RunContext\\n\\nagent = Agent(\\n    \\'openai:gpt-4o\\',\\n    deps_type=str,  # (1)!\\n    system_prompt=\"Use the customer\\'s name while replying to them.\",  # (2)!\\n)\\n\\n\\n@agent.system_prompt  # (3)!\\ndef add_the_users_name(ctx: RunContext[str]) -> str:\\n    return f\"The user\\'s name is {ctx.deps}.\"\\n\\n\\n@agent.system_prompt\\ndef add_the_date() -> str:  # (4)!\\n    return f\\'The date is {date.today()}.\\'\\n\\n\\nresult = agent.run_sync(\\'What is the date?\\', deps=\\'Frank\\')\\nprint(result.data)\\n#> Hello Frank, the date today is 2032-01-02.\\n```\\n\\n1. The agent expects a string dependency.\\n2. Static system prompt defined at agent creation time.\\n3. Dynamic system prompt defined via a decorator with [`RunContext`][pydantic_ai.tools.RunContext], this is called just after `run_sync`, not when the agent is created, so can benefit from runtime information like the dependencies used on that run.\\n4. Another dynamic system prompt, system prompts don\\'t have to have the `RunContext` parameter.\\n\\n_(This example is complete, it can be run \"as is\")_', 'metadata': {'file_path': 'pydantic-ai/docs/agents.md', 'section_title': 'System Prompts'}}, {'source_file': 'pydantic-ai/docs/agents.md', 'title': 'Reflection and self-correction', 'content': '## Reflection and self-correction\\n\\nValidation errors from both function tool parameter validation and [structured result validation](results.md#structured-result-validation) can be passed back to the model with a request to retry.\\n\\nYou can also raise [`ModelRetry`][pydantic_ai.exceptions.ModelRetry] from within a [tool](tools.md) or [result validator function](results.md#result-validators-functions) to tell the model it should retry generating a response.\\n\\n- The default retry count is **1** but can be altered for the [entire agent][pydantic_ai.Agent.__init__], a [specific tool][pydantic_ai.Agent.tool], or a [result validator][pydantic_ai.Agent.__init__].\\n- You can access the current retry count from within a tool or result validator via [`ctx.retry`][pydantic_ai.tools.RunContext].\\n\\nHere\\'s an example:\\n\\n```python {title=\"tool_retry.py\"}\\nfrom pydantic import BaseModel\\n\\nfrom pydantic_ai import Agent, RunContext, ModelRetry\\n\\nfrom fake_database import DatabaseConn\\n\\n\\nclass ChatResult(BaseModel):\\n    user_id: int\\n    message: str\\n\\n\\nagent = Agent(\\n    \\'openai:gpt-4o\\',\\n    deps_type=DatabaseConn,\\n    result_type=ChatResult,\\n)\\n\\n\\n@agent.tool(retries=2)\\ndef get_user_by_name(ctx: RunContext[DatabaseConn], name: str) -> int:\\n    \"\"\"Get a user\\'s ID from their full name.\"\"\"\\n    print(name)\\n    #> John\\n    #> John Doe\\n    user_id = ctx.deps.users.get(name=name)\\n    if user_id is None:\\n        raise ModelRetry(\\n            f\\'No user found with name {name!r}, remember to provide their full name\\'\\n        )\\n    return user_id\\n\\n\\nresult = agent.run_sync(\\n    \\'Send a message to John Doe asking for coffee next week\\', deps=DatabaseConn()\\n)\\nprint(result.data)\\n\"\"\"\\nuser_id=123 message=\\'Hello John, would you be free for coffee sometime next week? Let me know what works for you!\\'\\n\"\"\"\\n```', 'metadata': {'file_path': 'pydantic-ai/docs/agents.md', 'section_title': 'Reflection and self-correction'}}, {'source_file': 'pydantic-ai/docs/agents.md', 'title': 'Model errors', 'content': '## Model errors\\n\\nIf models behave unexpectedly (e.g., the retry limit is exceeded, or their API returns `503`), agent runs will raise [`UnexpectedModelBehavior`][pydantic_ai.exceptions.UnexpectedModelBehavior].\\n\\nIn these cases, [`capture_run_messages`][pydantic_ai.capture_run_messages] can be used to access the messages exchanged during the run to help diagnose the issue.\\n\\n```python\\nfrom pydantic_ai import Agent, ModelRetry, UnexpectedModelBehavior, capture_run_messages\\n\\nagent = Agent(\\'openai:gpt-4o\\')\\n\\n\\n@agent.tool_plain\\ndef calc_volume(size: int) -> int:  # (1)!\\n    if size == 42:\\n        return size**3\\n    else:\\n        raise ModelRetry(\\'Please try again.\\')\\n\\n\\nwith capture_run_messages() as messages:  # (2)!\\n    try:\\n        result = agent.run_sync(\\'Please get me the volume of a box with size 6.\\')\\n    except UnexpectedModelBehavior as e:\\n        print(\\'An error occurred:\\', e)\\n        #> An error occurred: Tool exceeded max retries count of 1\\n        print(\\'cause:\\', repr(e.__cause__))\\n        #> cause: ModelRetry(\\'Please try again.\\')\\n        print(\\'messages:\\', messages)\\n        \"\"\"\\n        messages:\\n        [\\n            ModelRequest(\\n                parts=[\\n                    UserPromptPart(\\n                        content=\\'Please get me the volume of a box with size 6.\\',\\n                        timestamp=datetime.datetime(...),\\n                        part_kind=\\'user-prompt\\',\\n                    )\\n                ],\\n                kind=\\'request\\',\\n            ),\\n            ModelResponse(\\n                parts=[\\n                    ToolCallPart(\\n                        tool_name=\\'calc_volume\\',\\n                        args=ArgsDict(args_dict={\\'size\\': 6}),\\n                        tool_call_id=None,\\n                        part_kind=\\'tool-call\\',\\n                    )\\n                ],\\n                timestamp=datetime.datetime(...),\\n                kind=\\'response\\',\\n            ),\\n            ModelRequest(\\n                parts=[\\n                    RetryPromptPart(\\n                        content=\\'Please try again.\\',\\n                        tool_name=\\'calc_volume\\',\\n                        tool_call_id=None,\\n                        timestamp=datetime.datetime(...),\\n                        part_kind=\\'retry-prompt\\',\\n                    )\\n                ],\\n                kind=\\'request\\',\\n            ),\\n            ModelResponse(\\n                parts=[\\n                    ToolCallPart(\\n                        tool_name=\\'calc_volume\\',\\n                        args=ArgsDict(args_dict={\\'size\\': 6}),\\n                        tool_call_id=None,\\n                        part_kind=\\'tool-call\\',\\n                    )\\n                ],\\n                timestamp=datetime.datetime(...),\\n                kind=\\'response\\',\\n            ),\\n        ]\\n        \"\"\"\\n    else:\\n        print(result.data)\\n```\\n\\n1. Define a tool that will raise `ModelRetry` repeatedly in this case.\\n2. [`capture_run_messages`][pydantic_ai.capture_run_messages] is used to capture the messages exchanged during the run.\\n\\n_(This example is complete, it can be run \"as is\")_\\n\\n!!! note\\n    If you call [`run`][pydantic_ai.Agent.run], [`run_sync`][pydantic_ai.Agent.run_sync], or [`run_stream`][pydantic_ai.Agent.run_stream] more than once within a single `capture_run_messages` context, `messages` will represent the messages exchanged during the first call only.', 'metadata': {'file_path': 'pydantic-ai/docs/agents.md', 'section_title': 'Model errors'}}, {'source_file': 'pydantic-ai/docs/logfire.md', 'title': 'Debugging and Monitoring', 'content': '# Debugging and Monitoring\\n\\nApplications that use LLMs have some challenges that are well known and understood: LLMs are **slow**, **unreliable** and **expensive**.\\n\\nThese applications also have some challenges that most developers have encountered much less often: LLMs are **fickle** and **non-deterministic**. Subtle changes in a prompt can completely change a model\\'s performance, and there\\'s no `EXPLAIN` query you can run to understand why.\\n\\n!!! danger \"Warning\"\\n    From a software engineers point of view, you can think of LLMs as the worst database you\\'ve ever heard of, but worse.\\n\\n    If LLMs weren\\'t so bloody useful, we\\'d never touch them.\\n\\nTo build successful applications with LLMs, we need new tools to understand both model performance, and the behavior of applications that rely on them.\\n\\nLLM Observability tools that just let you understand how your model is performing are useless: making API calls to an LLM is easy, it\\'s building that into an application that\\'s hard.', 'metadata': {'file_path': 'pydantic-ai/docs/logfire.md', 'section_title': 'Debugging and Monitoring'}}, {'source_file': 'pydantic-ai/docs/logfire.md', 'title': 'Pydantic Logfire', 'content': '## Pydantic Logfire\\n\\n[Pydantic Logfire](https://pydantic.dev/logfire) is an observability platform developed by the team who created and maintain Pydantic and PydanticAI. Logfire aims to let you understand your entire application: Gen AI, classic predictive AI, HTTP traffic, database queries and everything else a modern application needs.\\n\\n!!! tip \"Pydantic Logfire is a commercial product\"\\n    Logfire is a commercially supported, hosted platform with an extremely generous and perpetual [free tier](https://pydantic.dev/pricing/).\\n    You can sign up and start using Logfire in a couple of minutes.\\n\\nPydanticAI has built-in (but optional) support for Logfire via the [`logfire-api`](https://github.com/pydantic/logfire/tree/main/logfire-api) no-op package.\\n\\nThat means if the `logfire` package is installed and configured, detailed information about agent runs is sent to Logfire. But if the `logfire` package is not installed, there\\'s virtually no overhead and nothing is sent.\\n\\nHere\\'s an example showing details of running the [Weather Agent](examples/weather-agent.md) in Logfire:\\n\\n![Weather Agent Logfire](img/logfire-weather-agent.png)', 'metadata': {'file_path': 'pydantic-ai/docs/logfire.md', 'section_title': 'Pydantic Logfire'}}, {'source_file': 'pydantic-ai/docs/logfire.md', 'title': 'Using Logfire', 'content': '## Using Logfire\\n\\nTo use logfire, you\\'ll need a logfire [account](https://logfire.pydantic.dev), and logfire installed:\\n\\n```bash\\npip/uv-add \\'pydantic-ai[logfire]\\'\\n```\\n\\nThen authenticate your local environment with logfire:\\n\\n```bash\\npy-cli logfire auth\\n```\\n\\nAnd configure a project to send data to:\\n\\n```bash\\npy-cli logfire projects new\\n```\\n\\n(Or use an existing project with `logfire projects use`)\\n\\nThe last step is to add logfire to your code:\\n\\n```python {title=\"adding_logfire.py\"}\\nimport logfire\\n\\nlogfire.configure()\\n```\\n\\nThe [logfire documentation](https://logfire.pydantic.dev/docs/) has more details on how to use logfire, including how to instrument other libraries like Pydantic, HTTPX and FastAPI.\\n\\nSince Logfire is build on [OpenTelemetry](https://opentelemetry.io/), you can use the Logfire Python SDK to send data to any OpenTelemetry collector.\\n\\nOnce you have logfire set up, there are two primary ways it can help you understand your application:\\n\\n* **Debugging** — Using the live view to see what\\'s happening in your application in real-time.\\n* **Monitoring** — Using SQL and dashboards to observe the behavior of your application, Logfire is effectively a SQL database that stores information about how your application is running.\\n\\n### Debugging\\n\\nTo demonstrate how Logfire can let you visualise the flow of a PydanticAI run, here\\'s the view you get from Logfire while running the [chat app examples](examples/chat-app.md):\\n\\n{{ video(\\'a764aff5840534dc77eba7d028707bfa\\', 25) }}\\n\\n### Monitoring Performance\\n\\nWe can also query data with SQL in Logfire to monitor the performance of an application. Here\\'s a real world example of using Logfire to monitor PydanticAI runs inside Logfire itself:\\n\\n![Logfire monitoring PydanticAI](img/logfire-monitoring-pydanticai.png)', 'metadata': {'file_path': 'pydantic-ai/docs/logfire.md', 'section_title': 'Using Logfire'}}, {'source_file': 'pydantic-ai/docs/graph.md', 'title': 'Graphs', 'content': '# Graphs\\n\\n!!! danger \"Don\\'t use a nail gun unless you need a nail gun\"\\n    If PydanticAI [agents](agents.md) are a hammer, and [multi-agent workflows](multi-agent-applications.md) are a sledgehammer, then graphs are a nail gun:\\n\\n    * sure, nail guns look cooler than hammers\\n    * but nail guns take a lot more setup than hammers\\n    * and nail guns don\\'t make you a better builder, they make you a builder with a nail gun\\n    * Lastly, (and at the risk of torturing this metaphor), if you\\'re a fan of medieval tools like mallets and untyped Python, you probably won\\'t like nail guns or our approach to graphs. (But then again, if you\\'re not a fan of type hints in Python, you\\'ve probably already bounced off PydanticAI to use one of the toy agent frameworks — good luck, and feel free to borrow my sledgehammer when you realize you need it)\\n\\n    In short, graphs are a powerful tool, but they\\'re not the right tool for every job. Please consider other [multi-agent approaches](multi-agent-applications.md) before proceeding.\\n\\n    If you\\'re not confident a graph-based approach is a good idea, it might be unnecessary.\\n\\nGraphs and finite state machines (FSMs) are a powerful abstraction to model, execute, control and visualize complex workflows.\\n\\nAlongside PydanticAI, we\\'ve developed `pydantic-graph` — an async graph and state machine library for Python where nodes and edges are defined using type hints.\\n\\nWhile this library is developed as part of PydanticAI; it has no dependency on `pydantic-ai` and can be considered as a pure  graph-based state machine library. You may find it useful whether or not you\\'re using PydanticAI or even building with GenAI.\\n\\n`pydantic-graph` is designed for advanced users and makes heavy use of Python generics and types hints. It is not designed to be as beginner-friendly as PydanticAI.\\n\\n!!! note \"Very Early beta\"\\n    Graph support was [introduced](https://github.com/pydantic/pydantic-ai/pull/528) in v0.0.19 and is in very earlier beta. The API is subject to change. The documentation is incomplete. The implementation is incomplete.', 'metadata': {'file_path': 'pydantic-ai/docs/graph.md', 'section_title': 'Graphs'}}, {'source_file': 'pydantic-ai/docs/graph.md', 'title': 'Installation', 'content': '## Installation\\n\\n`pydantic-graph` is a required dependency of `pydantic-ai`, and an optional dependency of `pydantic-ai-slim`, see [installation instructions](install.md#slim-install) for more information. You can also install it directly:\\n\\n```bash\\npip/uv-add pydantic-graph\\n```', 'metadata': {'file_path': 'pydantic-ai/docs/graph.md', 'section_title': 'Installation'}}, {'source_file': 'pydantic-ai/docs/graph.md', 'title': 'Graph Types', 'content': '## Graph Types\\n\\n`pydantic-graph` made up of a few key components:\\n\\n### GraphRunContext\\n\\n[`GraphRunContext`][pydantic_graph.nodes.GraphRunContext] — The context for the graph run, similar to PydanticAI\\'s [`RunContext`][pydantic_ai.tools.RunContext]. This holds the state of the graph and dependencies and is passed to nodes when they\\'re run.\\n\\n`GraphRunContext` is generic in the state type of the graph it\\'s used in, [`StateT`][pydantic_graph.state.StateT].\\n\\n### End\\n\\n[`End`][pydantic_graph.nodes.End] — return value to indicate the graph run should end.\\n\\n`End` is generic in the graph return type of the graph it\\'s used in, [`RunEndT`][pydantic_graph.nodes.RunEndT].\\n\\n### Nodes\\n\\nSubclasses of [`BaseNode`][pydantic_graph.nodes.BaseNode] define nodes for execution in the graph.\\n\\nNodes, which are generally [`dataclass`es][dataclasses.dataclass], generally consist of:\\n\\n* fields containing any parameters required/optional when calling the node\\n* the business logic to execute the node, in the [`run`][pydantic_graph.nodes.BaseNode.run] method\\n* return annotations of the [`run`][pydantic_graph.nodes.BaseNode.run] method, which are read by `pydantic-graph` to determine the outgoing edges of the node\\n\\nNodes are generic in:\\n\\n* **state**, which must have the same type as the state of graphs they\\'re included in, [`StateT`][pydantic_graph.state.StateT] has a default of `None`, so if you\\'re not using state you can omit this generic parameter, see [stateful graphs](#stateful-graphs) for more information\\n* **deps**, which must have the same type as the deps of the graph they\\'re included in, [`DepsT`][pydantic_graph.nodes.DepsT] has a default of `None`, so if you\\'re not using deps you can omit this generic parameter, see [dependency injection](#dependency-injection) for more information\\n* **graph return type** — this only applies if the node returns [`End`][pydantic_graph.nodes.End]. [`RunEndT`][pydantic_graph.nodes.RunEndT] has a default of [Never][typing.Never] so this generic parameter can be omitted if the node doesn\\'t return `End`, but must be included if it does.\\n\\nHere\\'s an example of a start or intermediate node in a graph — it can\\'t end the run as it doesn\\'t return [`End`][pydantic_graph.nodes.End]:\\n\\n```py {title=\"intermediate_node.py\" noqa=\"F821\" test=\"skip\"}\\nfrom dataclasses import dataclass\\n\\nfrom pydantic_graph import BaseNode, GraphRunContext\\n\\n\\n@dataclass\\nclass MyNode(BaseNode[MyState]):  # (1)!\\n    foo: int  # (2)!\\n\\n    async def run(\\n        self,\\n        ctx: GraphRunContext[MyState],  # (3)!\\n    ) -> AnotherNode:  # (4)!\\n        ...\\n        return AnotherNode()\\n```\\n\\n1. State in this example is `MyState` (not shown), hence `BaseNode` is parameterized with `MyState`. This node can\\'t end the run, so the `RunEndT` generic parameter is omitted and defaults to `Never`.\\n2. `MyNode` is a dataclass and has a single field `foo`, an `int`.\\n3. The `run` method takes a `GraphRunContext` parameter, again parameterized with state `MyState`.\\n4. The return type of the `run` method is `AnotherNode` (not shown), this is used to determine the outgoing edges of the node.\\n\\nWe could extend `MyNode` to optionally end the run if `foo` is divisible by 5:\\n\\n```py {title=\"intermediate_or_end_node.py\" hl_lines=\"7 13 15\" noqa=\"F821\" test=\"skip\"}\\nfrom dataclasses import dataclass\\n\\nfrom pydantic_graph import BaseNode, End, GraphRunContext\\n\\n\\n@dataclass\\nclass MyNode(BaseNode[MyState, None, int]):  # (1)!\\n    foo: int\\n\\n    async def run(\\n        self,\\n        ctx: GraphRunContext[MyState],\\n    ) -> AnotherNode | End[int]:  # (2)!\\n        if self.foo % 5 == 0:\\n            return End(self.foo)\\n        else:\\n            return AnotherNode()\\n```\\n\\n1. We parameterize the node with the return type (`int` in this case) as well as state. Because generic parameters are positional-only, we have to include `None` as the second parameter representing deps.\\n2. The return type of the `run` method is now a union of `AnotherNode` and `End[int]`, this allows the node to end the run if `foo` is divisible by 5.\\n\\n### Graph\\n\\n[`Graph`][pydantic_graph.graph.Graph] — this is the execution graph itself, made up of a set of [node classes](#nodes) (i.e., `BaseNode` subclasses).\\n\\n`Graph` is generic in:\\n\\n* **state** the state type of the graph, [`StateT`][pydantic_graph.state.StateT]\\n* **deps** the deps type of the graph, [`DepsT`][pydantic_graph.nodes.DepsT]\\n* **graph return type** the return type of the graph run, [`RunEndT`][pydantic_graph.nodes.RunEndT]\\n\\nHere\\'s an example of a simple graph:\\n\\n```py {title=\"graph_example.py\" py=\"3.10\"}\\nfrom __future__ import annotations\\n\\nfrom dataclasses import dataclass\\n\\nfrom pydantic_graph import BaseNode, End, Graph, GraphRunContext\\n\\n\\n@dataclass\\nclass DivisibleBy5(BaseNode[None, None, int]):  # (1)!\\n    foo: int\\n\\n    async def run(\\n        self,\\n        ctx: GraphRunContext,\\n    ) -> Increment | End[int]:\\n        if self.foo % 5 == 0:\\n            return End(self.foo)\\n        else:\\n            return Increment(self.foo)\\n\\n\\n@dataclass\\nclass Increment(BaseNode):  # (2)!\\n    foo: int\\n\\n    async def run(self, ctx: GraphRunContext) -> DivisibleBy5:\\n        return DivisibleBy5(self.foo + 1)\\n\\n\\nfives_graph = Graph(nodes=[DivisibleBy5, Increment])  # (3)!\\nresult, history = fives_graph.run_sync(DivisibleBy5(4))  # (4)!\\nprint(result)\\n#> 5', 'metadata': {'file_path': 'pydantic-ai/docs/graph.md', 'section_title': 'Graph Types'}}, {'source_file': 'pydantic-ai/docs/graph.md', 'title': \"the full history is quite verbose (see below), so we'll just print the summary\", 'content': '# the full history is quite verbose (see below), so we\\'ll just print the summary\\nprint([item.data_snapshot() for item in history])\\n#> [DivisibleBy5(foo=4), Increment(foo=4), DivisibleBy5(foo=5), End(data=5)]\\n```\\n\\n1. The `DivisibleBy5` node is parameterized with `None` for the state param and `None` for the deps param as this graph doesn\\'t use state or deps, and `int` as it can end the run.\\n2. The `Increment` node doesn\\'t return `End`, so the `RunEndT` generic parameter is omitted, state can also be omitted as the graph doesn\\'t use state.\\n3. The graph is created with a sequence of nodes.\\n4. The graph is run synchronously with [`run_sync`][pydantic_graph.graph.Graph.run_sync] the initial state `None` and the start node `DivisibleBy5(4)` are passed as arguments.\\n\\n_(This example is complete, it can be run \"as is\" with Python 3.10+)_\\n\\nA [mermaid diagram](#mermaid-diagrams) for this graph can be generated with the following code:\\n\\n```py {title=\"graph_example_diagram.py\" py=\"3.10\"}\\nfrom graph_example import DivisibleBy5, fives_graph\\n\\nfives_graph.mermaid_code(start_node=DivisibleBy5)\\n```\\n\\n```mermaid\\n---\\ntitle: fives_graph\\n---\\nstateDiagram-v2\\n  [*] --> DivisibleBy5\\n  DivisibleBy5 --> Increment\\n  DivisibleBy5 --> [*]\\n  Increment --> DivisibleBy5\\n```', 'metadata': {'file_path': 'pydantic-ai/docs/graph.md', 'section_title': \"the full history is quite verbose (see below), so we'll just print the summary\"}}, {'source_file': 'pydantic-ai/docs/graph.md', 'title': 'Stateful Graphs', 'content': '## Stateful Graphs\\n\\nThe \"state\" concept in `pydantic-graph` provides an optional way to access and mutate an object (often a `dataclass` or Pydantic model) as nodes run in a graph. If you think of Graphs as a production line, then you state is the engine being passed along the line and built up by each node as the graph is run.\\n\\nIn the future, we intend to extend `pydantic-graph` to provide state persistence with the state recorded after each node is run, see [#695](https://github.com/pydantic/pydantic-ai/issues/695).\\n\\nHere\\'s an example of a graph which represents a vending machine where the user may insert coins and select a product to purchase.\\n\\n```python {title=\"vending_machine.py\" py=\"3.10\"}\\nfrom __future__ import annotations\\n\\nfrom dataclasses import dataclass\\n\\nfrom rich.prompt import Prompt\\n\\nfrom pydantic_graph import BaseNode, End, Graph, GraphRunContext\\n\\n\\n@dataclass\\nclass MachineState:  # (1)!\\n    user_balance: float = 0.0\\n    product: str | None = None\\n\\n\\n@dataclass\\nclass InsertCoin(BaseNode[MachineState]):  # (3)!\\n    async def run(self, ctx: GraphRunContext[MachineState]) -> CoinsInserted:  # (16)!\\n        return CoinsInserted(float(Prompt.ask(\\'Insert coins\\')))  # (4)!\\n\\n\\n@dataclass\\nclass CoinsInserted(BaseNode[MachineState]):\\n    amount: float  # (5)!\\n\\n    async def run(\\n        self, ctx: GraphRunContext[MachineState]\\n    ) -> SelectProduct | Purchase:  # (17)!\\n        ctx.state.user_balance += self.amount  # (6)!\\n        if ctx.state.product is not None:  # (7)!\\n            return Purchase(ctx.state.product)\\n        else:\\n            return SelectProduct()\\n\\n\\n@dataclass\\nclass SelectProduct(BaseNode[MachineState]):\\n    async def run(self, ctx: GraphRunContext[MachineState]) -> Purchase:\\n        return Purchase(Prompt.ask(\\'Select product\\'))\\n\\n\\nPRODUCT_PRICES = {  # (2)!\\n    \\'water\\': 1.25,\\n    \\'soda\\': 1.50,\\n    \\'crisps\\': 1.75,\\n    \\'chocolate\\': 2.00,\\n}\\n\\n\\n@dataclass\\nclass Purchase(BaseNode[MachineState, None, None]):  # (18)!\\n    product: str\\n\\n    async def run(\\n        self, ctx: GraphRunContext[MachineState]\\n    ) -> End | InsertCoin | SelectProduct:\\n        if price := PRODUCT_PRICES.get(self.product):  # (8)!\\n            ctx.state.product = self.product  # (9)!\\n            if ctx.state.user_balance >= price:  # (10)!\\n                ctx.state.user_balance -= price\\n                return End(None)\\n            else:\\n                diff = price - ctx.state.user_balance\\n                print(f\\'Not enough money for {self.product}, need {diff:0.2f} more\\')\\n                #> Not enough money for crisps, need 0.75 more\\n                return InsertCoin()  # (11)!\\n        else:\\n            print(f\\'No such product: {self.product}, try again\\')\\n            return SelectProduct()  # (12)!\\n\\n\\nvending_machine_graph = Graph(  # (13)!\\n    nodes=[InsertCoin, CoinsInserted, SelectProduct, Purchase]\\n)\\n\\n\\nasync def main():\\n    state = MachineState()  # (14)!\\n    await vending_machine_graph.run(InsertCoin(), state=state)  # (15)!\\n    print(f\\'purchase successful item={state.product} change={state.user_balance:0.2f}\\')\\n    #> purchase successful item=crisps change=0.25\\n```\\n\\n1. The state of the vending machine is defined as a dataclass with the user\\'s balance and the product they\\'ve selected, if any.\\n2. A dictionary of products mapped to prices.\\n3. The `InsertCoin` node, [`BaseNode`][pydantic_graph.nodes.BaseNode] is parameterized with `MachineState` as that\\'s the state used in this graph.\\n4. The `InsertCoin` node prompts the user to insert coins. We keep things simple by just entering a monetary amount as a float. Before you start thinking this is a toy too since it\\'s using [rich\\'s `Prompt.ask`][rich.prompt.PromptBase.ask] within nodes, see [below](#custom-control-flow) for how control flow can be managed when nodes require external input.\\n5. The `CoinsInserted` node; again this is a [`dataclass`][dataclasses.dataclass], in this case with one field `amount`, thus nodes calling `CoinsInserted` must provide an amount.\\n6. Update the user\\'s balance with the amount inserted.\\n7. If the user has already selected a product, go to `Purchase`, otherwise go to `SelectProduct`.\\n8. In the `Purchase` node, look up the price of the product if the user entered a valid product.\\n9. If the user did enter a valid product, set the product in the state so we don\\'t revisit `SelectProduct`.\\n10. If the balance is enough to purchase the product, adjust the balance to reflect the purchase and return [`End`][pydantic_graph.nodes.End] to end the graph. We\\'re not using the run return type, so we call `End` with `None`.\\n11. If the balance is insufficient, to go `InsertCoin` to prompt the user to insert more coins.\\n12. If the product is invalid, go to `SelectProduct` to prompt the user to select a product again.\\n13. The graph is created by passing a list of nodes to [`Graph`][pydantic_graph.graph.Graph]. Order of nodes is not important, but will alter how [diagrams](#mermaid-diagrams) are displayed.\\n14. Initialize the state. This will be passed to the graph run and mutated as the graph runs.\\n15. Run the graph with the initial state. Since the graph can be run from any node, we must pass the start node — in this case, `InsertCoin`. [`Graph.run`][pydantic_graph.graph.Graph.run] returns a tuple of the return value (`None`) in this case, and the [history][pydantic_graph.state.HistoryStep] of the graph run.\\n16. The return type of the node\\'s [`run`][pydantic_graph.nodes.BaseNode.run] method is important as it is used to determine the outgoing edges of the node. This information in turn is used to render [mermaid diagrams](#mermaid-diagrams) and is enforced at runtime to detect misbehavior as soon as possible.\\n17. The return type of `CoinsInserted`\\'s [`run`][pydantic_graph.nodes.BaseNode.run] method is a union, meaning multiple outgoing edges are possible.\\n18. Unlike other nodes, `Purchase` can end the run, so the [`RunEndT`][pydantic_graph.nodes.RunEndT] generic parameter must be set. In this case it\\'s `None` since the graph run return type is `None`.\\n\\n_(This example is complete, it can be run \"as is\" with Python 3.10+ — you\\'ll need to add `asyncio.run(main())` to run `main`)_\\n\\nA [mermaid diagram](#mermaid-diagrams) for this graph can be generated with the following code:\\n\\n```py {title=\"vending_machine_diagram.py\" py=\"3.10\"}\\nfrom vending_machine import InsertCoin, vending_machine_graph\\n\\nvending_machine_graph.mermaid_code(start_node=InsertCoin)\\n```\\n\\nThe diagram generated by the above code is:\\n\\n```mermaid\\n---\\ntitle: vending_machine_graph\\n---\\nstateDiagram-v2\\n  [*] --> InsertCoin\\n  InsertCoin --> CoinsInserted\\n  CoinsInserted --> SelectProduct\\n  CoinsInserted --> Purchase\\n  SelectProduct --> Purchase\\n  Purchase --> InsertCoin\\n  Purchase --> SelectProduct\\n  Purchase --> [*]\\n```\\n\\nSee [below](#mermaid-diagrams) for more information on generating diagrams.', 'metadata': {'file_path': 'pydantic-ai/docs/graph.md', 'section_title': 'Stateful Graphs'}}, {'source_file': 'pydantic-ai/docs/graph.md', 'title': 'GenAI Example', 'content': '## GenAI Example\\n\\nSo far we haven\\'t shown an example of a Graph that actually uses PydanticAI or GenAI at all.\\n\\nIn this example, one agent generates a welcome email to a user and the other agent provides feedback on the email.\\n\\nThis graph has a very simple structure:\\n\\n```mermaid\\n---\\ntitle: feedback_graph\\n---\\nstateDiagram-v2\\n  [*] --> WriteEmail\\n  WriteEmail --> Feedback\\n  Feedback --> WriteEmail\\n  Feedback --> [*]\\n```\\n\\n\\n```python {title=\"genai_email_feedback.py\" py=\"3.10\"}\\nfrom __future__ import annotations as _annotations\\n\\nfrom dataclasses import dataclass, field\\n\\nfrom pydantic import BaseModel, EmailStr\\n\\nfrom pydantic_ai import Agent\\nfrom pydantic_ai.format_as_xml import format_as_xml\\nfrom pydantic_ai.messages import ModelMessage\\nfrom pydantic_graph import BaseNode, End, Graph, GraphRunContext\\n\\n\\n@dataclass\\nclass User:\\n    name: str\\n    email: EmailStr\\n    interests: list[str]\\n\\n\\n@dataclass\\nclass Email:\\n    subject: str\\n    body: str\\n\\n\\n@dataclass\\nclass State:\\n    user: User\\n    write_agent_messages: list[ModelMessage] = field(default_factory=list)\\n\\n\\nemail_writer_agent = Agent(\\n    \\'google-vertex:gemini-1.5-pro\\',\\n    result_type=Email,\\n    system_prompt=\\'Write a welcome email to our tech blog.\\',\\n)\\n\\n\\n@dataclass\\nclass WriteEmail(BaseNode[State]):\\n    email_feedback: str | None = None\\n\\n    async def run(self, ctx: GraphRunContext[State]) -> Feedback:\\n        if self.email_feedback:\\n            prompt = (\\n                f\\'Rewrite the email for the user:\\\\n\\'\\n                f\\'{format_as_xml(ctx.state.user)}\\\\n\\'\\n                f\\'Feedback: {self.email_feedback}\\'\\n            )\\n        else:\\n            prompt = (\\n                f\\'Write a welcome email for the user:\\\\n\\'\\n                f\\'{format_as_xml(ctx.state.user)}\\'\\n            )\\n\\n        result = await email_writer_agent.run(\\n            prompt,\\n            message_history=ctx.state.write_agent_messages,\\n        )\\n        ctx.state.write_agent_messages += result.all_messages()\\n        return Feedback(result.data)\\n\\n\\nclass EmailRequiresWrite(BaseModel):\\n    feedback: str\\n\\n\\nclass EmailOk(BaseModel):\\n    pass\\n\\n\\nfeedback_agent = Agent[None, EmailRequiresWrite | EmailOk](\\n    \\'openai:gpt-4o\\',\\n    result_type=EmailRequiresWrite | EmailOk,  # type: ignore\\n    system_prompt=(\\n        \\'Review the email and provide feedback, email must reference the users specific interests.\\'\\n    ),\\n)\\n\\n\\n@dataclass\\nclass Feedback(BaseNode[State, None, Email]):\\n    email: Email\\n\\n    async def run(\\n        self,\\n        ctx: GraphRunContext[State],\\n    ) -> WriteEmail | End[Email]:\\n        prompt = format_as_xml({\\'user\\': ctx.state.user, \\'email\\': self.email})\\n        result = await feedback_agent.run(prompt)\\n        if isinstance(result.data, EmailRequiresWrite):\\n            return WriteEmail(email_feedback=result.data.feedback)\\n        else:\\n            return End(self.email)\\n\\n\\nasync def main():\\n    user = User(\\n        name=\\'John Doe\\',\\n        email=\\'john.joe@exmaple.com\\',\\n        interests=[\\'Haskel\\', \\'Lisp\\', \\'Fortran\\'],\\n    )\\n    state = State(user)\\n    feedback_graph = Graph(nodes=(WriteEmail, Feedback))\\n    email, _ = await feedback_graph.run(WriteEmail(), state=state)\\n    print(email)\\n    \"\"\"\\n    Email(\\n        subject=\\'Welcome to our tech blog!\\',\\n        body=\\'Hello John, Welcome to our tech blog! ...\\',\\n    )\\n    \"\"\"\\n```\\n\\n_(This example is complete, it can be run \"as is\" with Python 3.10+ — you\\'ll need to add `asyncio.run(main())` to run `main`)_', 'metadata': {'file_path': 'pydantic-ai/docs/graph.md', 'section_title': 'GenAI Example'}}, {'source_file': 'pydantic-ai/docs/graph.md', 'title': 'Custom Control Flow', 'content': '## Custom Control Flow\\n\\nIn many real-world applications, Graphs cannot run uninterrupted from start to finish — they might require external input, or run over an extended period of time such that a single process cannot execute the entire graph run from start to finish without interruption.\\n\\nIn these scenarios the [`next`][pydantic_graph.graph.Graph.next] method can be used to run the graph one node at a time.\\n\\nIn this example, an AI asks the user a question, the user provides an answer, the AI evaluates the answer and ends if the user got it right or asks another question if they got it wrong.\\n\\n??? example \"`ai_q_and_a_graph.py` — `question_graph` definition\"\\n    ```python {title=\"ai_q_and_a_graph.py\" noqa=\"I001\" py=\"3.10\"}\\n    from __future__ import annotations as _annotations\\n\\n    from dataclasses import dataclass, field\\n\\n    from pydantic_graph import BaseNode, End, Graph, GraphRunContext\\n\\n    from pydantic_ai import Agent\\n    from pydantic_ai.format_as_xml import format_as_xml\\n    from pydantic_ai.messages import ModelMessage\\n\\n    ask_agent = Agent(\\'openai:gpt-4o\\', result_type=str)\\n\\n\\n    @dataclass\\n    class QuestionState:\\n        question: str | None = None\\n        ask_agent_messages: list[ModelMessage] = field(default_factory=list)\\n        evaluate_agent_messages: list[ModelMessage] = field(default_factory=list)\\n\\n\\n    @dataclass\\n    class Ask(BaseNode[QuestionState]):\\n        async def run(self, ctx: GraphRunContext[QuestionState]) -> Answer:\\n            result = await ask_agent.run(\\n                \\'Ask a simple question with a single correct answer.\\',\\n                message_history=ctx.state.ask_agent_messages,\\n            )\\n            ctx.state.ask_agent_messages += result.all_messages()\\n            ctx.state.question = result.data\\n            return Answer(result.data)\\n\\n\\n    @dataclass\\n    class Answer(BaseNode[QuestionState]):\\n        question: str\\n        answer: str | None = None\\n\\n        async def run(self, ctx: GraphRunContext[QuestionState]) -> Evaluate:\\n            assert self.answer is not None\\n            return Evaluate(self.answer)\\n\\n\\n    @dataclass\\n    class EvaluationResult:\\n        correct: bool\\n        comment: str\\n\\n\\n    evaluate_agent = Agent(\\n        \\'openai:gpt-4o\\',\\n        result_type=EvaluationResult,\\n        system_prompt=\\'Given a question and answer, evaluate if the answer is correct.\\',\\n    )\\n\\n\\n    @dataclass\\n    class Evaluate(BaseNode[QuestionState]):\\n        answer: str\\n\\n        async def run(\\n            self,\\n            ctx: GraphRunContext[QuestionState],\\n        ) -> End[str] | Reprimand:\\n            assert ctx.state.question is not None\\n            result = await evaluate_agent.run(\\n                format_as_xml({\\'question\\': ctx.state.question, \\'answer\\': self.answer}),\\n                message_history=ctx.state.evaluate_agent_messages,\\n            )\\n            ctx.state.evaluate_agent_messages += result.all_messages()\\n            if result.data.correct:\\n                return End(result.data.comment)\\n            else:\\n                return Reprimand(result.data.comment)\\n\\n\\n    @dataclass\\n    class Reprimand(BaseNode[QuestionState]):\\n        comment: str\\n\\n        async def run(self, ctx: GraphRunContext[QuestionState]) -> Ask:\\n            print(f\\'Comment: {self.comment}\\')\\n            ctx.state.question = None\\n            return Ask()\\n\\n\\n    question_graph = Graph(nodes=(Ask, Answer, Evaluate, Reprimand))\\n    ```\\n\\n    _(This example is complete, it can be run \"as is\" with Python 3.10+)_\\n\\n\\n```python {title=\"ai_q_and_a_run.py\" noqa=\"I001\" py=\"3.10\"}\\nfrom rich.prompt import Prompt\\n\\nfrom pydantic_graph import End, HistoryStep\\n\\nfrom ai_q_and_a_graph import Ask, question_graph, QuestionState, Answer\\n\\n\\nasync def main():\\n    state = QuestionState()  # (1)!\\n    node = Ask()  # (2)!\\n    history: list[HistoryStep[QuestionState]] = []  # (3)!\\n    while True:\\n        node = await question_graph.next(node, history, state=state)  # (4)!\\n        if isinstance(node, Answer):\\n            node.answer = Prompt.ask(node.question)  # (5)!\\n        elif isinstance(node, End):  # (6)!\\n            print(f\\'Correct answer! {node.data}\\')\\n            #> Correct answer! Well done, 1 + 1 = 2\\n            print([e.data_snapshot() for e in history])\\n            \"\"\"\\n            [\\n                Ask(),\\n                Answer(question=\\'What is the capital of France?\\', answer=\\'Vichy\\'),\\n                Evaluate(answer=\\'Vichy\\'),\\n                Reprimand(comment=\\'Vichy is no longer the capital of France.\\'),\\n                Ask(),\\n                Answer(question=\\'what is 1 + 1?\\', answer=\\'2\\'),\\n                Evaluate(answer=\\'2\\'),\\n            ]\\n            \"\"\"\\n            return\\n        # otherwise just continue\\n```\\n\\n1. Create the state object which will be mutated by [`next`][pydantic_graph.graph.Graph.next].\\n2. The start node is `Ask` but will be updated by [`next`][pydantic_graph.graph.Graph.next] as the graph runs.\\n3. The history of the graph run is stored in a list of [`HistoryStep`][pydantic_graph.state.HistoryStep] objects. Again [`next`][pydantic_graph.graph.Graph.next] will update this list in place.\\n4. [Run][pydantic_graph.graph.Graph.next] the graph one node at a time, updating the state, current node and history as the graph runs.\\n5. If the current node is an `Answer` node, prompt the user for an answer.\\n6. Since we\\'re using [`next`][pydantic_graph.graph.Graph.next] we have to manually check for an [`End`][pydantic_graph.nodes.End] and exit the loop if we get one.\\n\\n_(This example is complete, it can be run \"as is\" with Python 3.10+ — you\\'ll need to add `asyncio.run(main())` to run `main`)_\\n\\nA [mermaid diagram](#mermaid-diagrams) for this graph can be generated with the following code:\\n\\n```py {title=\"ai_q_and_a_diagram.py\" py=\"3.10\"}\\nfrom ai_q_and_a_graph import Ask, question_graph\\n\\nquestion_graph.mermaid_code(start_node=Ask)\\n```\\n\\n```mermaid\\n---\\ntitle: question_graph\\n---\\nstateDiagram-v2\\n  [*] --> Ask\\n  Ask --> Answer\\n  Answer --> Evaluate\\n  Evaluate --> Reprimand\\n  Evaluate --> [*]\\n  Reprimand --> Ask\\n```\\n\\nYou maybe have noticed that although this examples transfers control flow out of the graph run, we\\'re still using [rich\\'s `Prompt.ask`][rich.prompt.PromptBase.ask] to get user input, with the process hanging while we wait for the user to enter a response. For an example of genuine out-of-process control flow, see the [question graph example](examples/question-graph.md).', 'metadata': {'file_path': 'pydantic-ai/docs/graph.md', 'section_title': 'Custom Control Flow'}}, {'source_file': 'pydantic-ai/docs/graph.md', 'title': 'Dependency Injection', 'content': '## Dependency Injection\\n\\nAs with PydanticAI, `pydantic-graph` supports dependency injection via a generic parameter on [`Graph`][pydantic_graph.graph.Graph] and [`BaseNode`][pydantic_graph.nodes.BaseNode], and the [`GraphRunContext.deps`][pydantic_graph.nodes.GraphRunContext.deps] fields.\\n\\nAs an example of dependency injection, let\\'s modify the `DivisibleBy5` example [above](#graph) to use a [`ProcessPoolExecutor`][concurrent.futures.ProcessPoolExecutor] to run the compute load in a separate process (this is a contrived example, `ProcessPoolExecutor` wouldn\\'t actually improve performance in this example):\\n\\n```py {title=\"deps_example.py\" py=\"3.10\" test=\"skip\" hl_lines=\"4 10-12 35-37 48-49\"}\\nfrom __future__ import annotations\\n\\nimport asyncio\\nfrom concurrent.futures import ProcessPoolExecutor\\nfrom dataclasses import dataclass\\n\\nfrom pydantic_graph import BaseNode, End, Graph, GraphRunContext\\n\\n\\n@dataclass\\nclass GraphDeps:\\n    executor: ProcessPoolExecutor\\n\\n\\n@dataclass\\nclass DivisibleBy5(BaseNode[None, None, int]):\\n    foo: int\\n\\n    async def run(\\n        self,\\n        ctx: GraphRunContext,\\n    ) -> Increment | End[int]:\\n        if self.foo % 5 == 0:\\n            return End(self.foo)\\n        else:\\n            return Increment(self.foo)\\n\\n\\n@dataclass\\nclass Increment(BaseNode):\\n    foo: int\\n\\n    async def run(self, ctx: GraphRunContext) -> DivisibleBy5:\\n        loop = asyncio.get_running_loop()\\n        compute_result = await loop.run_in_executor(\\n            ctx.deps.executor,\\n            self.compute,\\n        )\\n        return DivisibleBy5(compute_result)\\n\\n    def compute(self) -> int:\\n        return self.foo + 1\\n\\n\\nfives_graph = Graph(nodes=[DivisibleBy5, Increment])\\n\\n\\nasync def main():\\n    with ProcessPoolExecutor() as executor:\\n        deps = GraphDeps(executor)\\n        result, history = await fives_graph.run(DivisibleBy5(3), deps=deps)\\n    print(result)\\n    #> 5\\n    # the full history is quite verbose (see below), so we\\'ll just print the summary\\n    print([item.data_snapshot() for item in history])\\n    \"\"\"\\n    [\\n        DivisibleBy5(foo=3),\\n        Increment(foo=3),\\n        DivisibleBy5(foo=4),\\n        Increment(foo=4),\\n        DivisibleBy5(foo=5),\\n        End(data=5),\\n    ]\\n    \"\"\"\\n```\\n\\n_(This example is complete, it can be run \"as is\" with Python 3.10+ — you\\'ll need to add `asyncio.run(main())` to run `main`)_', 'metadata': {'file_path': 'pydantic-ai/docs/graph.md', 'section_title': 'Dependency Injection'}}, {'source_file': 'pydantic-ai/docs/graph.md', 'title': 'Mermaid Diagrams', 'content': '## Mermaid Diagrams\\n\\nPydantic Graph can generate [mermaid](https://mermaid.js.org/) [`stateDiagram-v2`](https://mermaid.js.org/syntax/stateDiagram.html) diagrams for graphs, as shown above.\\n\\nThese diagrams can be generated with:\\n\\n* [`Graph.mermaid_code`][pydantic_graph.graph.Graph.mermaid_code] to generate the mermaid code for a graph\\n* [`Graph.mermaid_image`][pydantic_graph.graph.Graph.mermaid_image] to generate an image of the graph using [mermaid.ink](https://mermaid.ink/)\\n* [`Graph.mermaid_save`][pydantic_graph.graph.Graph.mermaid_save] to generate an image of the graph using [mermaid.ink](https://mermaid.ink/) and save it to a file\\n\\nBeyond the diagrams shown above, you can also customize mermaid diagrams with the following options:\\n\\n* [`Edge`][pydantic_graph.nodes.Edge] allows you to apply a label to an edge\\n* [`BaseNode.docstring_notes`][pydantic_graph.nodes.BaseNode.docstring_notes] and [`BaseNode.get_note`][pydantic_graph.nodes.BaseNode.get_note] allows you to add notes to nodes\\n* The [`highlighted_nodes`][pydantic_graph.graph.Graph.mermaid_code] parameter allows you to highlight specific node(s) in the diagram\\n\\nPutting that together, we can edit the last [`ai_q_and_a_graph.py`](#custom-control-flow) example to:\\n\\n* add labels to some edges\\n* add a note to the `Ask` node\\n* highlight the `Answer` node\\n* save the diagram as a `PNG` image to file\\n\\n```python {title=\"ai_q_and_a_graph_extra.py\" test=\"skip\" lint=\"skip\" hl_lines=\"2 4 10-11 14 26 31\"}\\n...\\nfrom typing import Annotated\\n\\nfrom pydantic_graph import BaseNode, End, Graph, GraphRunContext, Edge\\n\\n...\\n\\n@dataclass\\nclass Ask(BaseNode[QuestionState]):\\n    \"\"\"Generate question using GPT-4o.\"\"\"\\n    docstring_notes = True\\n    async def run(\\n        self, ctx: GraphRunContext[QuestionState]\\n    ) -> Annotated[Answer, Edge(label=\\'Ask the question\\')]:\\n        ...\\n\\n...\\n\\n@dataclass\\nclass Evaluate(BaseNode[QuestionState]):\\n    answer: str\\n\\n    async def run(\\n            self,\\n            ctx: GraphRunContext[QuestionState],\\n    ) -> Annotated[End[str], Edge(label=\\'success\\')] | Reprimand:\\n        ...\\n\\n...\\n\\nquestion_graph.mermaid_save(\\'image.png\\', highlighted_nodes=[Answer])\\n```\\n\\n_(This example is not complete and cannot be run directly)_\\n\\nWould generate and image that looks like this:\\n\\n```mermaid\\n---\\ntitle: question_graph\\n---\\nstateDiagram-v2\\n  Ask --> Answer: Ask the question\\n  note right of Ask\\n    Judge the answer.\\n    Decide on next step.\\n  end note\\n  Answer --> Evaluate\\n  Evaluate --> Reprimand\\n  Evaluate --> [*]: success\\n  Reprimand --> Ask\\n\\nclassDef highlighted fill:#fdff32\\nclass Answer highlighted\\n```', 'metadata': {'file_path': 'pydantic-ai/docs/graph.md', 'section_title': 'Mermaid Diagrams'}}, {'source_file': 'pydantic-ai/docs/tools.md', 'title': 'Function Tools', 'content': '# Function Tools\\n\\nFunction tools provide a mechanism for models to retrieve extra information to help them generate a response.\\n\\nThey\\'re useful when it is impractical or impossible to put all the context an agent might need into the system prompt, or when you want to make agents\\' behavior more deterministic or reliable by deferring some of the logic required to generate a response to another (not necessarily AI-powered) tool.\\n\\n!!! info \"Function tools vs. RAG\"\\n    Function tools are basically the \"R\" of RAG (Retrieval-Augmented Generation) — they augment what the model can do by letting it request extra information.\\n\\n    The main semantic difference between PydanticAI Tools and RAG is RAG is synonymous with vector search, while PydanticAI tools are more general-purpose. (Note: we may add support for vector search functionality in the future, particularly an API for generating embeddings. See [#58](https://github.com/pydantic/pydantic-ai/issues/58))\\n\\nThere are a number of ways to register tools with an agent:\\n\\n* via the [`@agent.tool`][pydantic_ai.Agent.tool] decorator — for tools that need access to the agent [context][pydantic_ai.tools.RunContext]\\n* via the [`@agent.tool_plain`][pydantic_ai.Agent.tool_plain] decorator — for tools that do not need access to the agent [context][pydantic_ai.tools.RunContext]\\n* via the [`tools`][pydantic_ai.Agent.__init__] keyword argument to `Agent` which can take either plain functions, or instances of [`Tool`][pydantic_ai.tools.Tool]\\n\\n`@agent.tool` is considered the default decorator since in the majority of cases tools will need access to the agent context.\\n\\nHere\\'s an example using both:\\n\\n```python {title=\"dice_game.py\"}\\nimport random\\n\\nfrom pydantic_ai import Agent, RunContext\\n\\nagent = Agent(\\n    \\'gemini-1.5-flash\\',  # (1)!\\n    deps_type=str,  # (2)!\\n    system_prompt=(\\n        \"You\\'re a dice game, you should roll the die and see if the number \"\\n        \"you get back matches the user\\'s guess. If so, tell them they\\'re a winner. \"\\n        \"Use the player\\'s name in the response.\"\\n    ),\\n)\\n\\n\\n@agent.tool_plain  # (3)!\\ndef roll_die() -> str:\\n    \"\"\"Roll a six-sided die and return the result.\"\"\"\\n    return str(random.randint(1, 6))\\n\\n\\n@agent.tool  # (4)!\\ndef get_player_name(ctx: RunContext[str]) -> str:\\n    \"\"\"Get the player\\'s name.\"\"\"\\n    return ctx.deps\\n\\n\\ndice_result = agent.run_sync(\\'My guess is 4\\', deps=\\'Anne\\')  # (5)!\\nprint(dice_result.data)\\n#> Congratulations Anne, you guessed correctly! You\\'re a winner!\\n```\\n\\n1. This is a pretty simple task, so we can use the fast and cheap Gemini flash model.\\n2. We pass the user\\'s name as the dependency, to keep things simple we use just the name as a string as the dependency.\\n3. This tool doesn\\'t need any context, it just returns a random number. You could probably use a dynamic system prompt in this case.\\n4. This tool needs the player\\'s name, so it uses `RunContext` to access dependencies which are just the player\\'s name in this case.\\n5. Run the agent, passing the player\\'s name as the dependency.\\n\\n_(This example is complete, it can be run \"as is\")_\\n\\nLet\\'s print the messages from that game to see what happened:\\n\\n```python {title=\"dice_game_messages.py\"}\\nfrom dice_game import dice_result\\n\\nprint(dice_result.all_messages())\\n\"\"\"\\n[\\n    ModelRequest(\\n        parts=[\\n            SystemPromptPart(\\n                content=\"You\\'re a dice game, you should roll the die and see if the number you get back matches the user\\'s guess. If so, tell them they\\'re a winner. Use the player\\'s name in the response.\",\\n                dynamic_ref=None,\\n                part_kind=\\'system-prompt\\',\\n            ),\\n            UserPromptPart(\\n                content=\\'My guess is 4\\',\\n                timestamp=datetime.datetime(...),\\n                part_kind=\\'user-prompt\\',\\n            ),\\n        ],\\n        kind=\\'request\\',\\n    ),\\n    ModelResponse(\\n        parts=[\\n            ToolCallPart(\\n                tool_name=\\'roll_die\\',\\n                args=ArgsDict(args_dict={}),\\n                tool_call_id=None,\\n                part_kind=\\'tool-call\\',\\n            )\\n        ],\\n        timestamp=datetime.datetime(...),\\n        kind=\\'response\\',\\n    ),\\n    ModelRequest(\\n        parts=[\\n            ToolReturnPart(\\n                tool_name=\\'roll_die\\',\\n                content=\\'4\\',\\n                tool_call_id=None,\\n                timestamp=datetime.datetime(...),\\n                part_kind=\\'tool-return\\',\\n            )\\n        ],\\n        kind=\\'request\\',\\n    ),\\n    ModelResponse(\\n        parts=[\\n            ToolCallPart(\\n                tool_name=\\'get_player_name\\',\\n                args=ArgsDict(args_dict={}),\\n                tool_call_id=None,\\n                part_kind=\\'tool-call\\',\\n            )\\n        ],\\n        timestamp=datetime.datetime(...),\\n        kind=\\'response\\',\\n    ),\\n    ModelRequest(\\n        parts=[\\n            ToolReturnPart(\\n                tool_name=\\'get_player_name\\',\\n                content=\\'Anne\\',\\n                tool_call_id=None,\\n                timestamp=datetime.datetime(...),\\n                part_kind=\\'tool-return\\',\\n            )\\n        ],\\n        kind=\\'request\\',\\n    ),\\n    ModelResponse(\\n        parts=[\\n            TextPart(\\n                content=\"Congratulations Anne, you guessed correctly! You\\'re a winner!\",\\n                part_kind=\\'text\\',\\n            )\\n        ],\\n        timestamp=datetime.datetime(...),\\n        kind=\\'response\\',\\n    ),\\n]\\n\"\"\"\\n```\\n\\nWe can represent this with a diagram:\\n\\n```mermaid\\nsequenceDiagram\\n    participant Agent\\n    participant LLM\\n\\n    Note over Agent: Send prompts\\n    Agent ->> LLM: System: \"You\\'re a dice game...\"<br>User: \"My guess is 4\"\\n    activate LLM\\n    Note over LLM: LLM decides to use<br>a tool\\n\\n    LLM ->> Agent: Call tool<br>roll_die()\\n    deactivate LLM\\n    activate Agent\\n    Note over Agent: Rolls a six-sided die\\n\\n    Agent -->> LLM: ToolReturn<br>\"4\"\\n    deactivate Agent\\n    activate LLM\\n    Note over LLM: LLM decides to use<br>another tool\\n\\n    LLM ->> Agent: Call tool<br>get_player_name()\\n    deactivate LLM\\n    activate Agent\\n    Note over Agent: Retrieves player name\\n    Agent -->> LLM: ToolReturn<br>\"Anne\"\\n    deactivate Agent\\n    activate LLM\\n    Note over LLM: LLM constructs final response\\n\\n    LLM ->> Agent: ModelResponse<br>\"Congratulations Anne, ...\"\\n    deactivate LLM\\n    Note over Agent: Game session complete\\n```', 'metadata': {'file_path': 'pydantic-ai/docs/tools.md', 'section_title': 'Function Tools'}}, {'source_file': 'pydantic-ai/docs/tools.md', 'title': 'Registering Function Tools via kwarg', 'content': '## Registering Function Tools via kwarg\\n\\nAs well as using the decorators, we can register tools via the `tools` argument to the [`Agent` constructor][pydantic_ai.Agent.__init__]. This is useful when you want to re-use tools, and can also give more fine-grained control over the tools.\\n\\n```python {title=\"dice_game_tool_kwarg.py\"}\\nimport random\\n\\nfrom pydantic_ai import Agent, RunContext, Tool\\n\\n\\ndef roll_die() -> str:\\n    \"\"\"Roll a six-sided die and return the result.\"\"\"\\n    return str(random.randint(1, 6))\\n\\n\\ndef get_player_name(ctx: RunContext[str]) -> str:\\n    \"\"\"Get the player\\'s name.\"\"\"\\n    return ctx.deps\\n\\n\\nagent_a = Agent(\\n    \\'gemini-1.5-flash\\',\\n    deps_type=str,\\n    tools=[roll_die, get_player_name],  # (1)!\\n)\\nagent_b = Agent(\\n    \\'gemini-1.5-flash\\',\\n    deps_type=str,\\n    tools=[  # (2)!\\n        Tool(roll_die, takes_ctx=False),\\n        Tool(get_player_name, takes_ctx=True),\\n    ],\\n)\\ndice_result = agent_b.run_sync(\\'My guess is 4\\', deps=\\'Anne\\')\\nprint(dice_result.data)\\n#> Congratulations Anne, you guessed correctly! You\\'re a winner!\\n```\\n\\n1. The simplest way to register tools via the `Agent` constructor is to pass a list of functions, the function signature is inspected to determine if the tool takes [`RunContext`][pydantic_ai.tools.RunContext].\\n2. `agent_a` and `agent_b` are identical — but we can use [`Tool`][pydantic_ai.tools.Tool] to reuse tool definitions and give more fine-grained control over how tools are defined, e.g. setting their name or description, or using a custom [`prepare`](#tool-prepare) method.\\n\\n_(This example is complete, it can be run \"as is\")_', 'metadata': {'file_path': 'pydantic-ai/docs/tools.md', 'section_title': 'Registering Function Tools via kwarg'}}, {'source_file': 'pydantic-ai/docs/tools.md', 'title': 'Function Tools vs. Structured Results', 'content': '## Function Tools vs. Structured Results\\n\\nAs the name suggests, function tools use the model\\'s \"tools\" or \"functions\" API to let the model know what is available to call. Tools or functions are also used to define the schema(s) for structured responses, thus a model might have access to many tools, some of which call function tools while others end the run and return a result.', 'metadata': {'file_path': 'pydantic-ai/docs/tools.md', 'section_title': 'Function Tools vs. Structured Results'}}, {'source_file': 'pydantic-ai/docs/tools.md', 'title': 'Function tools and schema', 'content': '## Function tools and schema\\n\\nFunction parameters are extracted from the function signature, and all parameters except `RunContext` are used to build the schema for that tool call.\\n\\nEven better, PydanticAI extracts the docstring from functions and (thanks to [griffe](https://mkdocstrings.github.io/griffe/)) extracts parameter descriptions from the docstring and adds them to the schema.\\n\\n[Griffe supports](https://mkdocstrings.github.io/griffe/reference/docstrings/#docstrings) extracting parameter descriptions from `google`, `numpy` and `sphinx` style docstrings, and PydanticAI will infer the format to use based on the docstring. We plan to add support in the future to explicitly set the style to use, and warn/error if not all parameters are documented; see [#59](https://github.com/pydantic/pydantic-ai/issues/59).\\n\\nTo demonstrate a tool\\'s schema, here we use [`FunctionModel`][pydantic_ai.models.function.FunctionModel] to print the schema a model would receive:\\n\\n```python {title=\"tool_schema.py\"}\\nfrom pydantic_ai import Agent\\nfrom pydantic_ai.messages import ModelMessage, ModelResponse\\nfrom pydantic_ai.models.function import AgentInfo, FunctionModel\\n\\nagent = Agent()\\n\\n\\n@agent.tool_plain\\ndef foobar(a: int, b: str, c: dict[str, list[float]]) -> str:\\n    \"\"\"Get me foobar.\\n\\n    Args:\\n        a: apple pie\\n        b: banana cake\\n        c: carrot smoothie\\n    \"\"\"\\n    return f\\'{a} {b} {c}\\'\\n\\n\\ndef print_schema(messages: list[ModelMessage], info: AgentInfo) -> ModelResponse:\\n    tool = info.function_tools[0]\\n    print(tool.description)\\n    #> Get me foobar.\\n    print(tool.parameters_json_schema)\\n    \"\"\"\\n    {\\n        \\'properties\\': {\\n            \\'a\\': {\\'description\\': \\'apple pie\\', \\'title\\': \\'A\\', \\'type\\': \\'integer\\'},\\n            \\'b\\': {\\'description\\': \\'banana cake\\', \\'title\\': \\'B\\', \\'type\\': \\'string\\'},\\n            \\'c\\': {\\n                \\'additionalProperties\\': {\\'items\\': {\\'type\\': \\'number\\'}, \\'type\\': \\'array\\'},\\n                \\'description\\': \\'carrot smoothie\\',\\n                \\'title\\': \\'C\\',\\n                \\'type\\': \\'object\\',\\n            },\\n        },\\n        \\'required\\': [\\'a\\', \\'b\\', \\'c\\'],\\n        \\'type\\': \\'object\\',\\n        \\'additionalProperties\\': False,\\n    }\\n    \"\"\"\\n    return ModelResponse.from_text(content=\\'foobar\\')\\n\\n\\nagent.run_sync(\\'hello\\', model=FunctionModel(print_schema))\\n```\\n\\n_(This example is complete, it can be run \"as is\")_\\n\\nThe return type of tool can be anything which Pydantic can serialize to JSON as some models (e.g. Gemini) support semi-structured return values, some expect text (OpenAI) but seem to be just as good at extracting meaning from the data. If a Python object is returned and the model expects a string, the value will be serialized to JSON.\\n\\nIf a tool has a single parameter that can be represented as an object in JSON schema (e.g. dataclass, TypedDict, pydantic model), the schema for the tool is simplified to be just that object.\\n\\nHere\\'s an example, we use [`TestModel.agent_model_function_tools`][pydantic_ai.models.test.TestModel.agent_model_function_tools] to inspect the tool schema that would be passed to the model.\\n\\n```python {title=\"single_parameter_tool.py\"}\\nfrom pydantic import BaseModel\\n\\nfrom pydantic_ai import Agent\\nfrom pydantic_ai.models.test import TestModel\\n\\nagent = Agent()\\n\\n\\nclass Foobar(BaseModel):\\n    \"\"\"This is a Foobar\"\"\"\\n\\n    x: int\\n    y: str\\n    z: float = 3.14\\n\\n\\n@agent.tool_plain\\ndef foobar(f: Foobar) -> str:\\n    return str(f)\\n\\n\\ntest_model = TestModel()\\nresult = agent.run_sync(\\'hello\\', model=test_model)\\nprint(result.data)\\n#> {\"foobar\":\"x=0 y=\\'a\\' z=3.14\"}\\nprint(test_model.agent_model_function_tools)\\n\"\"\"\\n[\\n    ToolDefinition(\\n        name=\\'foobar\\',\\n        description=\\'This is a Foobar\\',\\n        parameters_json_schema={\\n            \\'properties\\': {\\n                \\'x\\': {\\'title\\': \\'X\\', \\'type\\': \\'integer\\'},\\n                \\'y\\': {\\'title\\': \\'Y\\', \\'type\\': \\'string\\'},\\n                \\'z\\': {\\'default\\': 3.14, \\'title\\': \\'Z\\', \\'type\\': \\'number\\'},\\n            },\\n            \\'required\\': [\\'x\\', \\'y\\'],\\n            \\'title\\': \\'Foobar\\',\\n            \\'type\\': \\'object\\',\\n        },\\n        outer_typed_dict_key=None,\\n    )\\n]\\n\"\"\"\\n```\\n\\n_(This example is complete, it can be run \"as is\")_', 'metadata': {'file_path': 'pydantic-ai/docs/tools.md', 'section_title': 'Function tools and schema'}}, {'source_file': 'pydantic-ai/docs/tools.md', 'title': 'Dynamic Function tools {#tool-prepare}', 'content': '## Dynamic Function tools {#tool-prepare}\\n\\nTools can optionally be defined with another function: `prepare`, which is called at each step of a run to\\ncustomize the definition of the tool passed to the model, or omit the tool completely from that step.\\n\\nA `prepare` method can be registered via the `prepare` kwarg to any of the tool registration mechanisms:\\n\\n* [`@agent.tool`][pydantic_ai.Agent.tool] decorator\\n* [`@agent.tool_plain`][pydantic_ai.Agent.tool_plain] decorator\\n* [`Tool`][pydantic_ai.tools.Tool] dataclass\\n\\nThe `prepare` method, should be of type [`ToolPrepareFunc`][pydantic_ai.tools.ToolPrepareFunc], a function which takes [`RunContext`][pydantic_ai.tools.RunContext] and a pre-built [`ToolDefinition`][pydantic_ai.tools.ToolDefinition], and should either return that `ToolDefinition` with or without modifying it, return a new `ToolDefinition`, or return `None` to indicate this tools should not be registered for that step.\\n\\nHere\\'s a simple `prepare` method that only includes the tool if the value of the dependency is `42`.\\n\\nAs with the previous example, we use [`TestModel`][pydantic_ai.models.test.TestModel] to demonstrate the behavior without calling a real model.\\n\\n```python {title=\"tool_only_if_42.py\"}\\nfrom typing import Union\\n\\nfrom pydantic_ai import Agent, RunContext\\nfrom pydantic_ai.tools import ToolDefinition\\n\\nagent = Agent(\\'test\\')\\n\\n\\nasync def only_if_42(\\n    ctx: RunContext[int], tool_def: ToolDefinition\\n) -> Union[ToolDefinition, None]:\\n    if ctx.deps == 42:\\n        return tool_def\\n\\n\\n@agent.tool(prepare=only_if_42)\\ndef hitchhiker(ctx: RunContext[int], answer: str) -> str:\\n    return f\\'{ctx.deps} {answer}\\'\\n\\n\\nresult = agent.run_sync(\\'testing...\\', deps=41)\\nprint(result.data)\\n#> success (no tool calls)\\nresult = agent.run_sync(\\'testing...\\', deps=42)\\nprint(result.data)\\n#> {\"hitchhiker\":\"42 a\"}\\n```\\n\\n_(This example is complete, it can be run \"as is\")_\\n\\nHere\\'s a more complex example where we change the description of the `name` parameter to based on the value of `deps`\\n\\nFor the sake of variation, we create this tool using the [`Tool`][pydantic_ai.tools.Tool] dataclass.\\n\\n```python {title=\"customize_name.py\"}\\nfrom __future__ import annotations\\n\\nfrom typing import Literal\\n\\nfrom pydantic_ai import Agent, RunContext\\nfrom pydantic_ai.models.test import TestModel\\nfrom pydantic_ai.tools import Tool, ToolDefinition\\n\\n\\ndef greet(name: str) -> str:\\n    return f\\'hello {name}\\'\\n\\n\\nasync def prepare_greet(\\n    ctx: RunContext[Literal[\\'human\\', \\'machine\\']], tool_def: ToolDefinition\\n) -> ToolDefinition | None:\\n    d = f\\'Name of the {ctx.deps} to greet.\\'\\n    tool_def.parameters_json_schema[\\'properties\\'][\\'name\\'][\\'description\\'] = d\\n    return tool_def\\n\\n\\ngreet_tool = Tool(greet, prepare=prepare_greet)\\ntest_model = TestModel()\\nagent = Agent(test_model, tools=[greet_tool], deps_type=Literal[\\'human\\', \\'machine\\'])\\n\\nresult = agent.run_sync(\\'testing...\\', deps=\\'human\\')\\nprint(result.data)\\n#> {\"greet\":\"hello a\"}\\nprint(test_model.agent_model_function_tools)\\n\"\"\"\\n[\\n    ToolDefinition(\\n        name=\\'greet\\',\\n        description=\\'\\',\\n        parameters_json_schema={\\n            \\'properties\\': {\\n                \\'name\\': {\\n                    \\'title\\': \\'Name\\',\\n                    \\'type\\': \\'string\\',\\n                    \\'description\\': \\'Name of the human to greet.\\',\\n                }\\n            },\\n            \\'required\\': [\\'name\\'],\\n            \\'type\\': \\'object\\',\\n            \\'additionalProperties\\': False,\\n        },\\n        outer_typed_dict_key=None,\\n    )\\n]\\n\"\"\"\\n```\\n\\n_(This example is complete, it can be run \"as is\")_', 'metadata': {'file_path': 'pydantic-ai/docs/tools.md', 'section_title': 'Dynamic Function tools {#tool-prepare}'}}, {'source_file': 'pydantic-ai/docs/examples/rag.md', 'title': 'RAG', 'content': '# RAG\\n\\nRAG search example. This demo allows you to ask question of the [logfire](https://pydantic.dev/logfire) documentation.\\n\\nDemonstrates:\\n\\n* [tools](../tools.md)\\n* [agent dependencies](../dependencies.md)\\n* RAG search\\n\\nThis is done by creating a database containing each section of the markdown documentation, then registering\\nthe search tool with the PydanticAI agent.\\n\\nLogic for extracting sections from markdown files and a JSON file with that data is available in\\n[this gist](https://gist.github.com/samuelcolvin/4b5bb9bb163b1122ff17e29e48c10992).\\n\\n[PostgreSQL with pgvector](https://github.com/pgvector/pgvector) is used as the search database, the easiest way to download and run pgvector is using Docker:\\n\\n```bash\\nmkdir postgres-data\\ndocker run --rm \\\\\\n  -e POSTGRES_PASSWORD=postgres \\\\\\n  -p 54320:5432 \\\\\\n  -v `pwd`/postgres-data:/var/lib/postgresql/data \\\\\\n  pgvector/pgvector:pg17\\n```\\n\\nAs with the [SQL gen](./sql-gen.md) example, we run postgres on port `54320` to avoid conflicts with any other postgres instances you may have running.\\nWe also mount the PostgreSQL `data` directory locally to persist the data if you need to stop and restart the container.\\n\\nWith that running and [dependencies installed and environment variables set](./index.md#usage), we can build the search database with (**WARNING**: this requires the `OPENAI_API_KEY` env variable and will calling the OpenAI embedding API around 300 times to generate embeddings for each section of the documentation):\\n\\n```bash\\npython/uv-run -m pydantic_ai_examples.rag build\\n```\\n\\n(Note building the database doesn\\'t use PydanticAI right now, instead it uses the OpenAI SDK directly.)\\n\\nYou can then ask the agent a question with:\\n\\n```bash\\npython/uv-run -m pydantic_ai_examples.rag search \"How do I configure logfire to work with FastAPI?\"\\n```', 'metadata': {'file_path': 'pydantic-ai/docs/examples/rag.md', 'section_title': 'RAG'}}, {'source_file': 'pydantic-ai/docs/examples/rag.md', 'title': 'Example Code', 'content': '## Example Code\\n\\n```python {title=\"rag.py\"}\\n#! examples/pydantic_ai_examples/rag.py\\n```', 'metadata': {'file_path': 'pydantic-ai/docs/examples/rag.md', 'section_title': 'Example Code'}}, {'source_file': 'pydantic-ai/docs/examples/bank-support.md', 'title': 'No Header', 'content': 'Small but complete example of using PydanticAI to build a support agent for a bank.\\n\\nDemonstrates:\\n\\n* [dynamic system prompt](../agents.md#system-prompts)\\n* [structured `result_type`](../results.md#structured-result-validation)\\n* [tools](../tools.md)', 'metadata': {'file_path': 'pydantic-ai/docs/examples/bank-support.md', 'section_title': 'No Header'}}, {'source_file': 'pydantic-ai/docs/examples/bank-support.md', 'title': 'Running the Example', 'content': '## Running the Example\\n\\nWith [dependencies installed and environment variables set](./index.md#usage), run:\\n\\n```bash\\npython/uv-run -m pydantic_ai_examples.bank_support\\n```\\n\\n(or `PYDANTIC_AI_MODEL=gemini-1.5-flash ...`)', 'metadata': {'file_path': 'pydantic-ai/docs/examples/bank-support.md', 'section_title': 'Running the Example'}}, {'source_file': 'pydantic-ai/docs/examples/bank-support.md', 'title': 'Example Code', 'content': '## Example Code\\n\\n```python {title=\"bank_support.py\"}\\n#! examples/pydantic_ai_examples/bank_support.py\\n```', 'metadata': {'file_path': 'pydantic-ai/docs/examples/bank-support.md', 'section_title': 'Example Code'}}, {'source_file': 'pydantic-ai/docs/examples/flight-booking.md', 'title': 'No Header', 'content': 'Example of a multi-agent flow where one agent delegates work to another, then hands off control to a third agent.\\n\\nDemonstrates:\\n\\n* [agent delegation](../multi-agent-applications.md#agent-delegation)\\n* [programmatic agent hand-off](../multi-agent-applications.md#programmatic-agent-hand-off)\\n* [usage limits](../agents.md#usage-limits)\\n\\nIn this scenario, a group of agents work together to find the best flight for a user.\\n\\nThe control flow for this example can be summarised as follows:\\n\\n```mermaid\\ngraph TD\\n  START --> search_agent(\"search agent\")\\n  search_agent --> extraction_agent(\"extraction agent\")\\n  extraction_agent --> search_agent\\n  search_agent --> human_confirm(\"human confirm\")\\n  human_confirm --> search_agent\\n  search_agent --> FAILED\\n  human_confirm --> find_seat_function(\"find seat function\")\\n  find_seat_function --> human_seat_choice(\"human seat choice\")\\n  human_seat_choice --> find_seat_agent(\"find seat agent\")\\n  find_seat_agent --> find_seat_function\\n  find_seat_function --> buy_flights(\"buy flights\")\\n  buy_flights --> SUCCESS\\n```', 'metadata': {'file_path': 'pydantic-ai/docs/examples/flight-booking.md', 'section_title': 'No Header'}}, {'source_file': 'pydantic-ai/docs/examples/flight-booking.md', 'title': 'Running the Example', 'content': '## Running the Example\\n\\nWith [dependencies installed and environment variables set](./index.md#usage), run:\\n\\n```bash\\npython/uv-run -m pydantic_ai_examples.flight_booking\\n```', 'metadata': {'file_path': 'pydantic-ai/docs/examples/flight-booking.md', 'section_title': 'Running the Example'}}, {'source_file': 'pydantic-ai/docs/examples/flight-booking.md', 'title': 'Example Code', 'content': '## Example Code\\n\\n```python {title=\"flight_booking.py\"}\\n#! examples/pydantic_ai_examples/flight_booking.py\\n```', 'metadata': {'file_path': 'pydantic-ai/docs/examples/flight-booking.md', 'section_title': 'Example Code'}}, {'source_file': 'pydantic-ai/docs/examples/stream-whales.md', 'title': 'No Header', 'content': 'Information about whales — an example of streamed structured response validation.\\n\\nDemonstrates:\\n\\n* [streaming structured responses](../results.md#streaming-structured-responses)\\n\\nThis script streams structured responses from GPT-4 about whales, validates the data\\nand displays it as a dynamic table using [`rich`](https://github.com/Textualize/rich) as the data is received.', 'metadata': {'file_path': 'pydantic-ai/docs/examples/stream-whales.md', 'section_title': 'No Header'}}, {'source_file': 'pydantic-ai/docs/examples/stream-whales.md', 'title': 'Running the Example', 'content': \"## Running the Example\\n\\nWith [dependencies installed and environment variables set](./index.md#usage), run:\\n\\n```bash\\npython/uv-run -m pydantic_ai_examples.stream_whales\\n```\\n\\nShould give an output like this:\\n\\n{{ video('53dd5e7664c20ae90ed90ae42f606bf3', 25) }}\", 'metadata': {'file_path': 'pydantic-ai/docs/examples/stream-whales.md', 'section_title': 'Running the Example'}}, {'source_file': 'pydantic-ai/docs/examples/stream-whales.md', 'title': 'Example Code', 'content': '## Example Code\\n\\n```python {title=\"stream_whales.py\"}\\n#! examples/pydantic_ai_examples/stream_whales.py\\n```', 'metadata': {'file_path': 'pydantic-ai/docs/examples/stream-whales.md', 'section_title': 'Example Code'}}, {'source_file': 'pydantic-ai/docs/examples/sql-gen.md', 'title': 'SQL Generation', 'content': '# SQL Generation\\n\\nExample demonstrating how to use PydanticAI to generate SQL queries based on user input.\\n\\nDemonstrates:\\n\\n* [dynamic system prompt](../agents.md#system-prompts)\\n* [structured `result_type`](../results.md#structured-result-validation)\\n* [result validation](../results.md#result-validators-functions)\\n* [agent dependencies](../dependencies.md)', 'metadata': {'file_path': 'pydantic-ai/docs/examples/sql-gen.md', 'section_title': 'SQL Generation'}}, {'source_file': 'pydantic-ai/docs/examples/sql-gen.md', 'title': 'Running the Example', 'content': '## Running the Example\\n\\nThe resulting SQL is validated by running it as an `EXPLAIN` query on PostgreSQL. To run the example, you first need to run PostgreSQL, e.g. via Docker:\\n\\n```bash\\ndocker run --rm -e POSTGRES_PASSWORD=postgres -p 54320:5432 postgres\\n```\\n_(we run postgres on port `54320` to avoid conflicts with any other postgres instances you may have running)_\\n\\nWith [dependencies installed and environment variables set](./index.md#usage), run:\\n\\n```bash\\npython/uv-run -m pydantic_ai_examples.sql_gen\\n```\\n\\nor to use a custom prompt:\\n\\n```bash\\npython/uv-run -m pydantic_ai_examples.sql_gen \"find me errors\"\\n```\\n\\nThis model uses `gemini-1.5-flash` by default since Gemini is good at single shot queries of this kind.', 'metadata': {'file_path': 'pydantic-ai/docs/examples/sql-gen.md', 'section_title': 'Running the Example'}}, {'source_file': 'pydantic-ai/docs/examples/sql-gen.md', 'title': 'Example Code', 'content': '## Example Code\\n\\n```python {title=\"sql_gen.py\"}\\n#! examples/pydantic_ai_examples/sql_gen.py\\n```', 'metadata': {'file_path': 'pydantic-ai/docs/examples/sql-gen.md', 'section_title': 'Example Code'}}, {'source_file': 'pydantic-ai/docs/examples/pydantic-model.md', 'title': 'Pydantic Model', 'content': '# Pydantic Model\\n\\nSimple example of using PydanticAI to construct a Pydantic model from a text input.\\n\\nDemonstrates:\\n\\n* [structured `result_type`](../results.md#structured-result-validation)', 'metadata': {'file_path': 'pydantic-ai/docs/examples/pydantic-model.md', 'section_title': 'Pydantic Model'}}, {'source_file': 'pydantic-ai/docs/examples/pydantic-model.md', 'title': 'Running the Example', 'content': '## Running the Example\\n\\nWith [dependencies installed and environment variables set](./index.md#usage), run:\\n\\n```bash\\npython/uv-run -m pydantic_ai_examples.pydantic_model\\n```\\n\\nThis examples uses `openai:gpt-4o` by default, but it works well with other models, e.g. you can run it\\nwith Gemini using:\\n\\n```bash\\nPYDANTIC_AI_MODEL=gemini-1.5-pro python/uv-run -m pydantic_ai_examples.pydantic_model\\n```\\n\\n(or `PYDANTIC_AI_MODEL=gemini-1.5-flash ...`)', 'metadata': {'file_path': 'pydantic-ai/docs/examples/pydantic-model.md', 'section_title': 'Running the Example'}}, {'source_file': 'pydantic-ai/docs/examples/pydantic-model.md', 'title': 'Example Code', 'content': '## Example Code\\n\\n```python {title=\"pydantic_model.py\"}\\n#! examples/pydantic_ai_examples/pydantic_model.py\\n```', 'metadata': {'file_path': 'pydantic-ai/docs/examples/pydantic-model.md', 'section_title': 'Example Code'}}, {'source_file': 'pydantic-ai/docs/examples/chat-app.md', 'title': 'Chat App with FastAPI', 'content': '# Chat App with FastAPI\\n\\nSimple chat app example build with FastAPI.\\n\\nDemonstrates:\\n\\n* [reusing chat history](../message-history.md)\\n* [serializing messages](../message-history.md#accessing-messages-from-results)\\n* [streaming responses](../results.md#streamed-results)\\n\\nThis demonstrates storing chat history between requests and using it to give the model context for new responses.\\n\\nMost of the complex logic here is between `chat_app.py` which streams the response to the browser,\\nand `chat_app.ts` which renders messages in the browser.', 'metadata': {'file_path': 'pydantic-ai/docs/examples/chat-app.md', 'section_title': 'Chat App with FastAPI'}}, {'source_file': 'pydantic-ai/docs/examples/chat-app.md', 'title': 'Running the Example', 'content': '## Running the Example\\n\\nWith [dependencies installed and environment variables set](./index.md#usage), run:\\n\\n```bash\\npython/uv-run -m pydantic_ai_examples.chat_app\\n```\\n\\nThen open the app at [localhost:8000](http://localhost:8000).\\n\\n![Example conversation](../img/chat-app-example.png)', 'metadata': {'file_path': 'pydantic-ai/docs/examples/chat-app.md', 'section_title': 'Running the Example'}}, {'source_file': 'pydantic-ai/docs/examples/chat-app.md', 'title': 'Example Code', 'content': '## Example Code\\n\\nPython code that runs the chat app:\\n\\n```python {title=\"chat_app.py\"}\\n#! examples/pydantic_ai_examples/chat_app.py\\n```\\n\\nSimple HTML page to render the app:\\n\\n```html {title=\"chat_app.html\"}\\n#! examples/pydantic_ai_examples/chat_app.html\\n```\\n\\nTypeScript to handle rendering the messages, to keep this simple (and at the risk of offending frontend developers) the typescript code is passed to the browser as plain text and transpiled in the browser.\\n\\n```ts {title=\"chat_app.ts\"}\\n#! examples/pydantic_ai_examples/chat_app.ts\\n```', 'metadata': {'file_path': 'pydantic-ai/docs/examples/chat-app.md', 'section_title': 'Example Code'}}, {'source_file': 'pydantic-ai/docs/examples/question-graph.md', 'title': 'Question Graph', 'content': '# Question Graph\\n\\nExample of a graph for asking and evaluating questions.\\n\\nDemonstrates:\\n\\n* [`pydantic_graph`](../graph.md)', 'metadata': {'file_path': 'pydantic-ai/docs/examples/question-graph.md', 'section_title': 'Question Graph'}}, {'source_file': 'pydantic-ai/docs/examples/question-graph.md', 'title': 'Running the Example', 'content': '## Running the Example\\n\\nWith [dependencies installed and environment variables set](./index.md#usage), run:\\n\\n```bash\\npython/uv-run -m pydantic_ai_examples.question_graph\\n```', 'metadata': {'file_path': 'pydantic-ai/docs/examples/question-graph.md', 'section_title': 'Running the Example'}}, {'source_file': 'pydantic-ai/docs/examples/question-graph.md', 'title': 'Example Code', 'content': '## Example Code\\n\\n```python {title=\"question_graph.py\"}\\n#! examples/pydantic_ai_examples/question_graph.py\\n```\\n\\nThe mermaid diagram generated in this example looks like this:\\n\\n```mermaid\\n---\\ntitle: question_graph\\n---\\nstateDiagram-v2\\n  [*] --> Ask\\n  Ask --> Answer: ask the question\\n  Answer --> Evaluate: answer the question\\n  Evaluate --> Congratulate\\n  Evaluate --> Castigate\\n  Congratulate --> [*]: success\\n  Castigate --> Ask: try again\\n```', 'metadata': {'file_path': 'pydantic-ai/docs/examples/question-graph.md', 'section_title': 'Example Code'}}, {'source_file': 'pydantic-ai/docs/examples/index.md', 'title': 'Examples', 'content': '# Examples\\n\\nExamples of how to use PydanticAI and what it can do.', 'metadata': {'file_path': 'pydantic-ai/docs/examples/index.md', 'section_title': 'Examples'}}, {'source_file': 'pydantic-ai/docs/examples/index.md', 'title': 'Usage', 'content': '## Usage\\n\\nThese examples are distributed with `pydantic-ai` so you can run them either by cloning the [pydantic-ai repo](https://github.com/pydantic/pydantic-ai) or by simply installing `pydantic-ai` from PyPI with `pip` or `uv`.\\n\\n### Installing required dependencies\\n\\nEither way you\\'ll need to install extra dependencies to run some examples, you just need to install the `examples` optional dependency group.\\n\\nIf you\\'ve installed `pydantic-ai` via pip/uv, you can install the extra dependencies with:\\n\\n```bash\\npip/uv-add \\'pydantic-ai[examples]\\'\\n```\\n\\nIf you clone the repo, you should instead use `uv sync --extra examples` to install extra dependencies.\\n\\n### Setting model environment variables\\n\\nThese examples will need you to set up authentication with one or more of the LLMs, see the [model configuration](../models.md) docs for details on how to do this.\\n\\nTL;DR: in most cases you\\'ll need to set one of the following environment variables:\\n\\n=== \"OpenAI\"\\n\\n    ```bash\\n    export OPENAI_API_KEY=your-api-key\\n    ```\\n\\n=== \"Google Gemini\"\\n\\n    ```bash\\n    export GEMINI_API_KEY=your-api-key\\n    ```\\n\\n### Running Examples\\n\\nTo run the examples (this will work whether you installed `pydantic_ai`, or cloned the repo), run:\\n\\n```bash\\npython/uv-run -m pydantic_ai_examples.<example_module_name>\\n```\\n\\nFor examples, to run the very simple [`pydantic_model`](./pydantic-model.md) example:\\n\\n```bash\\npython/uv-run -m pydantic_ai_examples.pydantic_model\\n```\\n\\nIf you like one-liners and you\\'re using uv, you can run a pydantic-ai example with zero setup:\\n\\n```bash\\nOPENAI_API_KEY=\\'your-api-key\\' \\\\\\n  uv run --with \\'pydantic-ai[examples]\\' \\\\\\n  -m pydantic_ai_examples.pydantic_model\\n```\\n\\n---\\n\\nYou\\'ll probably want to edit examples in addition to just running them. You can copy the examples to a new directory with:\\n\\n```bash\\npython/uv-run -m pydantic_ai_examples --copy-to examples/\\n```', 'metadata': {'file_path': 'pydantic-ai/docs/examples/index.md', 'section_title': 'Usage'}}, {'source_file': 'pydantic-ai/docs/examples/stream-markdown.md', 'title': 'No Header', 'content': \"This example shows how to stream markdown from an agent, using the [`rich`](https://github.com/Textualize/rich) library to highlight the output in the terminal.\\n\\nIt'll run the example with both OpenAI and Google Gemini models if the required environment variables are set.\\n\\nDemonstrates:\\n\\n* [streaming text responses](../results.md#streaming-text)\", 'metadata': {'file_path': 'pydantic-ai/docs/examples/stream-markdown.md', 'section_title': 'No Header'}}, {'source_file': 'pydantic-ai/docs/examples/stream-markdown.md', 'title': 'Running the Example', 'content': '## Running the Example\\n\\nWith [dependencies installed and environment variables set](./index.md#usage), run:\\n\\n```bash\\npython/uv-run -m pydantic_ai_examples.stream_markdown\\n```', 'metadata': {'file_path': 'pydantic-ai/docs/examples/stream-markdown.md', 'section_title': 'Running the Example'}}, {'source_file': 'pydantic-ai/docs/examples/stream-markdown.md', 'title': 'Example Code', 'content': '## Example Code\\n\\n```python\\n#! examples/pydantic_ai_examples/stream_markdown.py\\n```', 'metadata': {'file_path': 'pydantic-ai/docs/examples/stream-markdown.md', 'section_title': 'Example Code'}}, {'source_file': 'pydantic-ai/docs/examples/weather-agent.md', 'title': 'No Header', 'content': 'Example of PydanticAI with multiple tools which the LLM needs to call in turn to answer a question.\\n\\nDemonstrates:\\n\\n* [tools](../tools.md)\\n* [agent dependencies](../dependencies.md)\\n* [streaming text responses](../results.md#streaming-text)\\n* Building a [Gradio](https://www.gradio.app/) UI for the agent\\n\\nIn this case the idea is a \"weather\" agent — the user can ask for the weather in multiple locations,\\nthe agent will use the `get_lat_lng` tool to get the latitude and longitude of the locations, then use\\nthe `get_weather` tool to get the weather for those locations.', 'metadata': {'file_path': 'pydantic-ai/docs/examples/weather-agent.md', 'section_title': 'No Header'}}, {'source_file': 'pydantic-ai/docs/examples/weather-agent.md', 'title': 'Running the Example', 'content': \"## Running the Example\\n\\nTo run this example properly, you might want to add two extra API keys **(Note if either key is missing, the code will fall back to dummy data, so they're not required)**:\\n\\n* A weather API key from [tomorrow.io](https://www.tomorrow.io/weather-api/) set via `WEATHER_API_KEY`\\n* A geocoding API key from [geocode.maps.co](https://geocode.maps.co/) set via `GEO_API_KEY`\\n\\nWith [dependencies installed and environment variables set](./index.md#usage), run:\\n\\n```bash\\npython/uv-run -m pydantic_ai_examples.weather_agent\\n```\", 'metadata': {'file_path': 'pydantic-ai/docs/examples/weather-agent.md', 'section_title': 'Running the Example'}}, {'source_file': 'pydantic-ai/docs/examples/weather-agent.md', 'title': 'Example Code', 'content': '## Example Code\\n\\n```python {title=\"pydantic_ai_examples/weather_agent.py\"}\\n#! examples/pydantic_ai_examples/weather_agent.py\\n```', 'metadata': {'file_path': 'pydantic-ai/docs/examples/weather-agent.md', 'section_title': 'Example Code'}}, {'source_file': 'pydantic-ai/docs/examples/weather-agent.md', 'title': 'Running the UI', 'content': \"## Running the UI\\n\\nYou can build multi-turn chat applications for your agent with [Gradio](https://www.gradio.app/), a framework for building AI web applications entirely in python. Gradio comes with built-in chat components and agent support so the entire UI will be implemented in a single python file!\\n\\nHere's what the UI looks like for the weather agent:\\n\\n{{ video('c549d8d8827ded15f326f998e428e6c3', 6) }}\\n\\nNote, to run the UI, you'll need Python 3.10+.\\n\\n```bash\\npip install gradio>=5.9.0\\npython/uv-run -m pydantic_ai_examples.weather_agent_gradio\\n```\", 'metadata': {'file_path': 'pydantic-ai/docs/examples/weather-agent.md', 'section_title': 'Running the UI'}}, {'source_file': 'pydantic-ai/docs/examples/weather-agent.md', 'title': 'UI Code', 'content': '## UI Code\\n\\n```python {title=\"pydantic_ai_examples/weather_agent_gradio.py\"}\\n#! pydantic_ai_examples/weather_agent_gradio.py\\n```', 'metadata': {'file_path': 'pydantic-ai/docs/examples/weather-agent.md', 'section_title': 'UI Code'}}, {'source_file': 'pydantic-ai/docs/api/agent.md', 'title': '`pydantic_ai.agent`', 'content': '# `pydantic_ai.agent`\\n\\n::: pydantic_ai.agent\\n    options:\\n        members:\\n            - Agent\\n            - EndStrategy\\n            - RunResultData\\n            - capture_run_messages', 'metadata': {'file_path': 'pydantic-ai/docs/api/agent.md', 'section_title': '`pydantic_ai.agent`'}}, {'source_file': 'pydantic-ai/docs/api/exceptions.md', 'title': '`pydantic_ai.exceptions`', 'content': '# `pydantic_ai.exceptions`\\n\\n::: pydantic_ai.exceptions', 'metadata': {'file_path': 'pydantic-ai/docs/api/exceptions.md', 'section_title': '`pydantic_ai.exceptions`'}}, {'source_file': 'pydantic-ai/docs/api/settings.md', 'title': '`pydantic_ai.settings`', 'content': '# `pydantic_ai.settings`\\n\\n::: pydantic_ai.settings\\n    options:\\n      inherited_members: true\\n      members:\\n        - ModelSettings\\n        - UsageLimits', 'metadata': {'file_path': 'pydantic-ai/docs/api/settings.md', 'section_title': '`pydantic_ai.settings`'}}, {'source_file': 'pydantic-ai/docs/api/messages.md', 'title': '`pydantic_ai.messages`', 'content': '# `pydantic_ai.messages`\\n\\nThe structure of [`ModelMessage`][pydantic_ai.messages.ModelMessage] can be shown as a graph:\\n\\n```mermaid\\ngraph RL\\n    SystemPromptPart(SystemPromptPart) --- ModelRequestPart\\n    UserPromptPart(UserPromptPart) --- ModelRequestPart\\n    ToolReturnPart(ToolReturnPart) --- ModelRequestPart\\n    RetryPromptPart(RetryPromptPart) --- ModelRequestPart\\n    TextPart(TextPart) --- ModelResponsePart\\n    ToolCallPart(ToolCallPart) --- ModelResponsePart\\n    ModelRequestPart(\"ModelRequestPart<br>(Union)\") --- ModelRequest\\n    ModelRequest(\"ModelRequest(parts=list[...])\") --- ModelMessage\\n    ModelResponsePart(\"ModelResponsePart<br>(Union)\") --- ModelResponse\\n    ModelResponse(\"ModelResponse(parts=list[...])\") --- ModelMessage(\"ModelMessage<br>(Union)\")\\n```\\n\\n::: pydantic_ai.messages', 'metadata': {'file_path': 'pydantic-ai/docs/api/messages.md', 'section_title': '`pydantic_ai.messages`'}}, {'source_file': 'pydantic-ai/docs/api/result.md', 'title': '`pydantic_ai.result`', 'content': '# `pydantic_ai.result`\\n\\n::: pydantic_ai.result\\n    options:\\n      inherited_members: true', 'metadata': {'file_path': 'pydantic-ai/docs/api/result.md', 'section_title': '`pydantic_ai.result`'}}, {'source_file': 'pydantic-ai/docs/api/usage.md', 'title': '`pydantic_ai.usage`', 'content': '# `pydantic_ai.usage`\\n\\n::: pydantic_ai.usage', 'metadata': {'file_path': 'pydantic-ai/docs/api/usage.md', 'section_title': '`pydantic_ai.usage`'}}, {'source_file': 'pydantic-ai/docs/api/format_as_xml.md', 'title': '`pydantic_ai.format_as_xml`', 'content': '# `pydantic_ai.format_as_xml`\\n\\n::: pydantic_ai.format_as_xml', 'metadata': {'file_path': 'pydantic-ai/docs/api/format_as_xml.md', 'section_title': '`pydantic_ai.format_as_xml`'}}, {'source_file': 'pydantic-ai/docs/api/tools.md', 'title': '`pydantic_ai.tools`', 'content': '# `pydantic_ai.tools`\\n\\n::: pydantic_ai.tools', 'metadata': {'file_path': 'pydantic-ai/docs/api/tools.md', 'section_title': '`pydantic_ai.tools`'}}, {'source_file': 'pydantic-ai/docs/api/models/anthropic.md', 'title': '`pydantic_ai.models.anthropic`', 'content': '# `pydantic_ai.models.anthropic`', 'metadata': {'file_path': 'pydantic-ai/docs/api/models/anthropic.md', 'section_title': '`pydantic_ai.models.anthropic`'}}, {'source_file': 'pydantic-ai/docs/api/models/anthropic.md', 'title': 'Setup', 'content': '## Setup\\n\\nFor details on how to set up authentication with this model, see [model configuration for Anthropic](../../models.md#anthropic).\\n\\n::: pydantic_ai.models.anthropic', 'metadata': {'file_path': 'pydantic-ai/docs/api/models/anthropic.md', 'section_title': 'Setup'}}, {'source_file': 'pydantic-ai/docs/api/models/groq.md', 'title': '`pydantic_ai.models.groq`', 'content': '# `pydantic_ai.models.groq`', 'metadata': {'file_path': 'pydantic-ai/docs/api/models/groq.md', 'section_title': '`pydantic_ai.models.groq`'}}, {'source_file': 'pydantic-ai/docs/api/models/groq.md', 'title': 'Setup', 'content': '## Setup\\n\\nFor details on how to set up authentication with this model, see [model configuration for Groq](../../models.md#groq).\\n\\n::: pydantic_ai.models.groq', 'metadata': {'file_path': 'pydantic-ai/docs/api/models/groq.md', 'section_title': 'Setup'}}, {'source_file': 'pydantic-ai/docs/api/models/openai.md', 'title': '`pydantic_ai.models.openai`', 'content': '# `pydantic_ai.models.openai`', 'metadata': {'file_path': 'pydantic-ai/docs/api/models/openai.md', 'section_title': '`pydantic_ai.models.openai`'}}, {'source_file': 'pydantic-ai/docs/api/models/openai.md', 'title': 'Setup', 'content': '## Setup\\n\\nFor details on how to set up authentication with this model, see [model configuration for OpenAI](../../models.md#openai).\\n\\n::: pydantic_ai.models.openai', 'metadata': {'file_path': 'pydantic-ai/docs/api/models/openai.md', 'section_title': 'Setup'}}, {'source_file': 'pydantic-ai/docs/api/models/base.md', 'title': '`pydantic_ai.models`', 'content': '# `pydantic_ai.models`\\n\\n::: pydantic_ai.models\\n    options:\\n      members:\\n        - KnownModelName\\n        - Model\\n        - AgentModel\\n        - AbstractToolDefinition\\n        - StreamedResponse\\n        - ALLOW_MODEL_REQUESTS\\n        - check_allow_model_requests\\n        - override_allow_model_requests', 'metadata': {'file_path': 'pydantic-ai/docs/api/models/base.md', 'section_title': '`pydantic_ai.models`'}}, {'source_file': 'pydantic-ai/docs/api/models/ollama.md', 'title': '`pydantic_ai.models.ollama`', 'content': '# `pydantic_ai.models.ollama`', 'metadata': {'file_path': 'pydantic-ai/docs/api/models/ollama.md', 'section_title': '`pydantic_ai.models.ollama`'}}, {'source_file': 'pydantic-ai/docs/api/models/ollama.md', 'title': 'Setup', 'content': '## Setup\\n\\nFor details on how to set up authentication with this model, see [model configuration for Ollama](../../models.md#ollama).', 'metadata': {'file_path': 'pydantic-ai/docs/api/models/ollama.md', 'section_title': 'Setup'}}, {'source_file': 'pydantic-ai/docs/api/models/ollama.md', 'title': 'Example local usage', 'content': '## Example local usage\\n\\nWith `ollama` installed, you can run the server with the model you want to use:\\n\\n```bash {title=\"terminal-run-ollama\"}\\nollama run llama3.2\\n```\\n(this will pull the `llama3.2` model if you don\\'t already have it downloaded)\\n\\nThen run your code, here\\'s a minimal example:\\n\\n```python {title=\"ollama_example.py\"}\\nfrom pydantic import BaseModel\\n\\nfrom pydantic_ai import Agent\\n\\n\\nclass CityLocation(BaseModel):\\n    city: str\\n    country: str\\n\\n\\nagent = Agent(\\'ollama:llama3.2\\', result_type=CityLocation)\\n\\nresult = agent.run_sync(\\'Where were the olympics held in 2012?\\')\\nprint(result.data)\\n#> city=\\'London\\' country=\\'United Kingdom\\'\\nprint(result.usage())\\n\"\"\"\\nUsage(requests=1, request_tokens=57, response_tokens=8, total_tokens=65, details=None)\\n\"\"\"\\n```', 'metadata': {'file_path': 'pydantic-ai/docs/api/models/ollama.md', 'section_title': 'Example local usage'}}, {'source_file': 'pydantic-ai/docs/api/models/ollama.md', 'title': 'Example using a remote server', 'content': '## Example using a remote server\\n\\n```python {title=\"ollama_example_with_remote_server.py\"}\\nfrom pydantic import BaseModel\\n\\nfrom pydantic_ai import Agent\\nfrom pydantic_ai.models.ollama import OllamaModel\\n\\nollama_model = OllamaModel(\\n    model_name=\\'qwen2.5-coder:7b\\',  # (1)!\\n    base_url=\\'http://192.168.1.74:11434/v1\\',  # (2)!\\n)\\n\\n\\nclass CityLocation(BaseModel):\\n    city: str\\n    country: str\\n\\n\\nagent = Agent(model=ollama_model, result_type=CityLocation)\\n\\nresult = agent.run_sync(\\'Where were the olympics held in 2012?\\')\\nprint(result.data)\\n#> city=\\'London\\' country=\\'United Kingdom\\'\\nprint(result.usage())\\n\"\"\"\\nUsage(requests=1, request_tokens=57, response_tokens=8, total_tokens=65, details=None)\\n\"\"\"\\n```\\n\\n1. The name of the model running on the remote server\\n2. The url of the remote server\\n\\nSee [`OllamaModel`][pydantic_ai.models.ollama.OllamaModel] for more information\\n\\n::: pydantic_ai.models.ollama', 'metadata': {'file_path': 'pydantic-ai/docs/api/models/ollama.md', 'section_title': 'Example using a remote server'}}, {'source_file': 'pydantic-ai/docs/api/models/function.md', 'title': '`pydantic_ai.models.function`', 'content': '# `pydantic_ai.models.function`\\n\\nA model controlled by a local function.\\n\\n[`FunctionModel`][pydantic_ai.models.function.FunctionModel] is similar to [`TestModel`](test.md),\\nbut allows greater control over the model\\'s behavior.\\n\\nIts primary use case is for more advanced unit testing than is possible with `TestModel`.\\n\\nHere\\'s a minimal example:\\n\\n```py {title=\"function_model_usage.py\" call_name=\"test_my_agent\" noqa=\"I001\"}\\nfrom pydantic_ai import Agent\\nfrom pydantic_ai.messages import ModelMessage, ModelResponse\\nfrom pydantic_ai.models.function import FunctionModel, AgentInfo\\n\\nmy_agent = Agent(\\'openai:gpt-4o\\')\\n\\n\\nasync def model_function(\\n    messages: list[ModelMessage], info: AgentInfo\\n) -> ModelResponse:\\n    print(messages)\\n    \"\"\"\\n    [\\n        ModelRequest(\\n            parts=[\\n                UserPromptPart(\\n                    content=\\'Testing my agent...\\',\\n                    timestamp=datetime.datetime(...),\\n                    part_kind=\\'user-prompt\\',\\n                )\\n            ],\\n            kind=\\'request\\',\\n        )\\n    ]\\n    \"\"\"\\n    print(info)\\n    \"\"\"\\n    AgentInfo(\\n        function_tools=[], allow_text_result=True, result_tools=[], model_settings=None\\n    )\\n    \"\"\"\\n    return ModelResponse.from_text(\\'hello world\\')\\n\\n\\nasync def test_my_agent():\\n    \"\"\"Unit test for my_agent, to be run by pytest.\"\"\"\\n    with my_agent.override(model=FunctionModel(model_function)):\\n        result = await my_agent.run(\\'Testing my agent...\\')\\n        assert result.data == \\'hello world\\'\\n```\\n\\nSee [Unit testing with `FunctionModel`](../../testing-evals.md#unit-testing-with-functionmodel) for detailed documentation.\\n\\n::: pydantic_ai.models.function', 'metadata': {'file_path': 'pydantic-ai/docs/api/models/function.md', 'section_title': '`pydantic_ai.models.function`'}}, {'source_file': 'pydantic-ai/docs/api/models/mistral.md', 'title': '`pydantic_ai.models.mistral`', 'content': '# `pydantic_ai.models.mistral`', 'metadata': {'file_path': 'pydantic-ai/docs/api/models/mistral.md', 'section_title': '`pydantic_ai.models.mistral`'}}, {'source_file': 'pydantic-ai/docs/api/models/mistral.md', 'title': 'Setup', 'content': '## Setup\\n\\nFor details on how to set up authentication with this model, see [model configuration for Mistral](../../models.md#mistral).\\n\\n::: pydantic_ai.models.mistral', 'metadata': {'file_path': 'pydantic-ai/docs/api/models/mistral.md', 'section_title': 'Setup'}}, {'source_file': 'pydantic-ai/docs/api/models/vertexai.md', 'title': '`pydantic_ai.models.vertexai`', 'content': '# `pydantic_ai.models.vertexai`\\n\\nCustom interface to the `*-aiplatform.googleapis.com` API for Gemini models.\\n\\nThis model uses [`GeminiAgentModel`][pydantic_ai.models.gemini.GeminiAgentModel] with just the URL and auth method\\nchanged from [`GeminiModel`][pydantic_ai.models.gemini.GeminiModel], it relies on the VertexAI\\n[`generateContent`](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints/generateContent)\\nand\\n[`streamGenerateContent`](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints/streamGenerateContent)\\nfunction endpoints\\nhaving the same schemas as the equivalent [Gemini endpoints][pydantic_ai.models.gemini.GeminiModel].', 'metadata': {'file_path': 'pydantic-ai/docs/api/models/vertexai.md', 'section_title': '`pydantic_ai.models.vertexai`'}}, {'source_file': 'pydantic-ai/docs/api/models/vertexai.md', 'title': 'Setup', 'content': '## Setup\\n\\nFor details on how to set up authentication with this model as well as a comparison with the `generativelanguage.googleapis.com` API used by [`GeminiModel`][pydantic_ai.models.gemini.GeminiModel],\\nsee [model configuration for Gemini via VertexAI](../../models.md#gemini-via-vertexai).', 'metadata': {'file_path': 'pydantic-ai/docs/api/models/vertexai.md', 'section_title': 'Setup'}}, {'source_file': 'pydantic-ai/docs/api/models/vertexai.md', 'title': 'Example Usage', 'content': '## Example Usage\\n\\nWith the default google project already configured in your environment using \"application default credentials\":\\n\\n```python {title=\"vertex_example_env.py\"}\\nfrom pydantic_ai import Agent\\nfrom pydantic_ai.models.vertexai import VertexAIModel\\n\\nmodel = VertexAIModel(\\'gemini-1.5-flash\\')\\nagent = Agent(model)\\nresult = agent.run_sync(\\'Tell me a joke.\\')\\nprint(result.data)\\n#> Did you hear about the toothpaste scandal? They called it Colgate.\\n```\\n\\nOr using a service account JSON file:\\n\\n```python {title=\"vertex_example_service_account.py\"}\\nfrom pydantic_ai import Agent\\nfrom pydantic_ai.models.vertexai import VertexAIModel\\n\\nmodel = VertexAIModel(\\n    \\'gemini-1.5-flash\\',\\n    service_account_file=\\'path/to/service-account.json\\',\\n)\\nagent = Agent(model)\\nresult = agent.run_sync(\\'Tell me a joke.\\')\\nprint(result.data)\\n#> Did you hear about the toothpaste scandal? They called it Colgate.\\n```\\n\\n::: pydantic_ai.models.vertexai', 'metadata': {'file_path': 'pydantic-ai/docs/api/models/vertexai.md', 'section_title': 'Example Usage'}}, {'source_file': 'pydantic-ai/docs/api/models/gemini.md', 'title': '`pydantic_ai.models.gemini`', 'content': \"# `pydantic_ai.models.gemini`\\n\\nCustom interface to the `generativelanguage.googleapis.com` API using\\n[HTTPX](https://www.python-httpx.org/) and [Pydantic](https://docs.pydantic.dev/latest/).\\n\\nThe Google SDK for interacting with the `generativelanguage.googleapis.com` API\\n[`google-generativeai`](https://ai.google.dev/gemini-api/docs/quickstart?lang=python) reads like it was written by a\\nJava developer who thought they knew everything about OOP, spent 30 minutes trying to learn Python,\\ngave up and decided to build the library to prove how horrible Python is. It also doesn't use httpx for HTTP requests,\\nand tries to implement tool calling itself, but doesn't use Pydantic or equivalent for validation.\\n\\nWe therefore implement support for the API directly.\\n\\nDespite these shortcomings, the Gemini model is actually quite powerful and very fast.\", 'metadata': {'file_path': 'pydantic-ai/docs/api/models/gemini.md', 'section_title': '`pydantic_ai.models.gemini`'}}, {'source_file': 'pydantic-ai/docs/api/models/gemini.md', 'title': 'Setup', 'content': '## Setup\\n\\nFor details on how to set up authentication with this model, see [model configuration for Gemini](../../models.md#gemini).\\n\\n::: pydantic_ai.models.gemini', 'metadata': {'file_path': 'pydantic-ai/docs/api/models/gemini.md', 'section_title': 'Setup'}}, {'source_file': 'pydantic-ai/docs/api/models/test.md', 'title': '`pydantic_ai.models.test`', 'content': '# `pydantic_ai.models.test`\\n\\nUtility model for quickly testing apps built with PydanticAI.\\n\\nHere\\'s a minimal example:\\n\\n```py {title=\"test_model_usage.py\" call_name=\"test_my_agent\" noqa=\"I001\"}\\nfrom pydantic_ai import Agent\\nfrom pydantic_ai.models.test import TestModel\\n\\nmy_agent = Agent(\\'openai:gpt-4o\\', system_prompt=\\'...\\')\\n\\n\\nasync def test_my_agent():\\n    \"\"\"Unit test for my_agent, to be run by pytest.\"\"\"\\n    m = TestModel()\\n    with my_agent.override(model=m):\\n        result = await my_agent.run(\\'Testing my agent...\\')\\n        assert result.data == \\'success (no tool calls)\\'\\n    assert m.agent_model_function_tools == []\\n```\\n\\nSee [Unit testing with `TestModel`](../../testing-evals.md#unit-testing-with-testmodel) for detailed documentation.\\n\\n::: pydantic_ai.models.test', 'metadata': {'file_path': 'pydantic-ai/docs/api/models/test.md', 'section_title': '`pydantic_ai.models.test`'}}, {'source_file': 'pydantic-ai/docs/api/pydantic_graph/exceptions.md', 'title': '`pydantic_graph.exceptions`', 'content': '# `pydantic_graph.exceptions`\\n\\n::: pydantic_graph.exceptions', 'metadata': {'file_path': 'pydantic-ai/docs/api/pydantic_graph/exceptions.md', 'section_title': '`pydantic_graph.exceptions`'}}, {'source_file': 'pydantic-ai/docs/api/pydantic_graph/state.md', 'title': '`pydantic_graph.state`', 'content': '# `pydantic_graph.state`\\n\\n::: pydantic_graph.state', 'metadata': {'file_path': 'pydantic-ai/docs/api/pydantic_graph/state.md', 'section_title': '`pydantic_graph.state`'}}, {'source_file': 'pydantic-ai/docs/api/pydantic_graph/nodes.md', 'title': '`pydantic_graph.nodes`', 'content': '# `pydantic_graph.nodes`\\n\\n::: pydantic_graph.nodes\\n    options:\\n        members:\\n            - GraphRunContext\\n            - BaseNode\\n            - End\\n            - Edge\\n            - DepsT\\n            - RunEndT\\n            - NodeRunEndT', 'metadata': {'file_path': 'pydantic-ai/docs/api/pydantic_graph/nodes.md', 'section_title': '`pydantic_graph.nodes`'}}, {'source_file': 'pydantic-ai/docs/api/pydantic_graph/mermaid.md', 'title': '`pydantic_graph.mermaid`', 'content': '# `pydantic_graph.mermaid`\\n\\n::: pydantic_graph.mermaid', 'metadata': {'file_path': 'pydantic-ai/docs/api/pydantic_graph/mermaid.md', 'section_title': '`pydantic_graph.mermaid`'}}, {'source_file': 'pydantic-ai/docs/api/pydantic_graph/graph.md', 'title': '`pydantic_graph`', 'content': '# `pydantic_graph`\\n\\n::: pydantic_graph.graph', 'metadata': {'file_path': 'pydantic-ai/docs/api/pydantic_graph/graph.md', 'section_title': '`pydantic_graph`'}}, {'source_file': 'pydantic-ai/examples/README.md', 'title': 'PydanticAI Examples', 'content': '# PydanticAI Examples\\n\\n[![CI](https://github.com/pydantic/pydantic-ai/actions/workflows/ci.yml/badge.svg?event=push)](https://github.com/pydantic/pydantic-ai/actions/workflows/ci.yml?query=branch%3Amain)\\n[![Coverage](https://coverage-badge.samuelcolvin.workers.dev/pydantic/pydantic-ai.svg)](https://coverage-badge.samuelcolvin.workers.dev/redirect/pydantic/pydantic-ai)\\n[![PyPI](https://img.shields.io/pypi/v/pydantic-ai.svg)](https://pypi.python.org/pypi/pydantic-ai)\\n[![versions](https://img.shields.io/pypi/pyversions/pydantic-ai.svg)](https://github.com/pydantic/pydantic-ai)\\n[![license](https://img.shields.io/github/license/pydantic/pydantic-ai.svg?v)](https://github.com/pydantic/pydantic-ai/blob/main/LICENSE)\\n\\nExamples of how to use PydanticAI and what it can do.\\n\\nFor full documentation of these examples and how to run them, see [ai.pydantic.dev/examples/](https://ai.pydantic.dev/examples/).', 'metadata': {'file_path': 'pydantic-ai/examples/README.md', 'section_title': 'PydanticAI Examples'}}, {'source_file': 'pydantic-ai/pydantic_graph/README.md', 'title': 'Pydantic Graph', 'content': '# Pydantic Graph\\n\\n[![CI](https://github.com/pydantic/pydantic-ai/actions/workflows/ci.yml/badge.svg?event=push)](https://github.com/pydantic/pydantic-ai/actions/workflows/ci.yml?query=branch%3Amain)\\n[![Coverage](https://coverage-badge.samuelcolvin.workers.dev/pydantic/pydantic-ai.svg)](https://coverage-badge.samuelcolvin.workers.dev/redirect/pydantic/pydantic-ai)\\n[![PyPI](https://img.shields.io/pypi/v/pydantic-graph.svg)](https://pypi.python.org/pypi/pydantic-graph)\\n[![python versions](https://img.shields.io/pypi/pyversions/pydantic-graph.svg)](https://github.com/pydantic/pydantic-ai)\\n[![license](https://img.shields.io/github/license/pydantic/pydantic-ai.svg)](https://github.com/pydantic/pydantic-ai/blob/main/LICENSE)\\n\\nGraph and finite state machine library.\\n\\nThis library is developed as part of [PydanticAI](https://ai.pydantic.dev), however it has no dependency\\non `pydantic-ai` or related packages and can be considered as a pure graph-based state machine library. You may find it useful whether or not you\\'re using PydanticAI or even building with GenAI.\\n\\nAs with PydanticAI, this library prioritizes type safety and use of common Python syntax over esoteric, domain-specific use of Python syntax.\\n\\n`pydantic-graph` allows you to define graphs using standard Python syntax. In particular, edges are defined using the return type hint of nodes.\\n\\nFull documentation is available at [ai.pydantic.dev/graph](https://ai.pydantic.dev/graph).\\n\\nHere\\'s a basic example:\\n\\n```python {noqa=\"I001\" py=\"3.10\"}\\nfrom __future__ import annotations\\n\\nfrom dataclasses import dataclass\\n\\nfrom pydantic_graph import BaseNode, End, Graph, GraphRunContext\\n\\n\\n@dataclass\\nclass DivisibleBy5(BaseNode[None, None, int]):\\n    foo: int\\n\\n    async def run(\\n        self,\\n        ctx: GraphRunContext,\\n    ) -> Increment | End[int]:\\n        if self.foo % 5 == 0:\\n            return End(self.foo)\\n        else:\\n            return Increment(self.foo)\\n\\n\\n@dataclass\\nclass Increment(BaseNode):\\n    foo: int\\n\\n    async def run(self, ctx: GraphRunContext) -> DivisibleBy5:\\n        return DivisibleBy5(self.foo + 1)\\n\\n\\nfives_graph = Graph(nodes=[DivisibleBy5, Increment])\\nresult, history = fives_graph.run_sync(DivisibleBy5(4))\\nprint(result)\\n#> 5', 'metadata': {'file_path': 'pydantic-ai/pydantic_graph/README.md', 'section_title': 'Pydantic Graph'}}, {'source_file': 'pydantic-ai/pydantic_graph/README.md', 'title': \"the full history is quite verbose (see below), so we'll just print the summary\", 'content': \"# the full history is quite verbose (see below), so we'll just print the summary\\nprint([item.data_snapshot() for item in history])\\n#> [DivisibleBy5(foo=4), Increment(foo=4), DivisibleBy5(foo=5), End(data=5)]\\n```\", 'metadata': {'file_path': 'pydantic-ai/pydantic_graph/README.md', 'section_title': \"the full history is quite verbose (see below), so we'll just print the summary\"}}]\n"
     ]
    }
   ],
   "source": [
    "print(all_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import json\n",
    "from psycopg2.extras import execute_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Database connection parameters\n",
    "db_params = {\n",
    "    'dbname': 'postgres',\n",
    "    'user': 'postgres',\n",
    "    'password': 'postgres',\n",
    "    'host': 'localhost',\n",
    "    'port': '5432'\n",
    "}\n",
    "\n",
    "# Connect to the database\n",
    "conn = psycopg2.connect(**db_params)\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Prepare the data for insertion\n",
    "values = [\n",
    "    (\n",
    "        chunk['source_file'],\n",
    "        chunk['title'],\n",
    "        chunk['content'],\n",
    "        json.dumps(chunk['metadata'])\n",
    "    )\n",
    "    for chunk in all_chunks\n",
    "]\n",
    "\n",
    "# Insert the data\n",
    "execute_values(\n",
    "    cur,\n",
    "    \"\"\"\n",
    "    INSERT INTO documentation (source_file, title, content, metadata)\n",
    "    VALUES %s\n",
    "    \"\"\",\n",
    "    values\n",
    ")\n",
    "\n",
    "# Commit and close\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets check the database to make sure our data was added.\n",
    "- we can improve the chunkign and the storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Why use PydanticAI\n",
    "\n",
    "# * __Built by the Pydantic Team__\n",
    "# Built by the team behind [Pydantic](https://docs.pydantic.dev/latest/) (the validation layer of the OpenAI SDK, the Anthropic SDK, LangChain, LlamaIndex, AutoGPT, Transformers, CrewAI, Instructor and many more).\n",
    "\n",
    "# * __Model-agnostic__\n",
    "# Supports OpenAI, Anthropic, Gemini, Ollama, Groq, and Mistral, and there is a simple interface to implement support for [other models](https://ai.pydantic.dev/models/).\n",
    "\n",
    "# * __Pydantic Logfire Integration__\n",
    "# Seamlessly [integrates](https://ai.pydantic.dev/logfire/) with [Pydantic Logfire](https://pydantic.dev/logfire) for real-time debugging, performance monitoring, and behavior tracking of your LLM-powered applications.\n",
    "\n",
    "# * __Type-safe__\n",
    "# Designed to make [type checking](https://ai.pydantic.dev/agents/#static-type-checking) as powerful and informative as possible for you.\n",
    "\n",
    "# * __Python-centric Design__\n",
    "# Leverages Python's familiar control flow and agent composition to build your AI-driven projects, making it easy to apply standard Python best practices you'd use in any other (non-AI) project.\n",
    "\n",
    "# * __Structured Responses__\n",
    "# Harnesses the power of [Pydantic](https://docs.pydantic.dev/latest/) to [validate and structure](https://ai.pydantic.dev/results/#structured-result-validation) model outputs, ensuring responses are consistent across runs.\n",
    "\n",
    "# * __Dependency Injection System__\n",
    "# Offers an optional [dependency injection](https://ai.pydantic.dev/dependencies/) system to provide data and services to your agent's [system prompts](https://ai.pydantic.dev/agents/#system-prompts), [tools](https://ai.pydantic.dev/tools/) and [result validators](https://ai.pydantic.dev/results/#result-validators-functions).\n",
    "# This is useful for testing and eval-driven iterative development.\n",
    "\n",
    "# * __Streamed Responses__\n",
    "# Provides the ability to [stream](https://ai.pydantic.dev/results/#streamed-results) LLM outputs continuously, with immediate validation, ensuring rapid and accurate results.\n",
    "\n",
    "# * __Graph Support__\n",
    "# [Pydantic Graph](https://ai.pydantic.dev/graph) provides a powerful way to define graphs using typing hints, this is useful in complex applications where standard control flow can degrade to spaghetti code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use pgAI's vectorizer to generate embeddings for each chunk of text. These embeddings capture the semantic meaning of the text and are crucial for semantic search.\n",
    "\n",
    "**Note:** You'll need to have pgAI installed and configured in your PostgreSQL database. Refer to the pgAI documentation for installation instructions.\n",
    "\n",
    "**Run the SQL command in your database to create a vectorizer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3111195935.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[40], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    -- Create the vectorizer\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "-- Create the vectorizer\n",
    "SELECT ai.create_vectorizer(\n",
    "    'documentation'::regclass,\n",
    "    destination => 'documentation_embeddings',\n",
    "    embedding => ai.embedding_ollama('nomic-embed-text', 768),\n",
    "    chunking => ai.chunking_recursive_character_text_splitter('content')\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then, create embeddings for the content column in your documentation table:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "source": [
    "SELECT\n",
    "    content,\n",
    "    embedding <=> ai.ollama_embed('nomic-embed-text', 'what is an agent?', host => 'http://ollama:11434') as distance\n",
    "FROM documentation_embeddings\n",
    "ORDER BY distance\n",
    "LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Semantic Search with SQL\n",
    "Now we can perform semantic searches against our database using SQL queries. pgAI provides the <=> operator for cosine similarity searches. you can find this code as file 04 in the db_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "source": [
    "-- Semantic Search\n",
    "SELECT\n",
    "    content,\n",
    "    embedding <=> ai.ollama_embed('nomic-embed-text', 'what is an agent?', host => 'http://ollama:11434') as distance\n",
    "FROM documentation_embeddings\n",
    "ORDER BY distance\n",
    "LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Python Integration\n",
    "Finally, let's demonstrate how to integrate these semantic search capabilities into a Python application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database connection parameters\n",
    "db_params = {\n",
    "    'dbname': 'postgres',\n",
    "    'user': 'postgres',\n",
    "    'password': 'postgres',\n",
    "    'host': 'localhost',\n",
    "    'port': '5432'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_search_results(search_text):\n",
    "    # Reconnect to the database\n",
    "    conn = psycopg2.connect(**db_params)\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    query = \"\"\"\n",
    "    SELECT\n",
    "        content,\n",
    "        embedding <=> ai.ollama_embed('nomic-embed-text', %s, host => 'http://ollama:11434') as distance\n",
    "    FROM documentation_embeddings\n",
    "    ORDER BY distance\n",
    "    LIMIT 1;\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Execute the query with the search_text variable\n",
    "        cur.execute(query, (search_text,))\n",
    "        results = cur.fetchall()\n",
    "        \n",
    "        # Print results in markdown format\n",
    "        for row in results:\n",
    "            print(f\"## Search Result (Distance: {row[1]:.4f})\\n\")\n",
    "            # add markdown formatting\n",
    "            print(f\"{row[0]}\\n\")\n",
    "            print(\"---\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        # Always close cursor and connection\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'how to install pydantic'\n",
    "\n",
    "fetch_search_results(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
