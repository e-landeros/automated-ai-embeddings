{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the data files\n",
    "1. if you havent git cloned the files do so now.\n",
    "2. we will only grab the markdowns fiiles in the simple example. \n",
    "3. insert the process data into the database \n",
    "4. createa a vectorizer to process the data into embeddings. this is all handled by the pgai system\n",
    "5. Profit. Query the database in SQl\n",
    "6. brief python example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets grab all the markdown files in the directory and return a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_markdown_files(directory):\n",
    "    markdown_files = []\n",
    "    # Walk through the directory\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            # Check if the file has a .md extension\n",
    "            if file.endswith('.md'):\n",
    "                # Append the full path to the list\n",
    "                markdown_files.append(os.path.join(root, file))\n",
    "    return markdown_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets quickly print the files to make sure we are good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pydantic-ai/README.md', 'pydantic-ai/pydantic_ai_slim/README.md', 'pydantic-ai/tests/example_modules/README.md', 'pydantic-ai/docs/troubleshooting.md', 'pydantic-ai/docs/dependencies.md', 'pydantic-ai/docs/install.md', 'pydantic-ai/docs/help.md', 'pydantic-ai/docs/message-history.md', 'pydantic-ai/docs/testing-evals.md', 'pydantic-ai/docs/multi-agent-applications.md', 'pydantic-ai/docs/results.md', 'pydantic-ai/docs/index.md', 'pydantic-ai/docs/models.md', 'pydantic-ai/docs/contributing.md', 'pydantic-ai/docs/agents.md', 'pydantic-ai/docs/logfire.md', 'pydantic-ai/docs/graph.md', 'pydantic-ai/docs/tools.md', 'pydantic-ai/docs/examples/rag.md', 'pydantic-ai/docs/examples/bank-support.md', 'pydantic-ai/docs/examples/flight-booking.md', 'pydantic-ai/docs/examples/stream-whales.md', 'pydantic-ai/docs/examples/sql-gen.md', 'pydantic-ai/docs/examples/pydantic-model.md', 'pydantic-ai/docs/examples/chat-app.md', 'pydantic-ai/docs/examples/question-graph.md', 'pydantic-ai/docs/examples/index.md', 'pydantic-ai/docs/examples/stream-markdown.md', 'pydantic-ai/docs/examples/weather-agent.md', 'pydantic-ai/docs/api/agent.md', 'pydantic-ai/docs/api/exceptions.md', 'pydantic-ai/docs/api/settings.md', 'pydantic-ai/docs/api/messages.md', 'pydantic-ai/docs/api/result.md', 'pydantic-ai/docs/api/usage.md', 'pydantic-ai/docs/api/format_as_xml.md', 'pydantic-ai/docs/api/tools.md', 'pydantic-ai/docs/api/models/anthropic.md', 'pydantic-ai/docs/api/models/groq.md', 'pydantic-ai/docs/api/models/openai.md', 'pydantic-ai/docs/api/models/base.md', 'pydantic-ai/docs/api/models/ollama.md', 'pydantic-ai/docs/api/models/function.md', 'pydantic-ai/docs/api/models/mistral.md', 'pydantic-ai/docs/api/models/vertexai.md', 'pydantic-ai/docs/api/models/gemini.md', 'pydantic-ai/docs/api/models/test.md', 'pydantic-ai/docs/api/pydantic_graph/exceptions.md', 'pydantic-ai/docs/api/pydantic_graph/state.md', 'pydantic-ai/docs/api/pydantic_graph/nodes.md', 'pydantic-ai/docs/api/pydantic_graph/mermaid.md', 'pydantic-ai/docs/api/pydantic_graph/graph.md', 'pydantic-ai/examples/README.md', 'pydantic-ai/pydantic_graph/README.md']\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "directory_path = 'pydantic-ai'\n",
    "markdown_files = find_markdown_files(directory_path)\n",
    "print(markdown_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pydantic-ai/README.md\n",
      "pydantic-ai/pydantic_ai_slim/README.md\n",
      "pydantic-ai/tests/example_modules/README.md\n",
      "pydantic-ai/docs/troubleshooting.md\n",
      "pydantic-ai/docs/dependencies.md\n",
      "pydantic-ai/docs/install.md\n",
      "pydantic-ai/docs/help.md\n",
      "pydantic-ai/docs/message-history.md\n",
      "pydantic-ai/docs/testing-evals.md\n",
      "pydantic-ai/docs/multi-agent-applications.md\n",
      "pydantic-ai/docs/results.md\n",
      "pydantic-ai/docs/index.md\n",
      "pydantic-ai/docs/models.md\n",
      "pydantic-ai/docs/contributing.md\n",
      "pydantic-ai/docs/agents.md\n",
      "pydantic-ai/docs/logfire.md\n",
      "pydantic-ai/docs/graph.md\n",
      "pydantic-ai/docs/tools.md\n",
      "pydantic-ai/docs/examples/rag.md\n",
      "pydantic-ai/docs/examples/bank-support.md\n",
      "pydantic-ai/docs/examples/flight-booking.md\n",
      "pydantic-ai/docs/examples/stream-whales.md\n",
      "pydantic-ai/docs/examples/sql-gen.md\n",
      "pydantic-ai/docs/examples/pydantic-model.md\n",
      "pydantic-ai/docs/examples/chat-app.md\n",
      "pydantic-ai/docs/examples/question-graph.md\n",
      "pydantic-ai/docs/examples/index.md\n",
      "pydantic-ai/docs/examples/stream-markdown.md\n",
      "pydantic-ai/docs/examples/weather-agent.md\n",
      "pydantic-ai/docs/api/agent.md\n",
      "pydantic-ai/docs/api/exceptions.md\n",
      "pydantic-ai/docs/api/settings.md\n",
      "pydantic-ai/docs/api/messages.md\n",
      "pydantic-ai/docs/api/result.md\n",
      "pydantic-ai/docs/api/usage.md\n",
      "pydantic-ai/docs/api/format_as_xml.md\n",
      "pydantic-ai/docs/api/tools.md\n",
      "pydantic-ai/docs/api/models/anthropic.md\n",
      "pydantic-ai/docs/api/models/groq.md\n",
      "pydantic-ai/docs/api/models/openai.md\n",
      "pydantic-ai/docs/api/models/base.md\n",
      "pydantic-ai/docs/api/models/ollama.md\n",
      "pydantic-ai/docs/api/models/function.md\n",
      "pydantic-ai/docs/api/models/mistral.md\n",
      "pydantic-ai/docs/api/models/vertexai.md\n",
      "pydantic-ai/docs/api/models/gemini.md\n",
      "pydantic-ai/docs/api/models/test.md\n",
      "pydantic-ai/docs/api/pydantic_graph/exceptions.md\n",
      "pydantic-ai/docs/api/pydantic_graph/state.md\n",
      "pydantic-ai/docs/api/pydantic_graph/nodes.md\n",
      "pydantic-ai/docs/api/pydantic_graph/mermaid.md\n",
      "pydantic-ai/docs/api/pydantic_graph/graph.md\n",
      "pydantic-ai/examples/README.md\n",
      "pydantic-ai/pydantic_graph/README.md\n"
     ]
    }
   ],
   "source": [
    "# pretty print the markdown file names\n",
    "for file in markdown_files:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets open up one fo the files\n",
    "- as we can see we are successful grabing the pydantic-ai documentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div align=\"center\">\n",
      "  <a href=\"https://ai.pydantic.dev/\">\n",
      "    <picture>\n",
      "      <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://ai.pydantic.dev/img/pydantic-ai-dark.svg\">\n",
      "      <img src=\"https://ai.pydantic.dev/img/pydantic-ai-light.svg\" alt=\"PydanticAI\">\n",
      "    </picture>\n",
      "  </a>\n",
      "</div>\n",
      "<div align=\"center\">\n",
      "  <em>Agent Framework / shim to use Pydantic with LLMs</em>\n",
      "</div>\n",
      "<div align=\"center\">\n",
      "  <a href=\"https://github.com/pydantic/pydantic-ai/actions/workflows/ci.yml?query=branch%3Amain\"><img src=\"https://github.com/pydantic/pydantic-ai/actions/workflows/ci.yml/badge.svg?event=push\" alt=\"CI\"></a>\n",
      "  <a href=\"https://coverage-badge.samuelcolvin.workers.dev/redirect/pydantic/pydantic-ai\"><img src=\"https://coverage-badge.samuelcolvin.workers.dev/pydantic/pydantic-ai.svg\" alt=\"Coverage\"></a>\n",
      "  <a href=\"https://pypi.python.org/pypi/pydantic-ai\"><img src=\"https://img.shields.io/pypi/v/pydantic-ai.svg\" alt=\"PyPI\"></a>\n",
      "  <a href=\"https://github.com/pydantic/pydantic-ai\"><img src=\"https://img.shields.io/pypi/pyversions/pydantic-ai.svg\" alt=\"versions\"></a>\n",
      "  <a href=\"https://github.com/pydantic/pydantic-ai/blob/main/LICENSE\"><img src=\"https://img.shields.io/github/license/pydantic/pydantic-ai.svg?v\" alt=\"license\"></a>\n",
      "</div>\n",
      "\n",
      "---\n",
      "\n",
      "**Documentation**: [ai.pydantic.dev](https://ai.pydantic.dev/)\n",
      "\n",
      "---\n",
      "\n",
      "PydanticAI is a Python agent framework designed to make it less painful to build production grade applications with Generative AI.\n",
      "\n",
      "FastAPI revolutionized web development by offering an innovative and ergonomic design, built on the foundation of [Pydantic](https://docs.pydantic.dev).\n",
      "\n",
      "Similarly, virtually every agent framework and LLM library in Python uses Pydantic, yet when we began to use LLMs in [Pydantic Logfire](https://pydantic.dev/logfire), we couldn't find anything that gave us the same feeling.\n",
      "\n",
      "We built PydanticAI with one simple aim: to bring that FastAPI feeling to GenAI app development.\n",
      "\n",
      "## Why use PydanticAI\n",
      "\n",
      "* __Built by the Pydantic Team__\n",
      "Built by the team behind [Pydantic](https://docs.pydantic.dev/latest/) (the validation layer of the OpenAI SDK, the Anthropic SDK, LangChain, LlamaIndex, AutoGPT, Transformers, CrewAI, Instructor and many more).\n",
      "\n",
      "* __Model-agnostic__\n",
      "Supports OpenAI, Anthropic, Gemini, Ollama, Groq, and Mistral, and there is a simple interface to implement support for [other models](https://ai.pydantic.dev/models/).\n",
      "\n",
      "* __Pydantic Logfire Integration__\n",
      "Seamlessly [integrates](https://ai.pydantic.dev/logfire/) with [Pydantic Logfire](https://pydantic.dev/logfire) for real-time debugging, performance monitoring, and behavior tracking of your LLM-powered applications.\n",
      "\n",
      "* __Type-safe__\n",
      "Designed to make [type checking](https://ai.pydantic.dev/agents/#static-type-checking) as powerful and informative as possible for you.\n",
      "\n",
      "* __Python-centric Design__\n",
      "Leverages Python's familiar control flow and agent composition to build your AI-driven projects, making it easy to apply standard Python best practices you'd use in any other (non-AI) project.\n",
      "\n",
      "* __Structured Responses__\n",
      "Harnesses the power of [Pydantic](https://docs.pydantic.dev/latest/) to [validate and structure](https://ai.pydantic.dev/results/#structured-result-validation) model outputs, ensuring responses are consistent across runs.\n",
      "\n",
      "* __Dependency Injection System__\n",
      "Offers an optional [dependency injection](https://ai.pydantic.dev/dependencies/) system to provide data and services to your agent's [system prompts](https://ai.pydantic.dev/agents/#system-prompts), [tools](https://ai.pydantic.dev/tools/) and [result validators](https://ai.pydantic.dev/results/#result-validators-functions).\n",
      "This is useful for testing and eval-driven iterative development.\n",
      "\n",
      "* __Streamed Responses__\n",
      "Provides the ability to [stream](https://ai.pydantic.dev/results/#streamed-results) LLM outputs continuously, with immediate validation, ensuring rapid and accurate results.\n",
      "\n",
      "* __Graph Support__\n",
      "[Pydantic Graph](https://ai.pydantic.dev/graph) provides a powerful way to define graphs using typing hints, this is useful in complex applications where standard control flow can degrade to spaghetti code.\n",
      "\n",
      "## In Beta!\n",
      "\n",
      "PydanticAI is in early beta, the API is still subject to change and there's a lot more to do.\n",
      "[Feedback](https://github.com/pydantic/pydantic-ai/issues) is very welcome!\n",
      "\n",
      "## Hello World Example\n",
      "\n",
      "Here's a minimal example of PydanticAI:\n",
      "\n",
      "```python\n",
      "from pydantic_ai import Agent\n",
      "\n",
      "# Define a very simple agent including the model to use, you can also set the model when running the agent.\n",
      "agent = Agent(\n",
      "    'gemini-1.5-flash',\n",
      "    # Register a static system prompt using a keyword argument to the agent.\n",
      "    # For more complex dynamically-generated system prompts, see the example below.\n",
      "    system_prompt='Be concise, reply with one sentence.',\n",
      ")\n",
      "\n",
      "# Run the agent synchronously, conducting a conversation with the LLM.\n",
      "# Here the exchange should be very short: PydanticAI will send the system prompt and the user query to the LLM,\n",
      "# the model will return a text response. See below for a more complex run.\n",
      "result = agent.run_sync('Where does \"hello world\" come from?')\n",
      "print(result.data)\n",
      "\"\"\"\n",
      "The first known use of \"hello, world\" was in a 1974 textbook about the C programming language.\n",
      "\"\"\"\n",
      "```\n",
      "\n",
      "_(This example is complete, it can be run \"as is\")_\n",
      "\n",
      "Not very interesting yet, but we can easily add \"tools\", dynamic system prompts, and structured responses to build more powerful agents.\n",
      "\n",
      "## Tools & Dependency Injection Example\n",
      "\n",
      "Here is a concise example using PydanticAI to build a support agent for a bank:\n",
      "\n",
      "**(Better documented example [in the docs](https://ai.pydantic.dev/#tools-dependency-injection-example))**\n",
      "\n",
      "```python\n",
      "from dataclasses import dataclass\n",
      "\n",
      "from pydantic import BaseModel, Field\n",
      "from pydantic_ai import Agent, RunContext\n",
      "\n",
      "from bank_database import DatabaseConn\n",
      "\n",
      "\n",
      "# SupportDependencies is used to pass data, connections, and logic into the model that will be needed when running\n",
      "# system prompt and tool functions. Dependency injection provides a type-safe way to customise the behavior of your agents.\n",
      "@dataclass\n",
      "class SupportDependencies:\n",
      "    customer_id: int\n",
      "    db: DatabaseConn\n",
      "\n",
      "\n",
      "# This pydantic model defines the structure of the result returned by the agent.\n",
      "class SupportResult(BaseModel):\n",
      "    support_advice: str = Field(description='Advice returned to the customer')\n",
      "    block_card: bool = Field(description=\"Whether to block the customer's card\")\n",
      "    risk: int = Field(description='Risk level of query', ge=0, le=10)\n",
      "\n",
      "\n",
      "# This agent will act as first-tier support in a bank.\n",
      "# Agents are generic in the type of dependencies they accept and the type of result they return.\n",
      "# In this case, the support agent has type `Agent[SupportDependencies, SupportResult]`.\n",
      "support_agent = Agent(\n",
      "    'openai:gpt-4o',\n",
      "    deps_type=SupportDependencies,\n",
      "    # The response from the agent will, be guaranteed to be a SupportResult,\n",
      "    # if validation fails the agent is prompted to try again.\n",
      "    result_type=SupportResult,\n",
      "    system_prompt=(\n",
      "        'You are a support agent in our bank, give the '\n",
      "        'customer support and judge the risk level of their query.'\n",
      "    ),\n",
      ")\n",
      "\n",
      "\n",
      "# Dynamic system prompts can make use of dependency injection.\n",
      "# Dependencies are carried via the `RunContext` argument, which is parameterized with the `deps_type` from above.\n",
      "# If the type annotation here is wrong, static type checkers will catch it.\n",
      "@support_agent.system_prompt\n",
      "async def add_customer_name(ctx: RunContext[SupportDependencies]) -> str:\n",
      "    customer_name = await ctx.deps.db.customer_name(id=ctx.deps.customer_id)\n",
      "    return f\"The customer's name is {customer_name!r}\"\n",
      "\n",
      "\n",
      "# `tool` let you register functions which the LLM may call while responding to a user.\n",
      "# Again, dependencies are carried via `RunContext`, any other arguments become the tool schema passed to the LLM.\n",
      "# Pydantic is used to validate these arguments, and errors are passed back to the LLM so it can retry.\n",
      "@support_agent.tool\n",
      "async def customer_balance(\n",
      "    ctx: RunContext[SupportDependencies], include_pending: bool\n",
      ") -> float:\n",
      "    \"\"\"Returns the customer's current account balance.\"\"\"\n",
      "    # The docstring of a tool is also passed to the LLM as the description of the tool.\n",
      "    # Parameter descriptions are extracted from the docstring and added to the parameter schema sent to the LLM.\n",
      "    balance = await ctx.deps.db.customer_balance(\n",
      "        id=ctx.deps.customer_id,\n",
      "        include_pending=include_pending,\n",
      "    )\n",
      "    return balance\n",
      "\n",
      "\n",
      "...  # In a real use case, you'd add more tools and a longer system prompt\n",
      "\n",
      "\n",
      "async def main():\n",
      "    deps = SupportDependencies(customer_id=123, db=DatabaseConn())\n",
      "    # Run the agent asynchronously, conducting a conversation with the LLM until a final response is reached.\n",
      "    # Even in this fairly simple case, the agent will exchange multiple messages with the LLM as tools are called to retrieve a result.\n",
      "    result = await support_agent.run('What is my balance?', deps=deps)\n",
      "    # The result will be validated with Pydantic to guarantee it is a `SupportResult`, since the agent is generic,\n",
      "    # it'll also be typed as a `SupportResult` to aid with static type checking.\n",
      "    print(result.data)\n",
      "    \"\"\"\n",
      "    support_advice='Hello John, your current account balance, including pending transactions, is $123.45.' block_card=False risk=1\n",
      "    \"\"\"\n",
      "\n",
      "    result = await support_agent.run('I just lost my card!', deps=deps)\n",
      "    print(result.data)\n",
      "    \"\"\"\n",
      "    support_advice=\"I'm sorry to hear that, John. We are temporarily blocking your card to prevent unauthorized transactions.\" block_card=True risk=8\n",
      "    \"\"\"\n",
      "```\n",
      "\n",
      "## Next Steps\n",
      "\n",
      "To try PydanticAI yourself, follow the instructions [in the examples](https://ai.pydantic.dev/examples/).\n",
      "\n",
      "Read the [docs](https://ai.pydantic.dev/agents/) to learn more about building applications with PydanticAI.\n",
      "\n",
      "Read the [API Reference](https://ai.pydantic.dev/api/agent/) to understand PydanticAI's interface.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print the contents of the first file\n",
    "with open(markdown_files[0], 'r') as file:\n",
    "    print(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next up Getting the Markdown files into the the database\n",
    "This solution will:\n",
    "- Split each markdown file into chunks based on headers\n",
    "- Preserve metadata about the source file and section\n",
    "- Create a table with the necessary columns\n",
    "- Set up a vectorizer to automatically embed the content\n",
    "- Insert all the chunks into the database\n",
    "- The vectorizer will automatically process the chunks and create embeddings that you can use for semantic search.\n",
    "Remember to adjust the database connection parameters according to your setup. Also, make sure you have the pgai extension and Ollama running as described in your vectorizer quick start guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Dict\n",
    "\n",
    "def chunk_markdown_file(file_path: str) -> List[Dict]:\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    # Split by headers (## or #)\n",
    "    chunks = re.split(r'(?=^#{1,2}\\s)', content, flags=re.MULTILINE)\n",
    "    \n",
    "    processed_chunks = []\n",
    "    for chunk in chunks:\n",
    "        if chunk.strip():  # Skip empty chunks\n",
    "            # Extract header if it exists\n",
    "            header_match = re.match(r'^#{1,2}\\s+(.+)$', chunk.split('\\n')[0])\n",
    "            header = header_match.group(1) if header_match else \"No Header\"\n",
    "            \n",
    "            processed_chunks.append({\n",
    "                'source_file': file_path,\n",
    "                'title': header,\n",
    "                'content': chunk.strip(),\n",
    "                'metadata': {\n",
    "                    'file_path': file_path,\n",
    "                    'section_title': header\n",
    "                }\n",
    "            })\n",
    "    \n",
    "    return processed_chunks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing file: pydantic-ai/docs/results.md\n",
      "\n",
      "\n",
      "=== Chunk 1 ===\n",
      "Title: No Header\n",
      "Source: pydantic-ai/docs/results.md\n",
      "\n",
      "Content Preview (first 768 chars):\n",
      "Results are the final values returned from [running an agent](agents.md#running-agents).\n",
      "The result values are wrapped in [`RunResult`][pydantic_ai.result.RunResult] and [`StreamedRunResult`][pydantic_ai.result.StreamedRunResult] so you can access other data like [usage][pydantic_ai.usage.Usage] of the run and [message history](message-history.md#accessing-messages-from-results)\n",
      "\n",
      "Both `RunResult` and `StreamedRunResult` are generic in the data they wrap, so typing information about the data returned by the agent is preserved.\n",
      "\n",
      "```python {title=\"olympics.py\"}\n",
      "from pydantic import BaseModel\n",
      "\n",
      "from pydantic_ai import Agent\n",
      "\n",
      "\n",
      "class CityLocation(BaseModel):\n",
      "    city: str\n",
      "    country: str\n",
      "\n",
      "\n",
      "agent = Agent('gemini-1.5-flash', result_type=CityLocation)\n",
      "result = agent. ...\n",
      "\n",
      "Metadata: {'file_path': 'pydantic-ai/docs/results.md', 'section_title': 'No Header'}\n",
      "==================================================\n",
      "\n",
      "=== Chunk 2 ===\n",
      "Title: Result data {#structured-result-validation}\n",
      "Source: pydantic-ai/docs/results.md\n",
      "\n",
      "Content Preview (first 768 chars):\n",
      "## Result data {#structured-result-validation}\n",
      "\n",
      "When the result type is `str`, or a union including `str`, plain text responses are enabled on the model, and the raw text response from the model is used as the response data.\n",
      "\n",
      "If the result type is a union with multiple members (after remove `str` from the members), each member is registered as a separate tool with the model in order to reduce the complexity of the tool schemas and maximise the chances a model will respond correctly.\n",
      "\n",
      "If the result type schema is not of type `\"object\"`, the result type is wrapped in a single element object, so the schema of all tools registered with the model are object schemas.\n",
      "\n",
      "Structured results (like tools) use Pydantic to build the JSON schema used for the tool, and to v ...\n",
      "\n",
      "Metadata: {'file_path': 'pydantic-ai/docs/results.md', 'section_title': 'Result data {#structured-result-validation}'}\n",
      "==================================================\n",
      "\n",
      "=== Chunk 3 ===\n",
      "Title: Streamed Results\n",
      "Source: pydantic-ai/docs/results.md\n",
      "\n",
      "Content Preview (first 768 chars):\n",
      "## Streamed Results\n",
      "\n",
      "There two main challenges with streamed results:\n",
      "\n",
      "1. Validating structured responses before they're complete, this is achieved by \"partial validation\" which was recently added to Pydantic in [pydantic/pydantic#10748](https://github.com/pydantic/pydantic/pull/10748).\n",
      "2. When receiving a response, we don't know if it's the final response without starting to stream it and peeking at the content. PydanticAI streams just enough of the response to sniff out if it's a tool call or a result, then streams the whole thing and calls tools, or returns the stream as a [`StreamedRunResult`][pydantic_ai.result.StreamedRunResult].\n",
      "\n",
      "### Streaming Text\n",
      "\n",
      "Example of streamed text result:\n",
      "\n",
      "```python {title=\"streamed_hello_world.py\" line_length=\"120\"}\n",
      "from py ...\n",
      "\n",
      "Metadata: {'file_path': 'pydantic-ai/docs/results.md', 'section_title': 'Streamed Results'}\n",
      "==================================================\n",
      "\n",
      "=== Chunk 4 ===\n",
      "Title: Examples\n",
      "Source: pydantic-ai/docs/results.md\n",
      "\n",
      "Content Preview (first 768 chars):\n",
      "## Examples\n",
      "\n",
      "The following examples demonstrate how to use streamed responses in PydanticAI:\n",
      "\n",
      "- [Stream markdown](examples/stream-markdown.md)\n",
      "- [Stream Whales](examples/stream-whales.md) ...\n",
      "\n",
      "Metadata: {'file_path': 'pydantic-ai/docs/results.md', 'section_title': 'Examples'}\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Test on the first markdown file\n",
    "test_file = markdown_files[10]\n",
    "print(f\"Testing file: {test_file}\\n\")\n",
    "\n",
    "# Get chunks for the test file\n",
    "test_chunks = chunk_markdown_file(test_file)\n",
    "\n",
    "# Print each chunk in a readable format\n",
    "for i, chunk in enumerate(test_chunks, 1):\n",
    "    print(f\"\\n=== Chunk {i} ===\")\n",
    "    print(f\"Title: {chunk['title']}\")\n",
    "    print(f\"Source: {chunk['source_file']}\")\n",
    "    print(f\"\\nContent Preview (first 768 chars):\")\n",
    "    print(chunk['content'][:768], \"...\\n\")\n",
    "    print(\"Metadata:\", chunk['metadata'])\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thast good enough for this example DB. lets process and chunk up all of the files and then put them into a database tabel that we created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all markdown files\n",
    "all_chunks = []\n",
    "for file_path in markdown_files:\n",
    "    chunks = chunk_markdown_file(file_path)\n",
    "    all_chunks.extend(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  we will insert this into the database using psycopg2 but you can run a normal sql statement int he terminal as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import json\n",
    "from psycopg2.extras import execute_values\n",
    "\n",
    "# Database connection parameters\n",
    "db_params = {\n",
    "    'dbname': 'postgres',\n",
    "    'user': 'postgres',\n",
    "    'password': 'postgres',\n",
    "    'host': 'localhost',\n",
    "    'port': '5432'\n",
    "}\n",
    "\n",
    "# Connect to the database\n",
    "conn = psycopg2.connect(**db_params)\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Prepare the data for insertion\n",
    "values = [\n",
    "    (\n",
    "        chunk['source_file'],\n",
    "        chunk['title'],\n",
    "        chunk['content'],\n",
    "        json.dumps(chunk['metadata'])\n",
    "    )\n",
    "    for chunk in all_chunks\n",
    "]\n",
    "\n",
    "# Insert the data\n",
    "execute_values(\n",
    "    cur,\n",
    "    \"\"\"\n",
    "    INSERT INTO documentation (source_file, title, content, metadata)\n",
    "    VALUES %s\n",
    "    \"\"\",\n",
    "    values\n",
    ")\n",
    "\n",
    "# Commit and close\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets check the database to make sure our data was added.\n",
    "- we can improve the chunkign and the storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Why use PydanticAI\n",
    "\n",
    "# * __Built by the Pydantic Team__\n",
    "# Built by the team behind [Pydantic](https://docs.pydantic.dev/latest/) (the validation layer of the OpenAI SDK, the Anthropic SDK, LangChain, LlamaIndex, AutoGPT, Transformers, CrewAI, Instructor and many more).\n",
    "\n",
    "# * __Model-agnostic__\n",
    "# Supports OpenAI, Anthropic, Gemini, Ollama, Groq, and Mistral, and there is a simple interface to implement support for [other models](https://ai.pydantic.dev/models/).\n",
    "\n",
    "# * __Pydantic Logfire Integration__\n",
    "# Seamlessly [integrates](https://ai.pydantic.dev/logfire/) with [Pydantic Logfire](https://pydantic.dev/logfire) for real-time debugging, performance monitoring, and behavior tracking of your LLM-powered applications.\n",
    "\n",
    "# * __Type-safe__\n",
    "# Designed to make [type checking](https://ai.pydantic.dev/agents/#static-type-checking) as powerful and informative as possible for you.\n",
    "\n",
    "# * __Python-centric Design__\n",
    "# Leverages Python's familiar control flow and agent composition to build your AI-driven projects, making it easy to apply standard Python best practices you'd use in any other (non-AI) project.\n",
    "\n",
    "# * __Structured Responses__\n",
    "# Harnesses the power of [Pydantic](https://docs.pydantic.dev/latest/) to [validate and structure](https://ai.pydantic.dev/results/#structured-result-validation) model outputs, ensuring responses are consistent across runs.\n",
    "\n",
    "# * __Dependency Injection System__\n",
    "# Offers an optional [dependency injection](https://ai.pydantic.dev/dependencies/) system to provide data and services to your agent's [system prompts](https://ai.pydantic.dev/agents/#system-prompts), [tools](https://ai.pydantic.dev/tools/) and [result validators](https://ai.pydantic.dev/results/#result-validators-functions).\n",
    "# This is useful for testing and eval-driven iterative development.\n",
    "\n",
    "# * __Streamed Responses__\n",
    "# Provides the ability to [stream](https://ai.pydantic.dev/results/#streamed-results) LLM outputs continuously, with immediate validation, ensuring rapid and accurate results.\n",
    "\n",
    "# * __Graph Support__\n",
    "# [Pydantic Graph](https://ai.pydantic.dev/graph) provides a powerful way to define graphs using typing hints, this is useful in complex applications where standard control flow can degrade to spaghetti code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## now run  03_vectorizer in the db to create a new vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run a Semantic Search in SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "SELECT\n",
    "    content,\n",
    "    embedding <=> ai.ollama_embed('nomic-embed-text', 'what is an agent?', host => 'http://ollama:11434') as distance\n",
    "FROM documentation_embeddings\n",
    "ORDER BY distance\n",
    "LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets create a function to run the SQL semantic search in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database connection parameters\n",
    "db_params = {\n",
    "    'dbname': 'postgres',\n",
    "    'user': 'postgres',\n",
    "    'password': 'postgres',\n",
    "    'host': 'localhost',\n",
    "    'port': '5432'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_search_results(search_text):\n",
    "    # Reconnect to the database\n",
    "    conn = psycopg2.connect(**db_params)\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Define the query with a placeholder\n",
    "    query = \"\"\"\n",
    "    SELECT\n",
    "        content,\n",
    "        embedding <=> ai.ollama_embed('nomic-embed-text', %s, host => 'http://ollama:11434') as distance\n",
    "    FROM documentation_embeddings\n",
    "    ORDER BY distance\n",
    "    LIMIT 1;\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Execute the query with the search_text variable\n",
    "        cur.execute(query, (search_text,))\n",
    "        results = cur.fetchall()\n",
    "        \n",
    "        # Print results in markdown format\n",
    "        for row in results:\n",
    "            print(f\"## Search Result (Distance: {row[1]:.4f})\\n\")\n",
    "            # add markdown formatting\n",
    "            print(f\"{row[0]}\\n\")\n",
    "            print(\"---\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        # Always close cursor and connection\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Search Result (Distance: 0.1857)\n",
      "\n",
      "# Installation\n",
      "\n",
      "PydanticAI is available on PyPI as [`pydantic-ai`](https://pypi.org/project/pydantic-ai/) so installation is as simple as:\n",
      "\n",
      "```bash\n",
      "pip/uv-add pydantic-ai\n",
      "```\n",
      "\n",
      "(Requires Python 3.9+)\n",
      "\n",
      "This installs the `pydantic_ai` package, core dependencies, and libraries required to use all the models\n",
      "included in PydanticAI. If you want to use a specific model, you can install the [\"slim\"](#slim-install) version of PydanticAI.\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = 'how to install pydantic'\n",
    "\n",
    "fetch_search_results(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets take a look at the new pydantic ai library and make a simple rag agent.\n",
    "1. import the necessary libs\n",
    "2. Connect to the ollama model running in the container\n",
    "3. Create a new agent\n",
    "4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import Agent\n",
    "from pydantic_ai.models.ollama import OllamaModel\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_model = OllamaModel(\n",
    "    model_name='llama',  \n",
    "    base_url='http://ollama:11434',  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(ollama_model, \n",
    "              system_prompt=(\n",
    "                  \"You are an expert on building applications with pydantic-ai library\"\n",
    "                  \"Answer questions by providing code and suggestions.\"\n",
    "                  {query}\n",
    "              )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@agent.tool\n",
    "def search_tool(query: str):\n",
    "    \"\"\"\n",
    "    Tool to perform a semantic search using the provided query.\n",
    "    \"\"\"\n",
    "    fetch_search_results(query)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
